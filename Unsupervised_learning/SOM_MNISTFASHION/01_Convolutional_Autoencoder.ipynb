{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are autoencoders? ##\n",
    "    \n",
    "Autoencoders are a type of unsupervised neural network (i.e., no class labels or labeled data) that seek to:\n",
    "\n",
    "1. Accept an input set of data (i.e., the input).\n",
    "2. Internally compress the input data into a latent-space representation (i.e., a single vector that compresses and quantifies the input).\n",
    "3. Reconstruct the input data from this latent representation (i.e., the output).\n",
    "\n",
    "Typically, we think of an autoencoder having two components/subnetworks:\n",
    "\n",
    "1. Encoder: Accepts the input data and compresses it into the latent-space. If we denote our input data as x and the encoder as E, then the output latent-space representation, s, would be s = E(x).\n",
    "2. Decoder: The decoder is responsible for accepting the latent-space representation s and then reconstructing the original input. If we denote the decoder function as D and the output of the detector as o, then we can represent the decoder as o = D(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(object):\n",
    "    def __init__ (self, width, height, depth, filters, latentDim, learning_rate=1e-3):\n",
    "        '''\n",
    "        Inputs:\n",
    "            width(int): Width of the input image in pixels.\n",
    "            height(int): Height of the input image in pixels.\n",
    "            depth(int): Number of channels (i.e., depth) of the input volume.\n",
    "            filters(tuple(int)): A tuple that contains the set of filters for convolution operations.\n",
    "            latentDim(int): The number of neurons in our fully-connected (Dense) latent vector\n",
    "        '''\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.depth = depth\n",
    "        self.filters = filters\n",
    "        self.latentDim = latentDim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = Adam(learning_rate)\n",
    "        self.encoder, self.decoder, self.autoencoder = self.create_nets(width, height, depth, filters, latentDim)\n",
    "        \n",
    "    #-------------------------------------------------------------------------------------------------------------------       \n",
    "    def create_nets(self, width, height, depth, filters, latentDim):\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        # define the input to the encoder\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "        # loop over the number of filters\n",
    "        for f in filters:\n",
    "            # apply a CONV => RELU => BN operation\n",
    "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "        # flatten the network and then construct our latent vector\n",
    "        volumeSize = K.int_shape(x)\n",
    "        x = Flatten()(x)\n",
    "        latent = Dense(latentDim)(x)\n",
    "        # build the encoder model\n",
    "        encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "        # start building the decoder model which will accept the\n",
    "        # output of the encoder as its inputs\n",
    "        latentInputs = Input(shape=(latentDim,))\n",
    "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "        # loop over our number of filters again, but this time in\n",
    "        # reverse order\n",
    "        for f in filters[::-1]:\n",
    "            # apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "            x = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "                padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        # apply a single CONV_TRANSPOSE layer used to recover the\n",
    "        # original depth of the image\n",
    "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "        # build the decoder model\n",
    "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "        # our autoencoder is the encoder + decoder\n",
    "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "        \n",
    "        autoencoder.compile(loss=\"mse\", optimizer=self.optimizer)\n",
    "        # return 3 networks\n",
    "        return encoder, decoder, autoencoder\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------\n",
    "    def train(self, epochs, batch_size, train_data, train_label, val_data, val_label):\n",
    "        self.autoencoder.fit(\n",
    "            train_data, \n",
    "            train_label, \n",
    "            validation_data=(val_data, val_label), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n",
      "[INFO] building autoencoder...\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for and batch size\n",
    "EPOCHS = 25\n",
    "BS = 32\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, _), (testX, _)) = mnist.load_data()\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "# construct our convolutional autoencoder\n",
    "print(\"[INFO] building autoencoder...\")\n",
    "autoencoder = ConvAutoencoder(28, 28, 1, (32, 64), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 102s 2ms/sample - loss: 0.0184 - val_loss: 0.0112\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 98s 2ms/sample - loss: 0.0104 - val_loss: 0.0097\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 103s 2ms/sample - loss: 0.0094 - val_loss: 0.0088\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 103s 2ms/sample - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 103s 2ms/sample - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 103s 2ms/sample - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.0072 - val_loss: 0.0076\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0070 - val_loss: 0.0075\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 108s 2ms/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 109s 2ms/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 111s 2ms/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 109s 2ms/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 108s 2ms/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 109s 2ms/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 108s 2ms/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 108s 2ms/sample - loss: 0.0066 - val_loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "autoencoder.train(EPOCHS, BS, trainX, trainX, testX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(testX)\n",
    "tst = testX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = autoencoder.autoencoder.predict(tst[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71d34a6810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71fe3af1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
