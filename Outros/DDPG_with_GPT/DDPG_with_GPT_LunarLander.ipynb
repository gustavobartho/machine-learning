{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "8WT_0Y-KqQdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers, models, optimizers\n",
        "from scipy.special import softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vQX1S3qQdc"
      },
      "source": [
        "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeO6YvRbqQdf",
        "outputId": "98286af3-47a0-4c4a-8c82-c041308ccc0c"
      },
      "outputs": [],
      "source": [
        "#Ornstein-Uhlenbeck Noise \n",
        "class OUActionNoise(object):\n",
        "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.4, x0=None):\n",
        "        self.mean = mean\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        self.x_prev = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ahtUWGtJqQdh"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "a = np.zeros(15)\n",
        "b = OUActionNoise(a)\n",
        "a += b()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-n5JPCPqQdi",
        "outputId": "a2773b70-2744-4aa3-c777-6acd96c63981"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, batch_size, block_size):\n",
        "        '''\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            batch_size (integer): The batch size.\n",
        "            block_size (integer): \n",
        "        '''\n",
        "        self.buffer = []\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = size\n",
        "        self.block_size = block_size\n",
        "        \n",
        "    #--------------------------------------------------------------------------------    \n",
        "    def append(self, steps):\n",
        "        '''\n",
        "        Args:\n",
        "            state (Numpy array): The state.              \n",
        "            action (integer): The action.\n",
        "            reward (float): The reward.\n",
        "            done (boolen): True if the next state is a terminal state and False otherwise. Is transformed to integer so tha True = 1, False = 0\n",
        "            next_state (Numpy array): The next state.           \n",
        "        '''\n",
        "        if self.size == self.max_size: del self.buffer[:len(steps)]\n",
        "        for step in steps: self.buffer.append(step)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------    \n",
        "    def getBlock(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
        "        '''\n",
        "        # gets the random indexes of the block start\n",
        "        options = np.arange(self.size - self.block_size)\n",
        "        prob_diff = 1e-4\n",
        "        probs = softmax(np.arange(1-prob_diff, 1, (prob_diff)/len(options))[:len(options)])\n",
        "        \n",
        "        idxs = np.random.choice(options, size=(self.batch_size,), replace=False, p=probs)\n",
        "        get_data = lambda i, idx: [mem[i] for mem in self.buffer[idx:idx+self.block_size]]\n",
        "        \n",
        "        # generate the batch by stacking the blocks\n",
        "        states = np.array(np.stack([get_data(0, idx) for idx in idxs], axis=0))\n",
        "        actions = np.array(np.stack([get_data(1, idx) for idx in idxs], axis=0))\n",
        "        rewards = np.array(np.stack([get_data(2, idx) for idx in idxs], axis=0))\n",
        "        next_states = np.array(np.stack([get_data(3, idx) for idx in idxs], axis=0))\n",
        "        done = np.array(np.stack([get_data(4, idx) for idx in idxs], axis=0))\n",
        "        positions = np.array(np.stack([get_data(5, idx) for idx in idxs], axis=0))\n",
        "        new_positions = np.array(np.stack([get_data(6, idx) for idx in idxs], axis=0))\n",
        "\n",
        "        return  states, actions, rewards, next_states, done, positions, new_positions\n",
        "    \n",
        "    #--------------------------------------------------------------------------------  \n",
        "    @property  \n",
        "    def size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Number of elements in the buffer\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    @property \n",
        "    def hasMinLength(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Boolean indicating if the memory have the minimum number of elements or not\n",
        "        '''\n",
        "        return (self.size >= (self.batch_size + self.block_size))\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    @property  \n",
        "    def data(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            List with all the elements in the buffer\n",
        "        '''\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en0TDn5qQdl",
        "outputId": "c485ce90-2d8c-4f0a-cc48-6efad0b8233c"
      },
      "outputs": [],
      "source": [
        "gpt_kernel_initializer = lambda: initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "gpt_bias_initializer = lambda: initializers.Zeros()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vxHdLrUWqQdm"
      },
      "outputs": [],
      "source": [
        "# Individual Head of self-attention\n",
        "class Head(layers.Layer):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        # key, query and value layers\n",
        "        self.key = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.query = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.value = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        # dropout layer\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        B, T, C = x.shape\n",
        "        if(B is None): B = self.batch_size \n",
        "        if(T is None): T = self.block_size\n",
        "        if(C is None): C = self.state_dim\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
        "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
        "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "46rkRg5nqQdn"
      },
      "outputs": [],
      "source": [
        "# Layer with multiple self-attention Heads for data communication \n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(batch_size, block_size, state_dim, head_size, dropout) for _ in range(num_heads)]\n",
        "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
        "        self.proj = layers.Dense(units=state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        # concatenate the heads outputs in the C dimension\n",
        "        out =  tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        # apply the projection and the dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VGMjfU_IqQdo"
      },
      "outputs": [],
      "source": [
        "#Simple feed forward for data computation\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, state_dim, dropout, last_resize=True, spread_dim=None):\n",
        "        super().__init__()\n",
        "        last_layer = [\n",
        "            layers.Dense(state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), \n",
        "            layers.Dropout(dropout)\n",
        "        ] if last_resize else []\n",
        "        \n",
        "        self.net = models.Sequential([\n",
        "            layers.Dense(spread_dim if spread_dim is not None else 4 * state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
        "            layers.Dropout(dropout),\n",
        "            *last_layer\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YnyFKuPEqQdp"
      },
      "outputs": [],
      "source": [
        "# Block containing a multi head attention module and a feed forward linear computation\n",
        "class Block(layers.Layer):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, dropout, last_resize, spread_dim):\n",
        "        super().__init__()\n",
        "        head_size = state_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
        "        self.sa = MultiHeadAttention(batch_size, block_size, state_dim, num_heads, head_size, dropout)\n",
        "        self.ffwd = FeedForward(state_dim, dropout, last_resize, spread_dim)\n",
        "        self.ln1 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        # Multi head attention with layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # feed forward with layer norm\n",
        "        x = self.ffwd(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_value = lambda : initializers.RandomNormal(mean=0.0, stddev=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "V4uZbSagqQdq"
      },
      "outputs": [],
      "source": [
        "class GPTModel(models.Model):\n",
        "    def __init__(self, n_layer, batch_size, block_size, embedding_dim, out_dim, num_heads, dropout, ffw):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.state_embedding = layers.Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.position_embedding = layers.Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.blocks = models.Sequential([\n",
        "            Block(\n",
        "                batch_size, \n",
        "                block_size, \n",
        "                embedding_dim, \n",
        "                num_heads, \n",
        "                dropout, \n",
        "                last_resize = (i != n_layer - 1 ),  \n",
        "                spread_dim = out_dim if (i == n_layer - 1 ) else None,\n",
        "            )for i in range(n_layer)\n",
        "        ])\n",
        "        self.ffw = ffw\n",
        "\n",
        "    def call(self, inp, training=False):\n",
        "        inputs, positions = inp[0], inp[1]\n",
        "        B, T, C = inputs.shape\n",
        "        if(T is None): T = self.block_size\n",
        "        pos_emb = self.position_embedding(positions)\n",
        "        x = self.state_embedding(inputs) + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.ffw(x)\n",
        "        return logits\n",
        "    \n",
        "    def generate(self, states, positions):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = states[:, -self.block_size:, :]\n",
        "        pos_cond = positions[:, -self.block_size:, :]\n",
        "        # get the predictions\n",
        "        actions = self([idx_cond, pos_cond])\n",
        "        # focus only on the last time step\n",
        "        return actions\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        states, positions = input_shape\n",
        "        self.position_embedding.build(positions)\n",
        "        self.state_embedding.build(states)\n",
        "        super(GPTModel, self).build((2, None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdauE0A-qQdr",
        "outputId": "f775f548-57dc-43a6-877b-8cf35079d9a6"
      },
      "outputs": [],
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, position_dim, embedding_dim, num_heads, dropout, action_range, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "            layers.Dense(128,  activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(action_dim, activation='tanh'),\n",
        "            layers.Lambda(lambda i: i * action_range, dtype='float64'),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, state_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, state_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, positions):\n",
        "        return self.model.generate(states, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, positions):\n",
        "        return self.target_model.generate(states, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def act(self, states, positions):\n",
        "        action = self.predict(states, positions)\n",
        "        # Gets the last action only\n",
        "        action = action[0, -1, :]\n",
        "        return action\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_actor_model.h5')\n",
        "        self.target_model.save(path + '_actor_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iws_SlRZqQds",
        "outputId": "30dfbb02-a02f-4373-fb58-8fd4db3b5774"
      },
      "outputs": [],
      "source": [
        "class Critic(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, position_dim, embedding_dim, out_dim, num_heads, dropout, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim + action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "                layers.Dense(128,  activation='relu'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(out_dim, activation='linear'),\n",
        "            ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, self.inp_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, self.inp_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, actions, positions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.model.generate(inp, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, actions, positions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.target_model.generate(inp, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_critic_model.h5')\n",
        "        self.target_model.save(path + '_critic_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "n2y8GZ7tqQds"
      },
      "outputs": [],
      "source": [
        "class DDPG_GPT_Agent(object):\n",
        "    def __init__(self, a_n_layers, c_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, c_n_heads, \n",
        "        dropout, action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
        "        epsilon_min, position_dim, a_embedding_dim, c_embedding_dim\n",
        "        ):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_min = action_min\n",
        "        self.action_max = action_max\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.position_dim = position_dim\n",
        "\n",
        "        #Creates the Replay Buffer\n",
        "        self.memory = ReplayBuffer(memory_size, batch_size, block_size)\n",
        "\n",
        "        #Creates the noise generator\n",
        "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
        "\n",
        "        #Creates the actor\n",
        "        self.actor = Actor(\n",
        "            n_layer=a_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            position_dim=position_dim,\n",
        "            embedding_dim=a_embedding_dim,\n",
        "            num_heads=a_n_heads, \n",
        "            dropout=dropout, \n",
        "            action_range=action_max, \n",
        "            lr=a_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "        \n",
        "        #Creates the critic\n",
        "        self.critic = Critic(\n",
        "            n_layer=c_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            position_dim=position_dim,\n",
        "            embedding_dim=c_embedding_dim,\n",
        "            out_dim=1,\n",
        "            num_heads=c_n_heads, \n",
        "            dropout=dropout, \n",
        "            lr=c_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "    \n",
        "    #--------------------------------------------------------------------     \n",
        "    def act(self, env):\n",
        "        step = 1\n",
        "        done = False\n",
        "\n",
        "        states = env.reset().reshape(1, 1, -1)\n",
        "        positions = self.int_to_bin(step).reshape(1, 1, -1)\n",
        "\n",
        "        while not done:\n",
        "            env.render()\n",
        "            # Generating actions for a given group of states \n",
        "            action = self.policy(states, positions, explore=False)\n",
        "            # Apply the action in the environment\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            step += 1\n",
        "            # Append the new state to the states history\n",
        "            states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "            positions = tf.concat((positions, self.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "        \n",
        "        return\n",
        "\n",
        "    #-------------------------------------------------------------------- \n",
        "    def policy(self, states, positions, explore=True):\n",
        "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
        "        # gets the action\n",
        "        action = self.actor.act(states, positions)\n",
        "        # takes the exploration with the epsilon probability\n",
        "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
        "        # clip the action to be between min and max values\n",
        "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
        "        action[np.isnan(action)] = 0\n",
        "\n",
        "        return action\n",
        "    \n",
        "    #-------------------------------------------------------------------- \n",
        "    def int_to_bin(self, num):\n",
        "        bin_r = np.array([b for b in np.binary_repr(num, width=self.position_dim)])\n",
        "        bin_arr = np.zeros(self.position_dim)\n",
        "        bin_arr[np.where(bin_r == '1')] = 1\n",
        "        return bin_arr\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def record_memories(self, steps):\n",
        "        mem_fix = 0.8\n",
        "        steps = np.array(steps, dtype=object)\n",
        "        if (np.random.rand() >= mem_fix):\n",
        "            avg = np.average(steps[:, :-1], axis=0, weights=np.absolute(steps[:, 2])) # take the weighted average of the numerical data with the weights being the rewards absolute value\n",
        "            done = np.any(steps[:, -1]) # check if the mean state is a done state - done states are now added separately to avoid information loss\n",
        "            avg[4] = np.floor(avg[4]) # make the average step into an integer\n",
        "            steps = np.array([np.concatenate([avg, [done]])], dtype=object)\n",
        "        \n",
        "        step_bin = np.array([self.int_to_bin(int(st)) for st in steps[:, 4]])\n",
        "        new_step_bin = np.array([self.int_to_bin(int(st + 1)) for st in steps[:, 4]])\n",
        "        steps = np.delete(steps, 4, 1)\n",
        "        steps = np.array([[*step, bin_r, n_bin_r] for step, bin_r, n_bin_r in zip(steps, step_bin, new_step_bin)], dtype=object) \n",
        "        self.memory.append(steps)\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def learn(self, memory_steps):\n",
        "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
        "        self.record_memories(memory_steps)\n",
        "        if self.memory.hasMinLength: self.replay_memory()\n",
        "        return\n",
        "        \n",
        "    #--------------------------------------------------------------------    \n",
        "    def replay_memory(self):\n",
        "        \"\"\" Replay a batch of memories \"\"\"\n",
        "\n",
        "        # Get sample block from the replay buffer\n",
        "        states, actions, rewards, next_states, done, positions, next_positions = self.memory.getBlock()\n",
        "        \n",
        "        states = tf.convert_to_tensor(states, dtype='float32')\n",
        "        actions = tf.convert_to_tensor(actions, dtype='float32')\n",
        "        next_states = tf.convert_to_tensor(next_states, dtype='float32')\n",
        "        positions = tf.convert_to_tensor(positions, dtype='float32')\n",
        "        next_positions = tf.convert_to_tensor(next_positions, dtype='float32')\n",
        "\n",
        "        rewards = tf.expand_dims(tf.convert_to_tensor(rewards, dtype='float32'), axis=-1)\n",
        "\n",
        "        done = tf.expand_dims(tf.convert_to_tensor(done, dtype='float32'), axis=-1)\n",
        "\n",
        "        #Train the critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Compute the critic target values\n",
        "            target_actions = self.actor.target_predict(next_states, next_positions)\n",
        "            predicted_return = self.critic.target_predict(next_states, target_actions, next_positions)\n",
        "            gamma_values = lambda i: tf.expand_dims(tf.repeat([[self.gamma**(k - i) for k in range(i, rewards.shape[1])]], repeats=self.batch_size, axis=0), axis=-1)\n",
        "            weighted_next_rewards = lambda i: tf.math.reduce_sum(rewards[:, i:, :] * gamma_values(i), axis=1)\n",
        "            last_return_weight = lambda i: self.gamma ** (rewards.shape[1] - i)\n",
        "            last_return = predicted_return[:, -1, :]\n",
        "            state_done = lambda i: 1 - done[:, i, :]\n",
        "            y = tf.stack([(weighted_next_rewards(i) + (last_return_weight(i) * last_return * state_done(i))) for i in range(rewards.shape[1])], axis=1)\n",
        "            #y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions, next_positions) * (1 - done) # y = (B, T, 1)\n",
        "            # Predict the expected rewsteps += 1ard associated with taking the target predicted action in the state\n",
        "            critic_value = self.critic.predict(states, actions, positions)\n",
        "            # Compute the critic loss  \n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "            print(critic_loss)\n",
        "            \n",
        "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
        "        \n",
        "        #Train the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts = self.actor.predict(states, positions)\n",
        "            critic_grads = self.critic.predict(states, acts, positions)\n",
        "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
        "            \n",
        "        #Update the model weights\n",
        "        self.actor.transferWeights()\n",
        "        self.critic.transferWeights() \n",
        "            \n",
        "        #Decay the epsilon value\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
        "\n",
        "    def print_data(self, verbose, episode, step, score):\n",
        "        if verbose:\n",
        "            print(\"\\r                                                                                                     \", end=\"\")\n",
        "            print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(step)+\"\\tReward: \"+str(score) ,end=\"\")\n",
        "        return\n",
        "\n",
        "    #--------------------------------------------------------------------     \n",
        "    def train(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
        "        scores_history = []\n",
        "        steps_history = []\n",
        "        complete = 0\n",
        "        print(\"BEGIN\\n\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            done = False\n",
        "            score, step = 0, 1\n",
        "            state = env.reset()\n",
        "            states = state.reshape(1, 1, -1)\n",
        "            positions = self.int_to_bin(step).reshape(1, 1, -1)\n",
        "            \n",
        "            while not done:\n",
        "                done_step = []\n",
        "                memory_steps = []\n",
        "                while not done and (step % step_per_train != 0):\n",
        "                    action = self.policy(states, positions)\n",
        "                    self.print_data(verbose, episode, step, score)\n",
        "                    new_state, reward, done, _ = env.step(action)\n",
        "                    if (done): done_step.append([state, action, reward, new_state, step, done])\n",
        "                    else: memory_steps.append([state, action, reward, new_state, step, done])\n",
        "                    state = new_state\n",
        "                    step += 1\n",
        "                    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "                    positions = tf.concat((positions, self.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "                    score += reward\n",
        "                \n",
        "                step += 1\n",
        "                if len(memory_steps) > 0: self.learn(memory_steps)\n",
        "                if len(done_step) > 0: self.learn(done_step)\n",
        "\n",
        "            scores_history.append(score)\n",
        "            steps_history.append(step)\n",
        "            \n",
        "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
        "            if(score >= complete_value):\n",
        "                complete += 1\n",
        "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
        "                if end_on_complete and complete >= complete_num: break\n",
        "            \n",
        "            #These information are printed after each verbose_num episodes\n",
        "            if((episode+1)%verbose_num == 0):\n",
        "                print(\"\\r                                                                                                          \", end=\"\")\n",
        "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes, \n",
        "                      \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_num:]), \n",
        "                      \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_num:]), \n",
        "                      *[\"\\n\\tCompleted: \", complete] if complete_value != float('inf') else '', \n",
        "                      \"\\n--------------------------\",\n",
        "                    )\n",
        "                \n",
        "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
        "                if act_after_batch: self.act(env)\n",
        "                \n",
        "                #Set the number of completed episodes on the batch to zero\n",
        "                complete = 0\n",
        "\n",
        "        print(\"\\nFINISHED\")\n",
        "        \n",
        "        return scores_history, steps_history\n",
        "    #--------------------------------------------------------------------     \n",
        "    def save(self, path):\n",
        "        self.actor.saveModel(path)\n",
        "        self.critic.saveModel(path)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def load(self, a_path, c_path):\n",
        "        self.actor.loadModel(a_path)\n",
        "        self.critic.loadModel(c_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wS8tg8OqQdt",
        "outputId": "3d5e1dc5-3b35-4c65-b236-f11b903b86d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True, max_episode_steps=500)\n",
        "batch_size = 3\n",
        "block_size = 4\n",
        "position_dim = 14\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_min = env.action_space.low\n",
        "action_max = env.action_space.high\n",
        "dropout = 0.05\n",
        "memory_size = 500000\n",
        "gamma = 0.99\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.999\n",
        "epsilon_min = 0.4\n",
        "tau = 5e-4\n",
        "\n",
        "# Actor hyperparameter\n",
        "a_n_layer = 2\n",
        "a_num_heads = 2\n",
        "a_embedding_dim = 8\n",
        "a_learning_rate = 3e-4\n",
        "\n",
        "# Critic hyperparameter\n",
        "c_n_layer = 2\n",
        "c_num_heads = 3\n",
        "c_embedding_dim = 12\n",
        "c_learning_rate = 5e-4\n",
        "\n",
        "agent = DDPG_GPT_Agent(\n",
        "    a_n_layers = a_n_layer,\n",
        "    c_n_layers = c_n_layer, \n",
        "    batch_size = batch_size, \n",
        "    block_size=block_size, \n",
        "    state_dim=state_dim, \n",
        "    action_dim=action_dim, \n",
        "    a_embedding_dim=a_embedding_dim,\n",
        "    c_embedding_dim=c_embedding_dim,\n",
        "    a_n_heads=a_num_heads, \n",
        "    c_n_heads=c_num_heads,\n",
        "    dropout=dropout, \n",
        "    action_min=action_min, \n",
        "    action_max=action_max, \n",
        "    memory_size=memory_size, \n",
        "    gamma=gamma, \n",
        "    a_lr=a_learning_rate, \n",
        "    c_lr=c_learning_rate, \n",
        "    tau=tau, \n",
        "    epsilon=epsilon, \n",
        "    epsilon_decay=epsilon_decay, \n",
        "    epsilon_min=epsilon_min,\n",
        "    position_dim=position_dim,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCvb6NiqQdu",
        "outputId": "bb989c21-13bb-441f-bc9a-179631844892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEGIN\n",
            "\n",
            "Episode: 1\t Step: 19\tReward: -8.162201341488432                                                      tf.Tensor(1.7861376, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 29\tReward: 8.008537579551858                                                       tf.Tensor(12.110185, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 39\tReward: -5.3936105993790875                                                     tf.Tensor(19.121191, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 49\tReward: -14.376468916995094                                                     tf.Tensor(19.901138, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 59\tReward: -25.105435869162182                                                     tf.Tensor(18.692211, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 69\tReward: -2.1131489593504766                                                     tf.Tensor(14.969699, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 79\tReward: 18.48317133915806tf.Tensor(17.02842, shape=(), dtype=float32)           \n",
            "Episode: 1\t Step: 89\tReward: -9.18625810622925                                                       tf.Tensor(23.754053, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 99\tReward: -47.267406361256725                                                     tf.Tensor(73.70988, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 109\tReward: -68.37924064796036tf.Tensor(17.42587, shape=(), dtype=float32)         \n",
            "Episode: 1\t Step: 119\tReward: -85.44955969538799tf.Tensor(94.47852, shape=(), dtype=float32)         \n",
            "Episode: 1\t Step: 129\tReward: -97.41833928585726tf.Tensor(17.543793, shape=(), dtype=float32)        \n",
            "Episode: 1\t Step: 139\tReward: -111.5019139795912                                                     tf.Tensor(14.772591, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 149\tReward: -125.79413145106697                                                    tf.Tensor(59.954365, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 159\tReward: -136.8760858323248                                                     tf.Tensor(26.825064, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 169\tReward: -154.96654620333365tf.Tensor(22.639204, shape=(), dtype=float32)       \n",
            "Episode: 1\t Step: 179\tReward: -171.64410377036782tf.Tensor(38.186535, shape=(), dtype=float32)       \n",
            "Episode: 1\t Step: 189\tReward: -213.2522275484552                                                     tf.Tensor(24.917185, shape=(), dtype=float32)\n",
            "Episode: 1\t Step: 196\tReward: -258.9707027498269                                                     tf.Tensor(198.94257, shape=(), dtype=float32)\n",
            "tf.Tensor(21.78831, shape=(), dtype=float32)\n",
            "Episode: 2\t Step: 9\tReward: 15.36703012833063tf.Tensor(810.12555, shape=(), dtype=float32)           \n",
            "Episode: 2\t Step: 19\tReward: 15.387834174390747tf.Tensor(25.000368, shape=(), dtype=float32)         \n",
            "Episode: 2\t Step: 29\tReward: 2.3942887009358964tf.Tensor(35.741356, shape=(), dtype=float32)         \n",
            "Episode: 2\t Step: 39\tReward: 4.034679977702823tf.Tensor(21.13447, shape=(), dtype=float32)           \n",
            "Episode: 2\t Step: 49\tReward: 6.14749428121303tf.Tensor(27.74289, shape=(), dtype=float32)            \n",
            "Episode: 2\t Step: 59\tReward: 7.959048666331759tf.Tensor(76.38644, shape=(), dtype=float32)           \n",
            "Episode: 2\t Step: 69\tReward: 3.095811522110898tf.Tensor(2.1260386, shape=(), dtype=float32)          \n",
            "Episode: 2\t Step: 79\tReward: -21.294919899616573tf.Tensor(60.226208, shape=(), dtype=float32)        \n",
            "Episode: 2\t Step: 85\tReward: -29.87366915216439                                                      tf.Tensor(11.029067, shape=(), dtype=float32)\n",
            "tf.Tensor(42.52986, shape=(), dtype=float32)\n",
            "Episode: 3\t Step: 9\tReward: 1.3310861381485892                                                       tf.Tensor(55.374027, shape=(), dtype=float32)\n",
            "Episode: 3\t Step: 19\tReward: -8.354923598770611tf.Tensor(19.211168, shape=(), dtype=float32)         \n",
            "Episode: 3\t Step: 29\tReward: -11.779111665387095tf.Tensor(143.79182, shape=(), dtype=float32)        \n",
            "Episode: 3\t Step: 39\tReward: -20.799855154729933tf.Tensor(20.563185, shape=(), dtype=float32)        \n",
            "Episode: 3\t Step: 49\tReward: -13.980864373166012                                                     tf.Tensor(4019.4592, shape=(), dtype=float32)\n",
            "Episode: 3\t Step: 59\tReward: 6.177516856678333tf.Tensor(19.829115, shape=(), dtype=float32)          \n",
            "Episode: 3\t Step: 69\tReward: 8.366587279993736                                                       tf.Tensor(4.0928607, shape=(), dtype=float32)\n",
            "Episode: 3\t Step: 79\tReward: -46.31808989905242tf.Tensor(164.75464, shape=(), dtype=float32)         \n",
            "Episode: 3\t Step: 89\tReward: -104.16001601834989tf.Tensor(42.24286, shape=(), dtype=float32)         \n",
            "Episode: 3\t Step: 99\tReward: -156.89218945166596                                                     tf.Tensor(167.15735, shape=(), dtype=float32)\n",
            "Episode: 3\t Step: 109\tReward: -235.4469890813074tf.Tensor(92.93751, shape=(), dtype=float32)         \n",
            "Episode: 3\t Step: 116\tReward: -297.9373988563287                                                     tf.Tensor(55.66693, shape=(), dtype=float32)\n",
            "tf.Tensor(15.635598, shape=(), dtype=float32)\n",
            "Episode: 4\t Step: 9\tReward: -11.902804900015326tf.Tensor(26.227074, shape=(), dtype=float32)         \n",
            "Episode: 4\t Step: 19\tReward: -53.47017729697849tf.Tensor(67.85092, shape=(), dtype=float32)          \n",
            "Episode: 4\t Step: 29\tReward: -111.06076979721772tf.Tensor(106.099464, shape=(), dtype=float32)       \n",
            "Episode: 4\t Step: 39\tReward: -169.35387093368183tf.Tensor(514.0913, shape=(), dtype=float32)         \n",
            "Episode: 4\t Step: 49\tReward: -233.8754740424702                                                      tf.Tensor(97.432465, shape=(), dtype=float32)\n",
            "Episode: 4\t Step: 59\tReward: -296.99882011667586                                                     tf.Tensor(110.64797, shape=(), dtype=float32)\n",
            "Episode: 4\t Step: 69\tReward: -368.14314070327436                                                     tf.Tensor(223.43555, shape=(), dtype=float32)\n",
            "Episode: 4\t Step: 79\tReward: -449.50551730970466                                                     tf.Tensor(41.173737, shape=(), dtype=float32)\n",
            "tf.Tensor(24.956146, shape=(), dtype=float32)\n",
            "Episode: 5\t Step: 9\tReward: -5.169603698941776tf.Tensor(42.09418, shape=(), dtype=float32)           \n",
            "Episode: 5\t Step: 19\tReward: -23.053850658636048                                                     tf.Tensor(87.685036, shape=(), dtype=float32)\n",
            "Episode: 5\t Step: 29\tReward: -41.22549027361254tf.Tensor(139.37889, shape=(), dtype=float32)         \n",
            "Episode: 5\t Step: 39\tReward: -56.63917639186523tf.Tensor(56.709316, shape=(), dtype=float32)         \n",
            "Episode: 5\t Step: 49\tReward: -71.92729394929019                                                      tf.Tensor(4.0374026, shape=(), dtype=float32)\n",
            "Episode: 5\t Step: 59\tReward: -85.99915790509635                                                      tf.Tensor(143.56502, shape=(), dtype=float32)\n",
            "Episode: 5\t Step: 69\tReward: -91.44354490257413                                                      "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m verbose_num \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scores, steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49mnum_episodes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     step_per_train\u001b[39m=\u001b[39;49mstep_per_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     verbose_num\u001b[39m=\u001b[39;49mverbose_num,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     act_after_batch\u001b[39m=\u001b[39;49mact_after_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.train\u001b[0;34m(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory_steps) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(memory_steps)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(done_step) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(done_step)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m scores_history\u001b[39m.\u001b[39mappend(score)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.learn\u001b[0;34m(self, memory_steps)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m\"\"\" Append an experience to the memory and replay memory if possible \"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_memories(memory_steps)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mhasMinLength: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_memory()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.replay_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     \u001b[39m# Compute the critic target values\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     target_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mtarget_predict(next_states, next_positions)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     predicted_return \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mtarget_predict(next_states, target_actions, next_positions)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     gamma_values \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i: tf\u001b[39m.\u001b[39mexpand_dims(tf\u001b[39m.\u001b[39mrepeat([[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k \u001b[39m-\u001b[39m i) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i, rewards\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])]], repeats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     weighted_next_rewards \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i: tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_sum(rewards[:, i:, :] \u001b[39m*\u001b[39m gamma_values(i), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mCritic.target_predict\u001b[0;34m(self, states, actions, positions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m positions \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(positions, tf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m inp \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([states, actions], \u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_model\u001b[39m.\u001b[39;49mgenerate(inp, positions)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mGPTModel.generate\u001b[0;34m(self, states, positions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m pos_cond \u001b[39m=\u001b[39m positions[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size:, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# get the predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m([idx_cond, pos_cond])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# focus only on the last time step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m actions\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mGPTModel.call\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pos_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(positions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_embedding(inputs) \u001b[39m+\u001b[39m pos_emb\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/sequential.py:413\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    412\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m    415\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39m# the next layer.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:511\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m     \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:668\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 668\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    670\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    672\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[1;32m    673\u001b[0m ):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mBlock.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Multi head attention with layer norm\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# feed forward with layer norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(x)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mMultiHeadAttention.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# concatenate the heads outputs in the C dimension\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     out \u001b[39m=\u001b[39m  tf\u001b[39m.\u001b[39mconcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# apply the projection and the dropout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# concatenate the heads outputs in the C dimension\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     out \u001b[39m=\u001b[39m  tf\u001b[39m.\u001b[39mconcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# apply the projection and the dropout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mHead.call\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m wei \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(wei)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# perform the weighted aggregation of the values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(wei, v) \u001b[39m# (B, T, T) @ (B, T, C) -> (B, T, C)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/core/dense.py:244\u001b[0m, in \u001b[0;36mDense.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(a\u001b[39m=\u001b[39minputs, b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtensordot(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel, [[rank \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m], [\u001b[39m0\u001b[39;49m]])\n\u001b[1;32m    245\u001b[0m     \u001b[39m# Reshape the output back to the original ndim of the input.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:5149\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   5147\u001b[0m     \u001b[39mreturn\u001b[39;00m ab_matmul\n\u001b[1;32m   5148\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 5149\u001b[0m     \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49mreshape(\n\u001b[1;32m   5150\u001b[0m         ab_matmul, a_free_dims \u001b[39m+\u001b[39;49m b_free_dims, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   5151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5152\u001b[0m   a_free_dims \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(a_free_dims, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:199\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmanip.reshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     64\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(tensor, shape, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   \u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[1;32m    200\u001b[0m   tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[1;32m    201\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:8546\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8544\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   8545\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 8546\u001b[0m   \u001b[39mreturn\u001b[39;00m reshape_eager_fallback(\n\u001b[1;32m   8547\u001b[0m       tensor, shape, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[1;32m   8548\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[1;32m   8549\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:8568\u001b[0m, in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   8566\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape_eager_fallback\u001b[39m(tensor, shape, name, ctx):\n\u001b[1;32m   8567\u001b[0m   _attr_T, (tensor,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39margs_to_matching_eager([tensor], ctx, [])\n\u001b[0;32m-> 8568\u001b[0m   _attr_Tshape, (shape,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49margs_to_matching_eager([shape], ctx, [_dtypes\u001b[39m.\u001b[39;49mint32, _dtypes\u001b[39m.\u001b[39;49mint64, ], _dtypes\u001b[39m.\u001b[39;49mint32)\n\u001b[1;32m   8569\u001b[0m   _inputs_flat \u001b[39m=\u001b[39m [tensor, shape]\n\u001b[1;32m   8570\u001b[0m   _attrs \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, _attr_T, \u001b[39m\"\u001b[39m\u001b[39mTshape\u001b[39m\u001b[39m\"\u001b[39m, _attr_Tshape)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:254\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 254\u001b[0m   tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    255\u001b[0m   \u001b[39m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    256\u001b[0m   \u001b[39m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    257\u001b[0m   \u001b[39m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m   \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mdtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_dtypes:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\"\"\"Implementation of constant.\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m ctx \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m--> 275\u001b[0m \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39;49mexecuting_eagerly():\n\u001b[1;32m    276\u001b[0m   \u001b[39mif\u001b[39;00m trace\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_episodes = 5000\n",
        "step_per_train = 10\n",
        "verbose = True\n",
        "verbose_num = 10\n",
        "act_after_batch = True\n",
        "\n",
        "scores, steps = agent.train(\n",
        "    env=env, \n",
        "    num_episodes=num_episodes,\n",
        "    step_per_train=step_per_train,\n",
        "    verbose=verbose, \n",
        "    verbose_num=verbose_num,  \n",
        "    act_after_batch=act_after_batch,\n",
        ")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "agent.act(env)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'scores' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(scores, np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(scores)))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(scores, np.arange(len(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "step = 1\n",
        "done = False\n",
        "states = env.reset().reshape(1, 1, -1)\n",
        "positions = agent.int_to_bin(step).reshape(1, 1, -1)\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    # Generating actions for a given group of states \n",
        "    # gets the action\n",
        "    action = agent.actor.target_predict(states, positions)\n",
        "    # Gets the last action only\n",
        "    action = action[0, -1, :]\n",
        "    # clip the action to be between min and max values\n",
        "    action = np.clip(action, a_min=action_min, a_max=action_max)\n",
        "    action[np.isnan(action)] = 0\n",
        "    # Apply the action in the environment\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    step += 1\n",
        "    # Append the new state to the states history\n",
        "    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "    positions = tf.concat((positions, agent.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
