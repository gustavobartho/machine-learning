{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8WT_0Y-KqQdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, BatchNormalization, LayerNormalization, Lambda\n",
        "from tensorflow.keras.initializers import RandomNormal, Zeros\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vQX1S3qQdc"
      },
      "source": [
        "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeO6YvRbqQdf",
        "outputId": "98286af3-47a0-4c4a-8c82-c041308ccc0c"
      },
      "outputs": [],
      "source": [
        "#Ornstein-Uhlenbeck Noise \n",
        "class OUActionNoise(object):\n",
        "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.4, x0=None):\n",
        "        self.mean = mean\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        self.x_prev = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ahtUWGtJqQdh"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "a = np.zeros(4)\n",
        "b = OUActionNoise(a)\n",
        "a += b()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-n5JPCPqQdi",
        "outputId": "a2773b70-2744-4aa3-c777-6acd96c63981"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, batch_size):\n",
        "        '''\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            batch_size (integer): The batch size.\n",
        "            block_size (integer): \n",
        "        '''\n",
        "        self.buffer = [[]]\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = size\n",
        "        \n",
        "    #--------------------------------------------------    \n",
        "    def append(self, steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        if self.size >= self.max_size: del self.buffer[0]\n",
        "        for step in steps: self.buffer[-1].append(step)\n",
        "        # if done create new episode entry\n",
        "        # (state, action, reward, done)\n",
        "        if (steps[-1]['done']): self.buffer.append([])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    #--------------------------------------------------    \n",
        "    def getEpisodes(self):\n",
        "        prob_diff = 1e-2\n",
        "        probs = tf.nn.softmax(np.arange(1-prob_diff, 1, (prob_diff)/(self.size - 1))[:(self.size - 1)])\n",
        "        episodes = np.random.choice(\n",
        "            np.arange(self.size - 1), #don't chose the current step\n",
        "            size=(self.batch_size,), \n",
        "            replace=True\n",
        "        )\n",
        "        return  [self.buffer[episode] for episode in episodes]\n",
        "    \n",
        "    #--------------------------------------------------  \n",
        "    @property  \n",
        "    def size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Number of elements in the buffer\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "\n",
        "    #--------------------------------------------------  \n",
        "    @property \n",
        "    def hasMinLength(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Boolean indicating if the memory have the minimum number of elements or not\n",
        "        '''\n",
        "        return (self.size >= 8)\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    @property  \n",
        "    def data(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            List with all the elements in the buffer\n",
        "        '''\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en0TDn5qQdl",
        "outputId": "c485ce90-2d8c-4f0a-cc48-6efad0b8233c"
      },
      "outputs": [],
      "source": [
        "gpt_kernel_initializer = lambda: RandomNormal(mean=0.0, stddev=0.05)\n",
        "gpt_bias_initializer = lambda: Zeros()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "vxHdLrUWqQdm"
      },
      "outputs": [],
      "source": [
        "# Individual Head of self-attention\n",
        "class Head(Layer):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, batch_size, block_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # key, query and value layers\n",
        "        self.key = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.query = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.value = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout_v = Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        state_emb, global_pos_emb, local_pos_emb = inp[0], inp[1], inp[2]\n",
        "        B, T, C = state_emb.shape\n",
        "        if(B is None): B = self.batch_size \n",
        "        if(T is None): T = self.block_size\n",
        "        if(C is None): C = self.state_dim\n",
        "\n",
        "        k = self.key(global_pos_emb)   # (B,T,C)\n",
        "        #q = self.query(global_pos_emb) # (B,T,C)\n",
        "        \n",
        "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
        "        wei =  tf.matmul(k, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
        "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "\n",
        "        v = self.value(state_emb) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        out = self.dropout_v(out)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        #state_emb = global_pos_emb = local_pos_emb = embedding_dim\n",
        "        state_emb, global_pos_emb, local_pos_emb = input_shape\n",
        "        self.value.build(state_emb)\n",
        "        self.key.build(global_pos_emb)\n",
        "        self.query.build(local_pos_emb)\n",
        "        super(Head, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "46rkRg5nqQdn"
      },
      "outputs": [],
      "source": [
        "# Layer with multiple self-attention Heads for data communication \n",
        "class MultiHeadAttention(Layer):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, batch_size, block_size, embedding_dim, num_heads, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.heads = []\n",
        "        for _ in range(num_heads):\n",
        "            head = Head(\n",
        "                batch_size=batch_size,\n",
        "                block_size=block_size,\n",
        "                head_size=head_size,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            head.build(((None, None, embedding_dim), (None, None, embedding_dim), (None, None, embedding_dim)))\n",
        "            self.heads.append(head)\n",
        "        \n",
        "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
        "        self.proj = Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        # concatenate the heads outputs in the C dimension\n",
        "        out =  tf.concat([h(inp) for h in self.heads], axis=-1)\n",
        "        # apply thE projection and dropout\n",
        "        out = self.dropout(self.proj(out)) if self.num_heads > 1 else self.dropout(out)\n",
        "        return out\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        super(MultiHeadAttention, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "VGMjfU_IqQdo"
      },
      "outputs": [],
      "source": [
        "#Simple feed forward for data computation\n",
        "class FeedForward(Layer):\n",
        "    def __init__(self, embedding_dim, dropout, resize_to_input_dim=True, spread_dim=None):\n",
        "        # resize_to_input_dim -> Should be False only to last block element when posterior computation is gonna happen so it doesn't need to output embedding_dim sized elements to another block\n",
        "        # spread_dim -> the heads output comes concatenated in sequence so is computed and joint by the spread layer layer\n",
        "        super().__init__()\n",
        "        last_layer = [\n",
        "            Dense(embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), \n",
        "            Dropout(dropout)\n",
        "        ] if resize_to_input_dim else []\n",
        "        \n",
        "        self.net = Sequential([\n",
        "            Dense(spread_dim if spread_dim is not None else 4 * embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
        "            Dropout(dropout),\n",
        "            *last_layer\n",
        "        ])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YnyFKuPEqQdp"
      },
      "outputs": [],
      "source": [
        "# Block containing a multi head attention module and a feed forward linear computation\n",
        "class Block(Layer):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, batch_size, block_size, emb_dim, num_heads, dropout, resize_to_input_dim=None, spread_dim=None):\n",
        "        super().__init__()\n",
        "        self.resize_to_input_dim = resize_to_input_dim\n",
        "        head_size = emb_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
        "        \n",
        "        self.sa = MultiHeadAttention(batch_size, block_size, emb_dim, num_heads, head_size, dropout)\n",
        "        self.sa_ln = LayerNormalization()\n",
        "\n",
        "        self.ffwd = FeedForward(emb_dim, dropout, resize_to_input_dim, spread_dim)\n",
        "        self.ffwd_ln = LayerNormalization()\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        st_emp, global_pos_emb, local_pos_emb = inp[0], inp[1], inp[2]\n",
        "\n",
        "        # Multi head attention with layer norm\n",
        "        x = st_emp + self.sa([st_emp, global_pos_emb, local_pos_emb])\n",
        "        x = self.sa_ln(x)\n",
        "        \n",
        "        # feed forward with layer norm\n",
        "        x = (x + self.ffwd(x)) if self.resize_to_input_dim else self.ffwd(x)\n",
        "        x = self.ffwd_ln(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        st_emb, global_pos_emb, local_pos_emb = input_shape\n",
        "        self.sa.build(input_shape)\n",
        "        self.ffwd.build(st_emb)\n",
        "        super(Block, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_value = lambda : RandomNormal(mean=0.0, stddev=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "V4uZbSagqQdq"
      },
      "outputs": [],
      "source": [
        "class GPTModel(Model):\n",
        "    def __init__(self, n_layer, batch_size, block_size, embedding_dim, out_dim, num_heads, dropout, ffw):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.state_embedding = Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.action_embedding = Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        \n",
        "        self.blocks = []\n",
        "        for i in range(n_layer):\n",
        "            block = Block(batch_size, block_size, embedding_dim, num_heads, dropout,\n",
        "                resize_to_input_dim = (i != n_layer - 1 ),  \n",
        "                spread_dim = out_dim if (i == n_layer - 1 ) else None,\n",
        "            )\n",
        "            block.build(((None, None, embedding_dim), (None, None, embedding_dim), (None, None, embedding_dim)))\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.ffw = ffw\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def get_local_position_sin_encoding(self, batch_size, block_size, n=10000):\n",
        "        positions = np.tile([np.arange(block_size)], [batch_size, 1])\n",
        "        batch_size, block_size = positions.shape[:2]\n",
        "        aux = np.tile(np.tile([np.arange(self.embedding_dim)], [block_size, 1]), [batch_size, 1, 1])\n",
        "        denominator = tf.cast(n**((2*(aux//2))/self.embedding_dim), dtype=tf.float32)\n",
        "        val = tf.cast(np.tile(positions, [1, 1, self.embedding_dim]), dtype=tf.float32)\n",
        "        P = (np.sin(val/denominator) * ((aux + 1)%2)) + (np.cos(val/denominator)*(aux%2))\n",
        "        return P\n",
        "  \n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        states, global_positions, actions = inp[0], inp[1], inp[2]\n",
        "        B, T, C = states.shape\n",
        "        if(T is None): T = self.block_size\n",
        "        if(B is None): B = self.batch_size\n",
        "\n",
        "        #local_position = self.get_local_position_sin_encoding(batch_size=B, block_size=T)\n",
        "        act_emb = self.action_embedding(actions)\n",
        "        st_emb = self.state_embedding(states)\n",
        "        \n",
        "        for block in self.blocks: st_emb = block((st_emb, act_emb+global_positions, global_positions))\n",
        "        logits = self.ffw(st_emb)\n",
        "\n",
        "        return logits\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def generate(self, states, positions, actions):\n",
        "        # crop idx to the last block_size tokens\n",
        "        st_cond = states[:, -self.block_size:, :]\n",
        "        pos_cond = positions[:, -self.block_size:, :]\n",
        "        act_cond = actions[:, -self.block_size:, :]\n",
        "        # get the predictions\n",
        "        actions = self([st_cond, pos_cond, act_cond])\n",
        "        # focus only on the last time step\n",
        "        return actions\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        states, positions, actions = input_shape\n",
        "        self.action_embedding.build(actions)\n",
        "        self.state_embedding.build(states)\n",
        "        super(GPTModel, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdauE0A-qQdr",
        "outputId": "f775f548-57dc-43a6-877b-8cf35079d9a6"
      },
      "outputs": [],
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, embedding_dim, num_heads, dropout, action_range, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        ffw = lambda: Sequential([\n",
        "            Dense(32,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            LayerNormalization(),\n",
        "            Dropout(dropout),\n",
        "            Dense(action_dim, activation='tanh', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            Lambda(lambda i: i * action_range, dtype='float64'),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, state_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, state_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, positions, actions):\n",
        "        return self.model.generate(states, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, positions, actions):\n",
        "        return self.target_model.generate(states, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def act(self, states, positions, actions):\n",
        "        action = self.predict(states, positions, actions)\n",
        "        # Gets the last action only\n",
        "        action = action[0, -1, :]\n",
        "        return action\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_actor_model.h5')\n",
        "        self.target_model.save(path + '_actor_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = load_model(path)\n",
        "        self.model = load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iws_SlRZqQds",
        "outputId": "30dfbb02-a02f-4373-fb58-8fd4db3b5774"
      },
      "outputs": [],
      "source": [
        "class Critic(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, embedding_dim, out_dim, num_heads, dropout, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim + action_dim\n",
        "        ffw = lambda: Sequential([\n",
        "                Dense(32,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "                LayerNormalization(),\n",
        "                Dropout(dropout),\n",
        "                Dense(out_dim, activation='linear', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, self.inp_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, self.inp_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, next_actions, positions, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        next_actions = tf.cast(next_actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        inp = tf.concat([states, next_actions], 2)\n",
        "        return self.model.generate(inp, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, next_actions, positions, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        next_actions = tf.cast(next_actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        inp = tf.concat([states, next_actions], 2)\n",
        "        return self.target_model.generate(inp, positions, actions)\n",
        "    \n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_critic_model.h5')\n",
        "        self.target_model.save(path + '_critic_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = load_model(path)\n",
        "        self.model = load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "n2y8GZ7tqQds"
      },
      "outputs": [],
      "source": [
        "class DDPG_GPT_Agent(object):\n",
        "    def __init__(self, a_n_layers, c_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, c_n_heads, \n",
        "        dropout, action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
        "        epsilon_min, a_embedding_dim, c_embedding_dim, gamma_grow, gamma_max\n",
        "        ):\n",
        "        \n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_min = action_min\n",
        "        self.action_max = action_max\n",
        "        self.gamma_grow = gamma_grow\n",
        "        self.gamma_max = gamma_max\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.a_embedding_dim = a_embedding_dim\n",
        "        self.c_embedding_dim = c_embedding_dim\n",
        "\n",
        "        self.episode_batch_size = batch_size\n",
        "        self.steps_batch_size = 1\n",
        "\n",
        "        #Creates the Replay Buffer\n",
        "        self.memory = ReplayBuffer(memory_size, self.episode_batch_size)\n",
        "\n",
        "        #Creates the noise generator\n",
        "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
        "\n",
        "        #Creates the actor\n",
        "        self.actor = Actor(\n",
        "            n_layer=a_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            embedding_dim=a_embedding_dim,\n",
        "            num_heads=a_n_heads, \n",
        "            dropout=dropout, \n",
        "            action_range=action_max, \n",
        "            lr=a_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "        \n",
        "        #Creates the critic\n",
        "        self.critic = Critic(\n",
        "            n_layer=c_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            embedding_dim=c_embedding_dim,\n",
        "            out_dim=1,\n",
        "            num_heads=c_n_heads, \n",
        "            dropout=dropout, \n",
        "            lr=c_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "    \n",
        "    #--------------------------------------------------------------------     \n",
        "    def act(self, env):\n",
        "        action = np.zeros(self.action_dim)\n",
        "        state = env.reset()\n",
        "        step = np.array([1])\n",
        "\n",
        "        actions = action.reshape(1, 1, -1)\n",
        "        states = state.reshape(1, 1, -1)\n",
        "        positions = step.reshape(1, 1, -1)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action = self.policy(states, positions, actions, explore=False)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            step += 1\n",
        "\n",
        "            states = tf.concat((states, state.reshape(1, 1, -1)), axis=1)\n",
        "            positions = tf.concat((positions, np.array([step]).reshape(1, 1, -1)), axis=1)\n",
        "            actions = tf.concat((actions, action.reshape(1, 1, -1)), axis=1)\n",
        "        \n",
        "        return\n",
        "    \n",
        "    #-------------------------------------------------------------------- \n",
        "    def get_position_sin_encoding(self, embedding_dim, positions, n=100):\n",
        "        batch_size, block_size = positions.shape[:2]\n",
        "        aux = np.tile(np.tile([np.arange(embedding_dim)], [block_size, 1]), [batch_size, 1, 1])\n",
        "        denominator = tf.cast(n**((2*(aux//2))/embedding_dim), dtype=tf.float32)\n",
        "        val = tf.cast(np.tile(positions, [1, 1, embedding_dim]), dtype=tf.float32)\n",
        "        P = (np.sin(val/denominator) * ((aux + 1)%2)) + (np.cos(val/denominator)*(aux%2))\n",
        "        return P\n",
        "\n",
        "    #-------------------------------------------------------------------- \n",
        "    def policy(self, states, positions, actions, explore=True):\n",
        "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
        "        # gets the action\n",
        "        action = self.actor.act(states, self.get_position_sin_encoding(self.a_embedding_dim, positions), actions)\n",
        "        # takes the exploration with the epsilon probability\n",
        "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
        "        # clip the action to be between min and max values\n",
        "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
        "        action[np.isnan(action)] = 0\n",
        "\n",
        "        return action   \n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def record_memories(self, steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        self.memory.append(steps)\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def learn(self, memory_steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
        "        self.record_memories(memory_steps)\n",
        "        if self.memory.hasMinLength: self.replay_memory()\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def episode_to_batch(self, episode):\n",
        "        #episode = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        if len(episode) > (self.block_size + self.steps_batch_size):\n",
        "            steps_idxs = np.random.choice(np.arange(self.block_size, len(episode)), size=self.steps_batch_size-1, replace=False)\n",
        "            steps_idxs = np.append(steps_idxs, len(episode))\n",
        "        else: steps_idxs = np.arange(self.block_size, len(episode))\n",
        "        \n",
        "        batch = np.array([episode[i-self.block_size:i] for i in steps_idxs])\n",
        "        #batch = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        return batch\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def null_step(self, step):\n",
        "        #step = {'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}\n",
        "        step['reward'] = 0\n",
        "        step['done'] = True\n",
        "        return step\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def episode_pad(self, episode):\n",
        "        #episode = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        return np.concatenate((episode, [self.null_step(episode[-1]) for _ in range(self.block_size - len(episode) + 1)]))\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def get_episodes_batches(self, episodes):\n",
        "        #episodes = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        batch = None\n",
        "\n",
        "        join_episode = lambda final_value, aux_value: aux_value if final_value is None else np.concatenate((final_value, aux_value))\n",
        "\n",
        "        for episode in episodes:\n",
        "            if len(episode) <= self.block_size: episode = self.episode_pad(episode)\n",
        "            ep_batch = self.episode_to_batch(episode)\n",
        "            batch = join_episode(batch, ep_batch)\n",
        "\n",
        "        return batch\n",
        "\n",
        "    #--------------------------------------------------------------------    \n",
        "    def replay_memory(self):\n",
        "        \"\"\" Replay a batch of memories \"\"\"\n",
        "\n",
        "        # Get sample block from the replay buffer\n",
        "        episodes = self.memory.getEpisodes()\n",
        "        batch = self.get_episodes_batches(episodes)\n",
        "        #batch = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        to_tensor = lambda value: tf.convert_to_tensor(value, dtype='float32')\n",
        "        get_batch_element = lambda key, batch: to_tensor([[step[key] for step in block] for block in batch])\n",
        "        \n",
        "        positions = tf.expand_dims(get_batch_element('step', batch), axis=-1)\n",
        "        next_positions_actor = self.get_position_sin_encoding(self.a_embedding_dim, positions + 1)\n",
        "        next_positions_critic = self.get_position_sin_encoding(self.c_embedding_dim, positions + 1)\n",
        "        positions_actor = self.get_position_sin_encoding(self.a_embedding_dim, positions)\n",
        "        positions_critic = self.get_position_sin_encoding(self.c_embedding_dim, positions)\n",
        "\n",
        "        states = get_batch_element('state', batch)\n",
        "        next_states = get_batch_element('next_state', batch)\n",
        "\n",
        "        prev_actions = get_batch_element('prev_action', batch)  \n",
        "        actions = get_batch_element('action', batch)\n",
        "\n",
        "        rewards = tf.expand_dims(get_batch_element('reward', batch), axis=-1)\n",
        "        done = tf.expand_dims(get_batch_element('done', batch), axis=-1)\n",
        "\n",
        "        #Train the critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Compute the actor target actions\n",
        "            target_actions = self.actor.target_predict(next_states, next_positions_actor, actions)\n",
        "            # Compute the critic target values \n",
        "            predicted_return = self.critic.target_predict(next_states, target_actions, next_positions_critic, actions)\n",
        "            # The return for the last block element\n",
        "            last_return = predicted_return[:, -1, :]\n",
        "\n",
        "            # Compute the gamma tensor based on the block step\n",
        "            gamma_values = lambda i: tf.expand_dims(tf.repeat([[self.gamma**(k - i) for k in range(i, rewards.shape[1])]], repeats=rewards.shape[0], axis=0), axis=-1)\n",
        "            # Compute the gamma weighted reward for a given block step\n",
        "            weighted_next_rewards = lambda i: tf.math.reduce_sum(rewards[:, i:, :] * gamma_values(i), axis=1)\n",
        "            # The gamma weight for the last return bootstrap\n",
        "            last_return_weight = lambda i: self.gamma ** (rewards.shape[1] - i)\n",
        "            # Compute the done value for a block step\n",
        "            state_done = lambda i: 1 - done[:, i, :]\n",
        "            \n",
        "            # Compute the return target values\n",
        "            #y = tf.stack([(weighted_next_rewards(i) + (last_return_weight(i) * last_return * state_done(i))) for i in range(rewards.shape[1])], axis=1)\n",
        "            y = rewards + (self.gamma * predicted_return * (1 - done)) # y = (B, T, 1)\n",
        "            \n",
        "            # Predict the expected reward associated with taking the target predicted action in the state\n",
        "            critic_value = self.critic.predict(states, actions, positions_critic, prev_actions)\n",
        "            # Compute the critic loss  \n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "            \n",
        "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(critic_grad, self.critic.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "        \n",
        "        #Train the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts = self.actor.predict(states, positions_actor, prev_actions)\n",
        "            critic_grads = self.critic.predict(states, acts, positions_critic, prev_actions)\n",
        "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(actor_grad, self.actor.model.trainable_variables)\n",
        "            if grad is not None\n",
        "        )\n",
        "            \n",
        "        #Update the model weights\n",
        "        self.actor.transferWeights()\n",
        "        self.critic.transferWeights() \n",
        "        \n",
        "    #--------------------------------------------------\n",
        "    def print_data(self, verbose, episode, step, score):\n",
        "        if verbose:\n",
        "            print(\"\\r                                                                                                     \", end=\"\")\n",
        "            print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(step)+\"\\tReward: \"+str(round(score, 2)) ,end=\"\")\n",
        "        return\n",
        "\n",
        "    #--------------------------------------------------------------------     \n",
        "    def train(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
        "        scores_history = []\n",
        "        steps_history = []\n",
        "        complete = 0\n",
        "        print(\"BEGIN\\n\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            done = False\n",
        "            score, step = 0, 1\n",
        "            state = env.reset()\n",
        "            prev_action = np.zeros(self.action_dim)\n",
        "\n",
        "            states = state.reshape(1, 1, -1)\n",
        "            positions = np.array([step]).reshape(1, 1, -1)\n",
        "            actions = prev_action.reshape(1, 1, -1)\n",
        "            \n",
        "            while not done:\n",
        "                memory_steps = []\n",
        "\n",
        "                while not done and (step % step_per_train != 0):\n",
        "                    action = self.policy(states, positions, actions)\n",
        "                    self.print_data(verbose, episode, step, score)\n",
        "                    new_state, reward, done, _ = env.step(action)\n",
        "                    \n",
        "                    memory_steps.append({\n",
        "                        'step': step, \n",
        "                        'prev_action': prev_action, \n",
        "                        'state': state, \n",
        "                        'action':  action, \n",
        "                        'next_state': new_state, \n",
        "                        'reward': reward, \n",
        "                        'done':  int(done),\n",
        "                    })\n",
        "\n",
        "                    state = new_state\n",
        "                    prev_action = action\n",
        "                    step += 1\n",
        "                    score += reward\n",
        "\n",
        "                    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "                    positions = tf.concat((positions, np.array([step]).reshape(1, 1, -1)), axis=1)\n",
        "                    actions = tf.concat((actions, action.reshape(1, 1, -1)), axis=1)\n",
        "                \n",
        "                step += 1\n",
        "                if len(memory_steps) > 0: self.learn(memory_steps)\n",
        "                self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
        "                self.gamma = min(self.gamma_max, self.gamma*self.gamma_grow)\n",
        "\n",
        "            scores_history.append(score)\n",
        "            steps_history.append(step)\n",
        "            \n",
        "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
        "            if(score >= complete_value):\n",
        "                complete += 1\n",
        "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
        "                if end_on_complete and complete >= complete_num: break\n",
        "            \n",
        "            #These information are printed after each verbose_num episodes\n",
        "            if((episode+1)%verbose_num == 0):\n",
        "                print(\"\\r                                                                                                          \", end=\"\")\n",
        "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes, \n",
        "                      \"\\n\\tTotal reward: \", round(np.mean(scores_history[-verbose_num:]), 2), '+/-', round(np.std(scores_history[-verbose_num:]), 2), \n",
        "                      \"\\n\\tNum. steps: \", round(np.mean(steps_history[-verbose_num:]), 2), '+/-', round(np.std(steps_history[-verbose_num:]), 2), \n",
        "                      *[\"\\n\\tCompleted: \", complete] if complete_value != float('inf') else '', \n",
        "                      \"\\n--------------------------\",\n",
        "                    )\n",
        "                \n",
        "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
        "                if act_after_batch: self.act(env)\n",
        "                \n",
        "                #Set the number of completed episodes on the batch to zero\n",
        "                complete = 0\n",
        "\n",
        "        print(\"\\nFINISHED\")\n",
        "        \n",
        "        return scores_history, steps_history\n",
        "    #--------------------------------------------------------------------     \n",
        "    def save(self, path):\n",
        "        self.actor.saveModel(path)\n",
        "        self.critic.saveModel(path)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def load(self, a_path, c_path):\n",
        "        self.actor.loadModel(a_path)\n",
        "        self.critic.loadModel(c_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wS8tg8OqQdt",
        "outputId": "3d5e1dc5-3b35-4c65-b236-f11b903b86d7",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True, max_episode_steps=500)\n",
        "batch_size = 128\n",
        "block_size = 64\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_min = env.action_space.low\n",
        "action_max = env.action_space.high\n",
        "dropout = 0.1\n",
        "memory_size = 500\n",
        "\n",
        "gamma = 0.99\n",
        "gamma_grow = 1\n",
        "gamma_max = 0.99\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.9995\n",
        "epsilon_min = 0.3\n",
        "\n",
        "tau = 4e-4\n",
        "\n",
        "# Actor hyperparameter\n",
        "a_n_layer = 2\n",
        "a_num_heads = 1\n",
        "a_embedding_dim = 8\n",
        "a_learning_rate = 3e-4\n",
        "\n",
        "# Critic hyperparameter\n",
        "c_n_layer = 1\n",
        "c_num_heads = 1\n",
        "c_embedding_dim = 10\n",
        "c_learning_rate = 7e-4\n",
        "\n",
        "agent = DDPG_GPT_Agent(\n",
        "    a_n_layers = a_n_layer,\n",
        "    c_n_layers = c_n_layer, \n",
        "    batch_size = batch_size, \n",
        "    block_size=block_size, \n",
        "    state_dim=state_dim, \n",
        "    action_dim=action_dim, \n",
        "    a_embedding_dim=a_embedding_dim,\n",
        "    c_embedding_dim=c_embedding_dim,\n",
        "    a_n_heads=a_num_heads, \n",
        "    c_n_heads=c_num_heads,\n",
        "    dropout=dropout, \n",
        "    action_min=action_min, \n",
        "    action_max=action_max, \n",
        "    memory_size=memory_size, \n",
        "    gamma=gamma, \n",
        "    a_lr=a_learning_rate, \n",
        "    c_lr=c_learning_rate, \n",
        "    tau=tau, \n",
        "    epsilon=epsilon, \n",
        "    epsilon_decay=epsilon_decay, \n",
        "    epsilon_min=epsilon_min,\n",
        "    gamma_grow=gamma_grow,\n",
        "    gamma_max=gamma_max,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCvb6NiqQdu",
        "outputId": "bb989c21-13bb-441f-bc9a-179631844892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEGIN\n",
            "\n",
            "Episodes:  10 / 500                                                                                       \n",
            "\tTotal reward:  -540.87 +/- 295.19 \n",
            "\tNum. steps:  117.7 +/- 22.29 \n",
            "--------------------------\n",
            "Episodes:  20 / 500                                                                                       \n",
            "\tTotal reward:  -458.81 +/- 120.36 \n",
            "\tNum. steps:  97.9 +/- 14.34 \n",
            "--------------------------\n",
            "Episodes:  30 / 500                                                                                       \n",
            "\tTotal reward:  -281.7 +/- 190.68 \n",
            "\tNum. steps:  102.7 +/- 29.6 \n",
            "--------------------------\n",
            "Episodes:  40 / 500                                                                                       \n",
            "\tTotal reward:  -180.86 +/- 97.29 \n",
            "\tNum. steps:  103.0 +/- 14.04 \n",
            "--------------------------\n",
            "Episodes:  50 / 500                                                                                       \n",
            "\tTotal reward:  -145.7 +/- 64.58 \n",
            "\tNum. steps:  95.7 +/- 15.17 \n",
            "--------------------------\n",
            "Episodes:  60 / 500                                                                                       \n",
            "\tTotal reward:  -127.64 +/- 114.31 \n",
            "\tNum. steps:  154.2 +/- 105.15 \n",
            "--------------------------\n",
            "Episodes:  70 / 500                                                                                       \n",
            "\tTotal reward:  -263.33 +/- 184.94 \n",
            "\tNum. steps:  159.2 +/- 78.46 \n",
            "--------------------------\n",
            "Episodes:  80 / 500                                                                                       \n",
            "\tTotal reward:  -445.94 +/- 237.0 \n",
            "\tNum. steps:  198.5 +/- 60.97 \n",
            "--------------------------\n",
            "Episodes:  90 / 500                                                                                       \n",
            "\tTotal reward:  -487.58 +/- 345.19 \n",
            "\tNum. steps:  279.9 +/- 122.08 \n",
            "--------------------------\n",
            "Episodes:  100 / 500                                                                                      \n",
            "\tTotal reward:  -380.6 +/- 126.1 \n",
            "\tNum. steps:  202.0 +/- 40.5 \n",
            "--------------------------\n",
            "Episodes:  110 / 500                                                                                      \n",
            "\tTotal reward:  -381.61 +/- 131.17 \n",
            "\tNum. steps:  162.4 +/- 33.99 \n",
            "--------------------------\n",
            "Episodes:  120 / 500                                                                                      \n",
            "\tTotal reward:  -489.91 +/- 116.57 \n",
            "\tNum. steps:  127.5 +/- 17.3 \n",
            "--------------------------\n",
            "Episodes:  130 / 500                                                                                      \n",
            "\tTotal reward:  -383.09 +/- 126.07 \n",
            "\tNum. steps:  122.4 +/- 16.29 \n",
            "--------------------------\n",
            "Episodes:  140 / 500                                                                                      \n",
            "\tTotal reward:  -418.67 +/- 114.12 \n",
            "\tNum. steps:  134.5 +/- 8.81 \n",
            "--------------------------\n",
            "Episodes:  150 / 500                                                                                      \n",
            "\tTotal reward:  -431.95 +/- 80.71 \n",
            "\tNum. steps:  144.3 +/- 13.68 \n",
            "--------------------------\n",
            "Episodes:  160 / 500                                                                                      \n",
            "\tTotal reward:  -318.46 +/- 155.57 \n",
            "\tNum. steps:  143.4 +/- 30.59 \n",
            "--------------------------\n",
            "Episodes:  170 / 500                                                                                      \n",
            "\tTotal reward:  -328.07 +/- 78.36 \n",
            "\tNum. steps:  178.8 +/- 61.92 \n",
            "--------------------------\n",
            "Episodes:  180 / 500                                                                                      \n",
            "\tTotal reward:  -243.67 +/- 152.04 \n",
            "\tNum. steps:  142.7 +/- 30.45 \n",
            "--------------------------\n",
            "Episodes:  190 / 500                                                                                      \n",
            "\tTotal reward:  -236.76 +/- 62.82 \n",
            "\tNum. steps:  170.0 +/- 25.5 \n",
            "--------------------------\n",
            "Episodes:  200 / 500                                                                                      \n",
            "\tTotal reward:  -259.79 +/- 74.03 \n",
            "\tNum. steps:  176.1 +/- 17.1 \n",
            "--------------------------\n",
            "Episodes:  210 / 500                                                                                      \n",
            "\tTotal reward:  -213.21 +/- 71.9 \n",
            "\tNum. steps:  175.3 +/- 32.24 \n",
            "--------------------------\n",
            "Episodes:  220 / 500                                                                                      \n",
            "\tTotal reward:  -262.7 +/- 66.14 \n",
            "\tNum. steps:  166.9 +/- 33.58 \n",
            "--------------------------\n",
            "Episodes:  230 / 500                                                                                      \n",
            "\tTotal reward:  -262.42 +/- 57.25 \n",
            "\tNum. steps:  176.6 +/- 46.37 \n",
            "--------------------------\n",
            "Episodes:  240 / 500                                                                                      \n",
            "\tTotal reward:  -245.22 +/- 84.98 \n",
            "\tNum. steps:  144.7 +/- 23.09 \n",
            "--------------------------\n",
            "Episodes:  250 / 500                                                                                      \n",
            "\tTotal reward:  -238.42 +/- 91.98 \n",
            "\tNum. steps:  135.6 +/- 25.9 \n",
            "--------------------------\n",
            "Episodes:  260 / 500                                                                                      \n",
            "\tTotal reward:  -238.15 +/- 121.99 \n",
            "\tNum. steps:  125.6 +/- 30.56 \n",
            "--------------------------\n",
            "Episodes:  270 / 500                                                                                      \n",
            "\tTotal reward:  -201.8 +/- 101.3 \n",
            "\tNum. steps:  120.1 +/- 27.21 \n",
            "--------------------------\n",
            "Episodes:  280 / 500                                                                                      \n",
            "\tTotal reward:  -194.97 +/- 121.35 \n",
            "\tNum. steps:  162.9 +/- 79.65 \n",
            "--------------------------\n",
            "Episodes:  290 / 500                                                                                      \n",
            "\tTotal reward:  -96.31 +/- 72.51 \n",
            "\tNum. steps:  152.6 +/- 80.62 \n",
            "--------------------------\n",
            "Episodes:  300 / 500                                                                                      \n",
            "\tTotal reward:  -153.82 +/- 133.73 \n",
            "\tNum. steps:  118.1 +/- 28.92 \n",
            "--------------------------\n",
            "Episodes:  310 / 500                                                                                      \n",
            "\tTotal reward:  -130.09 +/- 99.6 \n",
            "\tNum. steps:  132.5 +/- 43.49 \n",
            "--------------------------\n",
            "Episodes:  320 / 500                                                                                      \n",
            "\tTotal reward:  -139.85 +/- 84.4 \n",
            "\tNum. steps:  142.1 +/- 46.61 \n",
            "--------------------------\n",
            "Episodes:  330 / 500                                                                                      \n",
            "\tTotal reward:  -104.88 +/- 102.64 \n",
            "\tNum. steps:  120.8 +/- 25.04 \n",
            "--------------------------\n",
            "Episodes:  340 / 500                                                                                      \n",
            "\tTotal reward:  -116.86 +/- 92.99 \n",
            "\tNum. steps:  122.5 +/- 40.5 \n",
            "--------------------------\n",
            "Episodes:  350 / 500                                                                                      \n",
            "\tTotal reward:  -231.12 +/- 131.7 \n",
            "\tNum. steps:  118.7 +/- 25.58 \n",
            "--------------------------\n",
            "Episodes:  360 / 500                                                                                      \n",
            "\tTotal reward:  -211.56 +/- 108.41 \n",
            "\tNum. steps:  141.3 +/- 112.81 \n",
            "--------------------------\n",
            "Episodes:  370 / 500                                                                                      \n",
            "\tTotal reward:  -140.52 +/- 79.96 \n",
            "\tNum. steps:  150.1 +/- 54.39 \n",
            "--------------------------\n",
            "Episodes:  380 / 500                                                                                      \n",
            "\tTotal reward:  -201.8 +/- 142.15 \n",
            "\tNum. steps:  189.1 +/- 68.46 \n",
            "--------------------------\n",
            "Episodes:  390 / 500                                                                                      \n",
            "\tTotal reward:  -140.43 +/- 77.94 \n",
            "\tNum. steps:  165.3 +/- 12.22 \n",
            "--------------------------\n",
            "Episodes:  400 / 500                                                                                      \n",
            "\tTotal reward:  -231.44 +/- 146.39 \n",
            "\tNum. steps:  169.4 +/- 22.38 \n",
            "--------------------------\n",
            "Episodes:  410 / 500                                                                                      \n",
            "\tTotal reward:  -149.84 +/- 52.44 \n",
            "\tNum. steps:  190.9 +/- 33.47 \n",
            "--------------------------\n",
            "Episodes:  420 / 500                                                                                      \n",
            "\tTotal reward:  -179.69 +/- 65.44 \n",
            "\tNum. steps:  220.9 +/- 63.5 \n",
            "--------------------------\n",
            "Episodes:  430 / 500                                                                                      \n",
            "\tTotal reward:  -309.3 +/- 160.88 \n",
            "\tNum. steps:  230.3 +/- 47.99 \n",
            "--------------------------\n",
            "Episodes:  440 / 500                                                                                      \n",
            "\tTotal reward:  -256.92 +/- 126.7 \n",
            "\tNum. steps:  210.9 +/- 31.98 \n",
            "--------------------------\n",
            "Episodes:  450 / 500                                                                                      \n",
            "\tTotal reward:  -223.91 +/- 110.98 \n",
            "\tNum. steps:  235.4 +/- 62.5 \n",
            "--------------------------\n",
            "Episodes:  460 / 500                                                                                      \n",
            "\tTotal reward:  -248.24 +/- 120.17 \n",
            "\tNum. steps:  212.4 +/- 65.64 \n",
            "--------------------------\n",
            "Episodes:  470 / 500                                                                                      \n",
            "\tTotal reward:  -238.56 +/- 85.95 \n",
            "\tNum. steps:  200.2 +/- 42.33 \n",
            "--------------------------\n",
            "Episodes:  480 / 500                                                                                      \n",
            "\tTotal reward:  -220.16 +/- 107.65 \n",
            "\tNum. steps:  203.0 +/- 43.77 \n",
            "--------------------------\n",
            "Episodes:  490 / 500                                                                                      \n",
            "\tTotal reward:  -271.91 +/- 55.56 \n",
            "\tNum. steps:  237.2 +/- 94.25 \n",
            "--------------------------\n",
            "Episodes:  500 / 500                                                                                      \n",
            "\tTotal reward:  -202.28 +/- 113.97 \n",
            "\tNum. steps:  184.4 +/- 36.03 \n",
            "--------------------------\n",
            "\n",
            "FINISHED\n"
          ]
        }
      ],
      "source": [
        "num_episodes = 500\n",
        "step_per_train = 4\n",
        "verbose = True\n",
        "verbose_num = 10\n",
        "act_after_batch = True\n",
        "\n",
        "scores, steps = agent.train(\n",
        "    env=env, \n",
        "    num_episodes=num_episodes,\n",
        "    step_per_train=step_per_train,\n",
        "    verbose=verbose, \n",
        "    verbose_num=verbose_num,  \n",
        "    act_after_batch=act_after_batch,\n",
        ")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True,  max_episode_steps=500)\n",
        "agent.act(env)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f08023d55d0>]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(len(scores)), scores, color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f080261b4f0>]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(len(steps)), steps, color='g')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
