{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8WT_0Y-KqQdW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-15 14:32:09.890655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-15 14:32:10.035890: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-15 14:32:10.035915: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-04-15 14:32:21.900555: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-15 14:32:21.919409: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-15 14:32:21.919498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers, models, optimizers\n",
        "from scipy.special import softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vQX1S3qQdc"
      },
      "source": [
        "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeO6YvRbqQdf",
        "outputId": "98286af3-47a0-4c4a-8c82-c041308ccc0c"
      },
      "outputs": [],
      "source": [
        "#Ornstein-Uhlenbeck Noise \n",
        "class OUActionNoise(object):\n",
        "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.3, x0=None):\n",
        "        self.mean = mean\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        self.x_prev = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ahtUWGtJqQdh"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "a = np.zeros(15)\n",
        "b = OUActionNoise(a)\n",
        "a += b()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-n5JPCPqQdi",
        "outputId": "a2773b70-2744-4aa3-c777-6acd96c63981"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, batch_size, block_size):\n",
        "        '''\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            batch_size (integer): The batch size.\n",
        "            block_size (integer): \n",
        "        '''\n",
        "        self.buffer = []\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = size\n",
        "        self.block_size = block_size\n",
        "        \n",
        "    #--------------------------------------------------------------------------------    \n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Args:\n",
        "            state (Numpy array): The state.              \n",
        "            action (integer): The action.\n",
        "            reward (float): The reward.\n",
        "            done (boolen): True if the next state is a terminal state and False otherwise. Is transformed to integer so tha True = 1, False = 0\n",
        "            next_state (Numpy array): The next state.           \n",
        "        '''\n",
        "        if self.size == self.max_size:\n",
        "            del self.buffer[0]\n",
        "        self.buffer.append((state, action, reward, next_state, int(done)))\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------    \n",
        "    def getBlock(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
        "        '''\n",
        "        # gets the random indexes of the block start\n",
        "        options = np.arange(self.size - self.block_size)\n",
        "        prob_diff = 1e-4\n",
        "        probs = softmax(np.arange(1-prob_diff, 1, (prob_diff)/len(options))[:len(options)])\n",
        "        \n",
        "        idxs = np.random.choice(options, size=(self.batch_size,), replace=False, p=probs)\n",
        "        get_data = lambda i, idx: [mem[i] for mem in self.buffer[idx:idx+self.block_size]]\n",
        "        \n",
        "        # generate the batch by stacking the blocks\n",
        "        states = np.array(np.stack([get_data(0, idx) for idx in idxs], axis=0))\n",
        "        actions = np.array(np.stack([get_data(1, idx) for idx in idxs], axis=0))\n",
        "        rewards = np.array(np.stack([get_data(2, idx) for idx in idxs], axis=0))\n",
        "        next_states = np.array(np.stack([get_data(3, idx) for idx in idxs], axis=0))\n",
        "        done = np.array(np.stack([get_data(4, idx) for idx in idxs], axis=0))\n",
        "\n",
        "        return  states, actions, rewards, next_states, done\n",
        "    \n",
        "    #--------------------------------------------------------------------------------  \n",
        "    @property  \n",
        "    def size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Number of elements in the buffer\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    @property \n",
        "    def hasMinLength(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Boolean indicating if the memory have the minimum number of elements or not\n",
        "        '''\n",
        "        return (self.size >= (self.batch_size + self.block_size))\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    @property  \n",
        "    def data(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            List with all the elements in the buffer\n",
        "        '''\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JVH2sdrMqQdk"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "# a replay buffer with max lenght of 500, that generates a batch with 32 blocks containing 9 'memory elements' in each \n",
        "rb = ReplayBuffer(500, 32, 8)\n",
        "for i in range(300):\n",
        "    # single state dimension = 5\n",
        "    state = np.random.rand(24)\n",
        "    next_state = np.random.rand(24)\n",
        "    # single action dimension = 4\n",
        "    action = np.random.rand(4)\n",
        "    # rewrd is number\n",
        "    reward = np.random.rand()\n",
        "    # done is a boolean\n",
        "    done = np.random.rand() > 0.5\n",
        "    rb.append(state, action, reward, next_state, done)\n",
        "\n",
        "print(rb.size)\n",
        "print(rb.hasMinLength)\n",
        "print(rb.data[0])\n",
        "\n",
        "st, act, rw, n_st, d = rb.getBlock()\n",
        "print(st.shape)\n",
        "print(act.shape)\n",
        "print(rw.shape)\n",
        "print(n_st.shape)\n",
        "print(d.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en0TDn5qQdl",
        "outputId": "c485ce90-2d8c-4f0a-cc48-6efad0b8233c"
      },
      "outputs": [],
      "source": [
        "gpt_kernel_initializer = lambda: initializers.RandomNormal(mean=0.0, stddev=0.04)\n",
        "gpt_bias_initializer = lambda: initializers.Zeros()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vxHdLrUWqQdm"
      },
      "outputs": [],
      "source": [
        "# Individual Head of self-attention\n",
        "class Head(layers.Layer):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        # key, query and value layers\n",
        "        self.key = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.query = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.value = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        # dropout layer\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        B, T, C = x.shape\n",
        "        if(B is None): B = self.batch_size \n",
        "        if(T is None): T = self.block_size\n",
        "        if(C is None): C = self.state_dim\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
        "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
        "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "46rkRg5nqQdn"
      },
      "outputs": [],
      "source": [
        "# Layer with multiple self-attention Heads for data communication \n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(batch_size, block_size, state_dim, head_size, dropout) for _ in range(num_heads)]\n",
        "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
        "        self.proj = layers.Dense(units=state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        # concatenate the heads outputs in the C dimension\n",
        "        out =  tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        # apply the projection and the dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VGMjfU_IqQdo"
      },
      "outputs": [],
      "source": [
        "#Simple feed forward for data computation\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, state_dim, dropout, last_resize=True, spread_dim=None):\n",
        "        super().__init__()\n",
        "        last_layer = [layers.Dense(state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), layers.Dropout(dropout)] if(last_resize) else []\n",
        "        self.net = models.Sequential([\n",
        "            layers.Dense(spread_dim if spread_dim is not None else 4 * state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
        "            layers.Dropout(dropout),\n",
        "            *last_layer\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YnyFKuPEqQdp"
      },
      "outputs": [],
      "source": [
        "# Block containing a multi head attention module and a feed forward linear computation\n",
        "class Block(layers.Layer):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, dropout, last_resize, spread_dim):\n",
        "        super().__init__()\n",
        "        head_size = state_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
        "        self.sa = MultiHeadAttention(batch_size, block_size, state_dim, num_heads, head_size, dropout)\n",
        "        self.ffwd = FeedForward(state_dim, dropout, last_resize, spread_dim)\n",
        "        self.ln1 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        # Multi head attention with layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # feed forward with layer norm\n",
        "        x = self.ffwd(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_value = lambda x: tf.random_uniform_initializer(- 1 / np.sqrt(x), 1 / np.sqrt(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V4uZbSagqQdq"
      },
      "outputs": [],
      "source": [
        "class GPTModel(models.Model):\n",
        "    def __init__(self, n_layer, batch_size, block_size, input_dim, out_dim, num_heads, dropout, ffw):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.position_embedding_table = layers.Embedding(block_size, input_dim, input_length=None, embeddings_initializer=gpt_kernel_initializer())\n",
        "        self.blocks = models.Sequential([\n",
        "            Block(\n",
        "                batch_size, \n",
        "                block_size, \n",
        "                input_dim, \n",
        "                num_heads, \n",
        "                dropout, \n",
        "                last_resize = (i != n_layer - 1 ),  \n",
        "                spread_dim = out_dim if (i == n_layer - 1 ) else None,\n",
        "            )for i in range(n_layer)\n",
        "        ])\n",
        "        self.ffw = ffw\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        B, T, C = inputs.shape\n",
        "        if(T is None): T = self.block_size\n",
        "        pos_emb = self.position_embedding_table(tf.range(tf.constant(T), dtype=tf.int32))\n",
        "        x = inputs + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.ffw(x)\n",
        "        return logits\n",
        "    \n",
        "\n",
        "    def generate(self, states):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = states[:, -self.block_size:, :]\n",
        "        # get the predictions\n",
        "        actions = self(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdauE0A-qQdr",
        "outputId": "f775f548-57dc-43a6-877b-8cf35079d9a6"
      },
      "outputs": [],
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, num_heads, dropout, action_range, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "              layers.Dense(128,  activation='relu', kernel_initializer=f_value(128), bias_initializer=f_value(128)),\n",
        "              layers.Dropout(dropout),\n",
        "              layers.BatchNormalization(),\n",
        "              layers.Dense(32,  activation='relu', kernel_initializer=f_value(32), bias_initializer=f_value(32)),\n",
        "              layers.Dropout(dropout),\n",
        "              layers.BatchNormalization(),\n",
        "              layers.Dense(action_dim, activation='tanh', kernel_initializer=f_value(1/(0.03**2)), bias_initializer=f_value(1/(0.03**2))),\n",
        "              layers.Lambda(lambda i: i * action_range, dtype='float64'),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            input_dim=state_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build((None, None, state_dim))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            input_dim=state_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build((None, None, state_dim))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states):\n",
        "        return self.model.generate(states)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states):\n",
        "        return self.target_model.generate(states)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def act(self, states):\n",
        "        action = self.predict(states)\n",
        "        # Gets the last action only\n",
        "        action = action[0, -1, :]\n",
        "        return action\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_actor_model.h5')\n",
        "        self.target_model.save(path + '_actor_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iws_SlRZqQds",
        "outputId": "30dfbb02-a02f-4373-fb58-8fd4db3b5774"
      },
      "outputs": [],
      "source": [
        "class Critic(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, out_dim, num_heads, dropout, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim + action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "              layers.Dense(128,  activation='relu', kernel_initializer=f_value(128), bias_initializer=f_value(128)),\n",
        "              layers.Dropout(dropout),\n",
        "              layers.BatchNormalization(),\n",
        "              layers.Dense(64,  activation='relu', kernel_initializer=f_value(64), bias_initializer=f_value(64)),\n",
        "              layers.Dropout(dropout),\n",
        "              layers.BatchNormalization(),\n",
        "              layers.Dense(16,  activation='relu', kernel_initializer=f_value(16), bias_initializer=f_value(16)),\n",
        "              layers.Dropout(dropout),\n",
        "              layers.BatchNormalization(),\n",
        "              layers.Dense(out_dim, activation='linear', kernel_initializer=f_value(1/(0.03**2)), bias_initializer=f_value(1/(0.03**2))),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            input_dim=self.inp_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build((None, None, self.inp_dim))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            input_dim=self.inp_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build((None, None, self.inp_dim))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32) \n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.model.generate(inp)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32) \n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.target_model.generate(inp)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_critic_model.h5')\n",
        "        self.target_model.save(path + '_critic_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n2y8GZ7tqQds"
      },
      "outputs": [],
      "source": [
        "class DDPG_GPT_Agent(object):\n",
        "    def __init__(self, a_n_layers, c_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, c_n_heads, \n",
        "                 dropout, action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
        "                 epsilon_min\n",
        "                ):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_min = action_min\n",
        "        self.action_max = action_max\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        #Creates the Replay Buffer\n",
        "        self.memory = ReplayBuffer(memory_size, batch_size, block_size)\n",
        "\n",
        "        #Creates the noise generator\n",
        "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
        "\n",
        "        #Creates the actor\n",
        "        self.actor = Actor(\n",
        "            n_layer=a_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            num_heads=a_n_heads, \n",
        "            dropout=dropout, \n",
        "            action_range=action_max, \n",
        "            lr=a_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "        \n",
        "        #Creates the critic\n",
        "        self.critic = Critic(\n",
        "            n_layer=c_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            out_dim=1,\n",
        "            num_heads=c_n_heads, \n",
        "            dropout=dropout, \n",
        "            lr=c_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "    \n",
        "    #--------------------------------------------------------------------     \n",
        "    def act(self, env):\n",
        "        states = env.reset().reshape(1, 1, -1)\n",
        "        done = False\n",
        "        while not done:\n",
        "            env.render()\n",
        "            # Generating actions for a given group of states \n",
        "            action = self.policy(states, explore=False)\n",
        "            # Apply the action in the environment\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            # Append the new state to the states history\n",
        "            states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "        \n",
        "    #-------------------------------------------------------------------- \n",
        "    def policy(self, states, explore=True):\n",
        "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
        "        # gets the action\n",
        "        action = self.actor.act(states)\n",
        "        # takes the exploration with the epsilon probability\n",
        "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
        "        # clip the action to be between min and max values\n",
        "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
        "        action[np.isnan(action)] = 0\n",
        "        return action\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
        "        mem_fix = 0.9\n",
        "        if (np.random.rand() <= mem_fix) or (reward >=0) or (reward <= -1): self.memory.append(state, action, reward, next_state, done)\n",
        "        if self.memory.hasMinLength: self.replay_memory()\n",
        "        return\n",
        "        \n",
        "    #--------------------------------------------------------------------    \n",
        "    def replay_memory(self):\n",
        "        \"\"\" Replay a batch of memories \"\"\"\n",
        "\n",
        "        # Get sample block from the replay buffer\n",
        "        states, actions, rewards, next_states, done = self.memory.getBlock()\n",
        "        \n",
        "        states = tf.convert_to_tensor(states, dtype='float32')\n",
        "        actions = tf.convert_to_tensor(actions, dtype='float32')\n",
        "        next_states = tf.convert_to_tensor(next_states, dtype='float32')\n",
        "\n",
        "        rewards = tf.convert_to_tensor(rewards, dtype='float32')\n",
        "        rewards = tf.reshape(rewards, (*rewards.shape, 1))\n",
        "\n",
        "        done = tf.convert_to_tensor(done, dtype='float32')\n",
        "        done = tf.reshape(done, (*done.shape, 1))\n",
        "\n",
        "        #Train the critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Compute the critic target values\n",
        "            target_actions = self.actor.target_predict(next_states)\n",
        "            # target actions = (1, T, A_C) // next_states = (1, T, S_C) - B = 1 because the Critic is an MLP so each block is treated as a batch\n",
        "            # Compute the 'actual' reward by getting the expected return from the next state \n",
        "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions) * (1 - done) # y = (B, T, 1)\n",
        "            # Predict the expected reward associated with taking the target predicted action in the state\n",
        "            critic_value = self.critic.predict(states, actions)\n",
        "            # Compute the critic loss \n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
        "        \n",
        "        #Train the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts = self.actor.predict(states)\n",
        "            critic_grads = self.critic.predict(states, acts)\n",
        "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
        "            \n",
        "        #Update the model weights\n",
        "        self.actor.transferWeights()\n",
        "        self.critic.transferWeights() \n",
        "            \n",
        "        #Decay the epsilon value\n",
        "        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
        "        #If its reach the minimum value it stops\n",
        "        else: self.epsilon = self.epsilon_min\n",
        "\n",
        "    #--------------------------------------------------------------------     \n",
        "    def train(self, env, num_episodes, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
        "        scores_history = []\n",
        "        steps_history = []\n",
        "        complete = 0\n",
        "        print(\"BEGIN\\n\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            done = False\n",
        "            score, steps = 0, 0\n",
        "            state = env.reset()\n",
        "            states = state.reshape(1, 1, -1)\n",
        "            \n",
        "            while not done:\n",
        "                action = self.policy(states)\n",
        "                \n",
        "                if verbose:\n",
        "                    print(\"\\r                                                                                                     \", end=\"\")\n",
        "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score) ,end=\"\")\n",
        "                    \n",
        "                new_state, reward, done, _ = env.step(action)\n",
        "                self.learn(state, action, reward, new_state, done)\n",
        "                state = new_state\n",
        "                states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "                score += reward\n",
        "                steps += 1\n",
        "\n",
        "            scores_history.append(score)\n",
        "            steps_history.append(steps)\n",
        "            \n",
        "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
        "            if(score >= complete_value):\n",
        "                complete += 1\n",
        "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
        "                if end_on_complete and complete >= complete_num: break\n",
        "            \n",
        "            #These information are printed after each verbose_num episodes\n",
        "            if((episode+1)%verbose_num == 0):\n",
        "                print(\"\\r                                                                                                          \", end=\"\")\n",
        "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes, \n",
        "                      \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_num:]), \n",
        "                      \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_num:]), \n",
        "                      *[\"\\n\\tCompleted: \", complete] if complete_value != float('inf') else '', \n",
        "                      \"\\n--------------------------\",\n",
        "                    )\n",
        "                \n",
        "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
        "                if act_after_batch: self.act(env)\n",
        "                \n",
        "                #Set the number of completed episodes on the batch to zero\n",
        "                complete = 0\n",
        "\n",
        "        print(\"\\nFINISHED\")\n",
        "        \n",
        "        return scores_history, steps_history\n",
        "    #--------------------------------------------------------------------     \n",
        "    def save(self, path):\n",
        "        self.actor.saveModel(path)\n",
        "        self.critic.saveModel(path)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def load(self, a_path, c_path):\n",
        "        self.actor.loadModel(a_path)\n",
        "        self.critic.loadModel(c_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wS8tg8OqQdt",
        "outputId": "3d5e1dc5-3b35-4c65-b236-f11b903b86d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n",
            "2023-04-15 14:33:04.366924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-15 14:33:04.367578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-15 14:33:04.381912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-15 14:33:04.386219: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-15 14:33:08.089211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-15 14:33:08.149203: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-04-15 14:33:08.214143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_min = env.action_space.low\n",
        "action_max = env.action_space.high\n",
        "dropout = 0.05\n",
        "memory_size = 500000\n",
        "gamma = 0.995\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.999\n",
        "epsilon_min = 0.4\n",
        "tau = 5e-4\n",
        "\n",
        "# Actor hyperparameter\n",
        "a_n_layer = 1\n",
        "a_num_heads = 2\n",
        "a_learning_rate = 2e-4\n",
        "\n",
        "\n",
        "# Critic hyperparameter\n",
        "c_n_layer = 1\n",
        "c_num_heads = 2\n",
        "c_learning_rate = 4e-4\n",
        "\n",
        "agent = DDPG_GPT_Agent(\n",
        "    a_n_layers = a_n_layer,\n",
        "    c_n_layers = c_n_layer, \n",
        "    batch_size = batch_size, \n",
        "    block_size=block_size, \n",
        "    state_dim=state_dim, \n",
        "    action_dim=action_dim, \n",
        "    a_n_heads=a_num_heads, \n",
        "    c_n_heads=c_num_heads,\n",
        "    dropout=dropout, \n",
        "    action_min=action_min, \n",
        "    action_max=action_max, \n",
        "    memory_size=memory_size, \n",
        "    gamma=gamma, \n",
        "    a_lr=a_learning_rate, \n",
        "    c_lr=c_learning_rate, \n",
        "    tau=tau, \n",
        "    epsilon=epsilon, \n",
        "    epsilon_decay=epsilon_decay, \n",
        "    epsilon_min=epsilon_min,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCvb6NiqQdu",
        "outputId": "bb989c21-13bb-441f-bc9a-179631844892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEGIN\n",
            "\n",
            "Episodes:  2 / 1000                                                                                       \n",
            "\tTotal reward:  -280.6171961950145 \n",
            "\tNum. steps:  93.5 \n",
            "--------------------------\n",
            "Episodes:  4 / 1000                                                                                       \n",
            "\tTotal reward:  -482.18797753544436 \n",
            "\tNum. steps:  72.5 \n",
            "--------------------------\n",
            "Episodes:  6 / 1000                                                                                       \n",
            "\tTotal reward:  -163.65767789493307 \n",
            "\tNum. steps:  61.0 \n",
            "--------------------------\n",
            "Episodes:  8 / 1000                                                                                       \n",
            "\tTotal reward:  -442.4831985439261 \n",
            "\tNum. steps:  60.5 \n",
            "--------------------------\n",
            "Episodes:  10 / 1000                                                                                      \n",
            "\tTotal reward:  -410.7147251832322 \n",
            "\tNum. steps:  67.5 \n",
            "--------------------------\n",
            "Episodes:  12 / 1000                                                                                      \n",
            "\tTotal reward:  -840.0278173026295 \n",
            "\tNum. steps:  140.0 \n",
            "--------------------------\n",
            "Episodes:  14 / 1000                                                                                      \n",
            "\tTotal reward:  -84.73960109931862 \n",
            "\tNum. steps:  153.5 \n",
            "--------------------------\n",
            "Episodes:  16 / 1000                                                                                      \n",
            "\tTotal reward:  -314.8583984715328 \n",
            "\tNum. steps:  202.0 \n",
            "--------------------------\n",
            "Episodes:  18 / 1000                                                                                      \n",
            "\tTotal reward:  -296.181690447547 \n",
            "\tNum. steps:  636.0 \n",
            "--------------------------\n",
            "Episodes:  20 / 1000                                                                                      \n",
            "\tTotal reward:  -213.44509045360826 \n",
            "\tNum. steps:  310.0 \n",
            "--------------------------\n",
            "Episodes:  22 / 1000                                                                                      \n",
            "\tTotal reward:  -224.50894513615958 \n",
            "\tNum. steps:  266.5 \n",
            "--------------------------\n",
            "Episodes:  24 / 1000                                                                                      \n",
            "\tTotal reward:  -174.19856162346736 \n",
            "\tNum. steps:  214.0 \n",
            "--------------------------\n",
            "Episodes:  26 / 1000                                                                                      \n",
            "\tTotal reward:  -67.53803418196152 \n",
            "\tNum. steps:  239.0 \n",
            "--------------------------\n",
            "Episodes:  28 / 1000                                                                                      \n",
            "\tTotal reward:  -153.23086128770808 \n",
            "\tNum. steps:  351.5 \n",
            "--------------------------\n",
            "Episodes:  30 / 1000                                                                                      \n",
            "\tTotal reward:  -60.50688042527713 \n",
            "\tNum. steps:  268.0 \n",
            "--------------------------\n",
            "Episodes:  32 / 1000                                                                                      \n",
            "\tTotal reward:  -179.59850107093683 \n",
            "\tNum. steps:  832.5 \n",
            "--------------------------\n",
            "Episodes:  34 / 1000                                                                                      \n",
            "\tTotal reward:  -288.79961735333535 \n",
            "\tNum. steps:  356.0 \n",
            "--------------------------\n",
            "Episodes:  36 / 1000                                                                                      \n",
            "\tTotal reward:  -610.5604100089145 \n",
            "\tNum. steps:  95.5 \n",
            "--------------------------\n",
            "Episodes:  38 / 1000                                                                                      \n",
            "\tTotal reward:  -459.70348780821644 \n",
            "\tNum. steps:  108.5 \n",
            "--------------------------\n",
            "Episodes:  40 / 1000                                                                                      \n",
            "\tTotal reward:  -378.1187311785767 \n",
            "\tNum. steps:  115.0 \n",
            "--------------------------\n",
            "Episodes:  42 / 1000                                                                                      \n",
            "\tTotal reward:  -851.1019730330039 \n",
            "\tNum. steps:  184.0 \n",
            "--------------------------\n",
            "Episodes:  44 / 1000                                                                                      \n",
            "\tTotal reward:  -302.4438641811692 \n",
            "\tNum. steps:  107.0 \n",
            "--------------------------\n",
            "Episodes:  46 / 1000                                                                                      \n",
            "\tTotal reward:  -436.98621027241535 \n",
            "\tNum. steps:  123.5 \n",
            "--------------------------\n",
            "Episodes:  48 / 1000                                                                                      \n",
            "\tTotal reward:  -279.3384460104488 \n",
            "\tNum. steps:  96.0 \n",
            "--------------------------\n",
            "Episodes:  50 / 1000                                                                                      \n",
            "\tTotal reward:  -911.3880872616966 \n",
            "\tNum. steps:  124.0 \n",
            "--------------------------\n",
            "Episodes:  52 / 1000                                                                                      \n",
            "\tTotal reward:  -412.9797467859061 \n",
            "\tNum. steps:  137.0 \n",
            "--------------------------\n",
            "Episodes:  54 / 1000                                                                                      \n",
            "\tTotal reward:  -292.3330039548127 \n",
            "\tNum. steps:  159.5 \n",
            "--------------------------\n",
            "Episodes:  56 / 1000                                                                                      \n",
            "\tTotal reward:  -248.98720372553623 \n",
            "\tNum. steps:  118.0 \n",
            "--------------------------\n",
            "Episodes:  58 / 1000                                                                                      \n",
            "\tTotal reward:  -288.7759182969704 \n",
            "\tNum. steps:  197.0 \n",
            "--------------------------\n",
            "Episodes:  60 / 1000                                                                                      \n",
            "\tTotal reward:  -337.4807082492771 \n",
            "\tNum. steps:  150.5 \n",
            "--------------------------\n",
            "Episodes:  62 / 1000                                                                                      \n",
            "\tTotal reward:  -363.4059148214156 \n",
            "\tNum. steps:  155.5 \n",
            "--------------------------\n",
            "Episodes:  64 / 1000                                                                                      \n",
            "\tTotal reward:  -343.3635336408459 \n",
            "\tNum. steps:  108.5 \n",
            "--------------------------\n",
            "Episodes:  66 / 1000                                                                                      \n",
            "\tTotal reward:  -287.992635492379 \n",
            "\tNum. steps:  166.0 \n",
            "--------------------------\n",
            "Episodes:  68 / 1000                                                                                      \n",
            "\tTotal reward:  -311.5132614295878 \n",
            "\tNum. steps:  110.0 \n",
            "--------------------------\n",
            "Episodes:  70 / 1000                                                                                      \n",
            "\tTotal reward:  -270.89224841607233 \n",
            "\tNum. steps:  127.5 \n",
            "--------------------------\n",
            "Episodes:  72 / 1000                                                                                      \n",
            "\tTotal reward:  -215.55987952873386 \n",
            "\tNum. steps:  170.5 \n",
            "--------------------------\n",
            "Episodes:  74 / 1000                                                                                      \n",
            "\tTotal reward:  -212.56013800223178 \n",
            "\tNum. steps:  168.5 \n",
            "--------------------------\n",
            "Episodes:  76 / 1000                                                                                      \n",
            "\tTotal reward:  -79.69064682094967 \n",
            "\tNum. steps:  124.0 \n",
            "--------------------------\n",
            "Episodes:  78 / 1000                                                                                      \n",
            "\tTotal reward:  -227.60663057387194 \n",
            "\tNum. steps:  151.5 \n",
            "--------------------------\n",
            "Episodes:  80 / 1000                                                                                      \n",
            "\tTotal reward:  -128.50052436760933 \n",
            "\tNum. steps:  135.5 \n",
            "--------------------------\n",
            "Episodes:  82 / 1000                                                                                      \n",
            "\tTotal reward:  -253.98725940122984 \n",
            "\tNum. steps:  215.0 \n",
            "--------------------------\n",
            "Episodes:  84 / 1000                                                                                      \n",
            "\tTotal reward:  -133.97849981373062 \n",
            "\tNum. steps:  136.5 \n",
            "--------------------------\n",
            "Episodes:  86 / 1000                                                                                      \n",
            "\tTotal reward:  -279.7923929916947 \n",
            "\tNum. steps:  145.5 \n",
            "--------------------------\n",
            "Episodes:  88 / 1000                                                                                      \n",
            "\tTotal reward:  -227.54062566772518 \n",
            "\tNum. steps:  119.0 \n",
            "--------------------------\n",
            "Episodes:  90 / 1000                                                                                      \n",
            "\tTotal reward:  -241.9193744667488 \n",
            "\tNum. steps:  146.5 \n",
            "--------------------------\n",
            "Episodes:  92 / 1000                                                                                      \n",
            "\tTotal reward:  -198.9949481583442 \n",
            "\tNum. steps:  122.5 \n",
            "--------------------------\n",
            "Episodes:  94 / 1000                                                                                      \n",
            "\tTotal reward:  -175.56213613217682 \n",
            "\tNum. steps:  92.0 \n",
            "--------------------------\n",
            "Episodes:  96 / 1000                                                                                      \n",
            "\tTotal reward:  -121.16608430832254 \n",
            "\tNum. steps:  118.0 \n",
            "--------------------------\n",
            "Episodes:  98 / 1000                                                                                      \n",
            "\tTotal reward:  -447.7967504983277 \n",
            "\tNum. steps:  84.5 \n",
            "--------------------------\n",
            "Episodes:  100 / 1000                                                                                     \n",
            "\tTotal reward:  -135.29620081253543 \n",
            "\tNum. steps:  178.0 \n",
            "--------------------------\n",
            "Episodes:  102 / 1000                                                                                     \n",
            "\tTotal reward:  -227.07829536441372 \n",
            "\tNum. steps:  157.0 \n",
            "--------------------------\n",
            "Episodes:  104 / 1000                                                                                     \n",
            "\tTotal reward:  -132.74192812173033 \n",
            "\tNum. steps:  175.0 \n",
            "--------------------------\n",
            "Episodes:  106 / 1000                                                                                     \n",
            "\tTotal reward:  -319.74954826846425 \n",
            "\tNum. steps:  153.0 \n",
            "--------------------------\n",
            "Episodes:  108 / 1000                                                                                     \n",
            "\tTotal reward:  -214.7459815673643 \n",
            "\tNum. steps:  180.0 \n",
            "--------------------------\n",
            "Episodes:  110 / 1000                                                                                     \n",
            "\tTotal reward:  -231.32957142060548 \n",
            "\tNum. steps:  119.5 \n",
            "--------------------------\n",
            "Episodes:  112 / 1000                                                                                     \n",
            "\tTotal reward:  -135.84814380718987 \n",
            "\tNum. steps:  172.0 \n",
            "--------------------------\n",
            "Episodes:  114 / 1000                                                                                     \n",
            "\tTotal reward:  -504.7822594558776 \n",
            "\tNum. steps:  99.5 \n",
            "--------------------------\n",
            "Episodes:  116 / 1000                                                                                     \n",
            "\tTotal reward:  -311.43530753342515 \n",
            "\tNum. steps:  141.0 \n",
            "--------------------------\n",
            "Episodes:  118 / 1000                                                                                     \n",
            "\tTotal reward:  -127.30090124609623 \n",
            "\tNum. steps:  189.0 \n",
            "--------------------------\n",
            "Episodes:  120 / 1000                                                                                     \n",
            "\tTotal reward:  -329.5718874823606 \n",
            "\tNum. steps:  74.5 \n",
            "--------------------------\n",
            "Episodes:  122 / 1000                                                                                     \n",
            "\tTotal reward:  -272.0444844319613 \n",
            "\tNum. steps:  176.0 \n",
            "--------------------------\n",
            "Episodes:  124 / 1000                                                                                     \n",
            "\tTotal reward:  -244.73606726558415 \n",
            "\tNum. steps:  181.5 \n",
            "--------------------------\n",
            "Episodes:  126 / 1000                                                                                     \n",
            "\tTotal reward:  -281.070915347713 \n",
            "\tNum. steps:  157.5 \n",
            "--------------------------\n",
            "Episodes:  128 / 1000                                                                                     \n",
            "\tTotal reward:  -204.482734153054 \n",
            "\tNum. steps:  82.0 \n",
            "--------------------------\n",
            "Episodes:  130 / 1000                                                                                     \n",
            "\tTotal reward:  -340.70964164496615 \n",
            "\tNum. steps:  176.5 \n",
            "--------------------------\n",
            "Episodes:  132 / 1000                                                                                     \n",
            "\tTotal reward:  -461.68853563722735 \n",
            "\tNum. steps:  205.5 \n",
            "--------------------------\n",
            "Episode: 133\t Step: 44\tReward: 9.109763723707957                                                     "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102: DeprecationWarning: An exception was ignored while fetching the attribute `__array__` from an object of type 'list'.  With the exception of `AttributeError` NumPy will always raise this exception in the future.  Raise this deprecation warning to see the original exception. (Warning added NumPy 1.21)\n",
            "  return ops.EagerTensor(value, ctx.device_name, dtype)\n",
            "Fatal Python error: pygame_parachute: (pygame parachute) Segmentation Fault\n",
            "Python runtime state: initialized\n",
            "\n",
            "Thread 0x00007fbfb98456c0 (most recent call first):\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 469 in select\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc03cff96c0 (most recent call first):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 320 in wait\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/IPython/core/history.py\", line 829 in run\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/IPython/core/history.py\", line 60 in only_when_enabled\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/decorator.py\", line 232 in fun\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc03d7fa6c0 (most recent call first):\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 469 in select\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199 in start\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/control.py\", line 23 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc03dffb6c0 (most recent call first):\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/iostream.py\", line 338 in _watch_pipe_fd\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc03e7fc6c0 (most recent call first):\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/iostream.py\", line 334 in _watch_pipe_fd\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc03ffff6c0 (most recent call first):\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/heartbeat.py\", line 103 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Thread 0x00007fc0448fd6c0 (most recent call first):\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 469 in select\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199 in start\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/iostream.py\", line 82 in _thread_main\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
            "\n",
            "Current thread 0x00007fc04ab13740 (most recent call first):\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 102 in convert_to_eager_tensor\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 304 in _constant_eager_impl\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 279 in _constant_impl\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 267 in constant\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 343 in _constant_tensor_conversion_function\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 1636 in convert_to_tensor\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py\", line 183 in wrapped\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 254 in args_to_matching_eager\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8568 in reshape_eager_fallback\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8546 in reshape\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\", line 199 in reshape\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\", line 1176 in op_dispatch_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\", line 5149 in tensordot\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\", line 1176 in op_dispatch_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/layers/core/dense.py\", line 244 in call\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132 in __call__\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 668 in _run_internal_graph\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 511 in call\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413 in call\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132 in __call__\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65 in error_handler\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 561 in __call__\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65 in error_handler\n",
            "  File \"/tmp/ipykernel_92210/4082971523.py\", line 25 in call\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96 in error_handler\n",
            "  File \"/home/user/.loca"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "num_episodes = 1000\n",
        "verbose = True\n",
        "verbose_num = 2\n",
        "act_after_batch = True\n",
        "\n",
        "scores, steps = agent.train(\n",
        "    env=env, \n",
        "    num_episodes=num_episodes, \n",
        "    verbose=verbose, \n",
        "    verbose_num=verbose_num,  \n",
        "    act_after_batch=act_after_batch,\n",
        ")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "agent.act(env)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
