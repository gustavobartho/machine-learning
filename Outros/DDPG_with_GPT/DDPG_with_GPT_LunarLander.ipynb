{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8WT_0Y-KqQdW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-24 22:31:14.159647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-24 22:31:17.431154: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-24 22:31:17.431282: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-04-24 22:31:30.236222: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-24 22:31:30.265373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
            "2023-04-24 22:31:30.265512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers, models, optimizers\n",
        "from scipy.special import softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vQX1S3qQdc"
      },
      "source": [
        "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeO6YvRbqQdf",
        "outputId": "98286af3-47a0-4c4a-8c82-c041308ccc0c"
      },
      "outputs": [],
      "source": [
        "#Ornstein-Uhlenbeck Noise \n",
        "class OUActionNoise(object):\n",
        "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.4, x0=None):\n",
        "        self.mean = mean\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        self.x_prev = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ahtUWGtJqQdh"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "a = np.zeros(4)\n",
        "b = OUActionNoise(a)\n",
        "a += b()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-n5JPCPqQdi",
        "outputId": "a2773b70-2744-4aa3-c777-6acd96c63981"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, batch_size):\n",
        "        '''\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            batch_size (integer): The batch size.\n",
        "            block_size (integer): \n",
        "        '''\n",
        "        self.buffer = [[]]\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = size\n",
        "        \n",
        "    #--------------------------------------------------    \n",
        "    def append(self, steps):\n",
        "        if self.size >= self.max_size: del self.buffer[0]\n",
        "        for step in steps: self.buffer[-1].append(step)\n",
        "        # if done create new episode entry\n",
        "        if (steps[-1][4]): self.buffer.append([])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    #--------------------------------------------------    \n",
        "    def getEpisodes(self):\n",
        "        prob_diff = 1e-2\n",
        "        probs = softmax(np.arange(1-prob_diff, 1, (prob_diff)/(self.size - 1))[:(self.size - 1)])\n",
        "        episodes = np.random.choice(np.arange(self.size - 1), size=(self.batch_size,), replace=True)\n",
        "        return  [self.buffer[episode] for episode in episodes]\n",
        "    \n",
        "    #--------------------------------------------------  \n",
        "    @property  \n",
        "    def size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Number of elements in the buffer\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "\n",
        "    #--------------------------------------------------  \n",
        "    @property \n",
        "    def hasMinLength(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Boolean indicating if the memory have the minimum number of elements or not\n",
        "        '''\n",
        "        return (self.size >= 5)\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    @property  \n",
        "    def data(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            List with all the elements in the buffer\n",
        "        '''\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en0TDn5qQdl",
        "outputId": "c485ce90-2d8c-4f0a-cc48-6efad0b8233c"
      },
      "outputs": [],
      "source": [
        "gpt_kernel_initializer = lambda: initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "gpt_bias_initializer = lambda: initializers.RandomNormal(mean=0.0, stddev=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vxHdLrUWqQdm"
      },
      "outputs": [],
      "source": [
        "# Individual Head of self-attention\n",
        "class Head(layers.Layer):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        # key, query and value layers\n",
        "        self.key = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.query = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.value = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        # dropout layer\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x, training=False):\n",
        "        B, T, C = x.shape\n",
        "        if(B is None): B = self.batch_size \n",
        "        if(T is None): T = self.block_size\n",
        "        if(C is None): C = self.state_dim\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
        "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
        "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "46rkRg5nqQdn"
      },
      "outputs": [],
      "source": [
        "# Layer with multiple self-attention Heads for data communication \n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(batch_size, block_size, state_dim, head_size, dropout) for _ in range(num_heads)]\n",
        "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
        "        self.proj = layers.Dense(units=state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x):\n",
        "        # concatenate the heads outputs in the C dimension\n",
        "        out =  tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        # apply the projection and the dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VGMjfU_IqQdo"
      },
      "outputs": [],
      "source": [
        "#Simple feed forward for data computation\n",
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, state_dim, dropout, last_resize=True, spread_dim=None):\n",
        "        super().__init__()\n",
        "        last_layer = [\n",
        "            layers.Dense(state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), \n",
        "            layers.Dropout(dropout)\n",
        "        ] if last_resize else []\n",
        "        \n",
        "        self.net = models.Sequential([\n",
        "            layers.Dense(spread_dim if spread_dim is not None else 4 * state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
        "            layers.Dropout(dropout),\n",
        "            *last_layer\n",
        "        ])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YnyFKuPEqQdp"
      },
      "outputs": [],
      "source": [
        "# Block containing a multi head attention module and a feed forward linear computation\n",
        "class Block(layers.Layer):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, batch_size, block_size, state_dim, num_heads, dropout, last_resize, spread_dim):\n",
        "        super().__init__()\n",
        "        head_size = state_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
        "        self.sa = MultiHeadAttention(batch_size, block_size, state_dim, num_heads, head_size, dropout)\n",
        "        self.ffwd = FeedForward(state_dim, dropout, last_resize, spread_dim)\n",
        "        self.ln1 = layers.LayerNormalization()\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x):\n",
        "        # Multi head attention with layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # feed forward with layer norm\n",
        "        x = self.ffwd(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#f_value = lambda : initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "f_value = lambda : initializers.RandomNormal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V4uZbSagqQdq"
      },
      "outputs": [],
      "source": [
        "class GPTModel(models.Model):\n",
        "    def __init__(self, n_layer, batch_size, block_size, embedding_dim, out_dim, num_heads, dropout, ffw):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.state_embedding = layers.Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.position_embedding = layers.Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.blocks = models.Sequential([\n",
        "            Block(\n",
        "                batch_size, \n",
        "                block_size, \n",
        "                embedding_dim, \n",
        "                num_heads, \n",
        "                dropout, \n",
        "                last_resize = (i != n_layer - 1 ),  \n",
        "                spread_dim = out_dim if (i == n_layer - 1 ) else None,\n",
        "            )for i in range(n_layer)\n",
        "        ])\n",
        "        self.ffw = ffw\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        inputs, positions = inp[0], inp[1]\n",
        "        B, T, C = inputs.shape\n",
        "        if(T is None): T = self.block_size\n",
        "        pos_emb = self.position_embedding(positions)\n",
        "        x = self.state_embedding(inputs) + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.ffw(x)\n",
        "        return logits\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def generate(self, states, positions):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = states[:, -self.block_size:, :]\n",
        "        pos_cond = positions[:, -self.block_size:, :]\n",
        "        # get the predictions\n",
        "        actions = self([idx_cond, pos_cond])\n",
        "        # focus only on the last time step\n",
        "        return actions\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        states, positions = input_shape\n",
        "        self.position_embedding.build(positions)\n",
        "        self.state_embedding.build(states)\n",
        "        super(GPTModel, self).build((2, None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdauE0A-qQdr",
        "outputId": "f775f548-57dc-43a6-877b-8cf35079d9a6"
      },
      "outputs": [],
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, position_dim, embedding_dim, num_heads, dropout, action_range, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "            layers.Dense(128,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(action_dim, activation='tanh', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            layers.Lambda(lambda i: i * action_range, dtype='float64'),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, state_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, state_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, positions):\n",
        "        return self.model.generate(states, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, positions):\n",
        "        return self.target_model.generate(states, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def act(self, states, positions):\n",
        "        action = self.predict(states, positions)\n",
        "        # Gets the last action only\n",
        "        action = action[0, -1, :]\n",
        "        return action\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_actor_model.h5')\n",
        "        self.target_model.save(path + '_actor_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iws_SlRZqQds",
        "outputId": "30dfbb02-a02f-4373-fb58-8fd4db3b5774"
      },
      "outputs": [],
      "source": [
        "class Critic(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, position_dim, embedding_dim, out_dim, num_heads, dropout, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim + action_dim\n",
        "        ffw = lambda: models.Sequential([\n",
        "                layers.Dense(64,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.Dense(out_dim, activation='linear', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = optimizers.Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, self.inp_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=128,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, self.inp_dim), (None, None, position_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, actions, positions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.model.generate(inp, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, actions, positions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        inp = tf.concat([states, actions], 2)\n",
        "        return self.target_model.generate(inp, positions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_critic_model.h5')\n",
        "        self.target_model.save(path + '_critic_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = models.load_model(path)\n",
        "        self.model = models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n2y8GZ7tqQds"
      },
      "outputs": [],
      "source": [
        "class DDPG_GPT_Agent(object):\n",
        "    def __init__(self, a_n_layers, c_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, c_n_heads, \n",
        "        dropout, action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
        "        epsilon_min, position_dim, a_embedding_dim, c_embedding_dim, gamma_grow, gamma_max\n",
        "        ):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_min = action_min\n",
        "        self.action_max = action_max\n",
        "        self.gamma_grow = gamma_grow\n",
        "        self.gamma_max = gamma_max\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.position_dim = position_dim\n",
        "\n",
        "        #Creates the Replay Buffer\n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
        "\n",
        "        #Creates the noise generator\n",
        "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
        "\n",
        "        #Creates the actor\n",
        "        self.actor = Actor(\n",
        "            n_layer=a_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            position_dim=position_dim,\n",
        "            embedding_dim=a_embedding_dim,\n",
        "            num_heads=a_n_heads, \n",
        "            dropout=dropout, \n",
        "            action_range=action_max, \n",
        "            lr=a_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "        \n",
        "        #Creates the critic\n",
        "        self.critic = Critic(\n",
        "            n_layer=c_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            position_dim=position_dim,\n",
        "            embedding_dim=c_embedding_dim,\n",
        "            out_dim=1,\n",
        "            num_heads=c_n_heads, \n",
        "            dropout=dropout, \n",
        "            lr=c_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "    \n",
        "    #--------------------------------------------------------------------     \n",
        "    def act(self, env):\n",
        "        step = 1\n",
        "        done = False\n",
        "\n",
        "        states = env.reset().reshape(1, 1, -1)\n",
        "        positions = self.int_to_bin(step).reshape(1, 1, -1)\n",
        "\n",
        "        while not done:\n",
        "            env.render()\n",
        "            # Generating actions for a given group of states \n",
        "            action = self.policy(states, positions, explore=False)\n",
        "            # Apply the action in the environment\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            step += 1\n",
        "            # Append the new state to the states history\n",
        "            states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "            positions = tf.concat((positions, self.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "        \n",
        "        return\n",
        "\n",
        "    #-------------------------------------------------------------------- \n",
        "    def policy(self, states, positions, explore=True):\n",
        "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
        "        # gets the action\n",
        "        action = self.actor.act(states, positions)\n",
        "        # takes the exploration with the epsilon probability\n",
        "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
        "        # clip the action to be between min and max values\n",
        "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
        "        action[np.isnan(action)] = 0\n",
        "\n",
        "        return action\n",
        "    \n",
        "    #-------------------------------------------------------------------- \n",
        "    def int_to_bin(self, num):\n",
        "        bin_r = np.array([b for b in np.binary_repr(num, width=self.position_dim)])\n",
        "        bin_arr = np.zeros(self.position_dim)\n",
        "        bin_arr[np.where(bin_r == '1')] = 1\n",
        "        return bin_arr\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def record_memories(self, steps):\n",
        "        mem_fix = 0.8\n",
        "        steps = np.array(steps, dtype=object)\n",
        "        \n",
        "        if (len(steps) > 1) and (np.random.rand() >= mem_fix):\n",
        "            # Takes the weighted average of the numerical data with the weights being the rewards absolute value\n",
        "            avg = np.average(steps[:, :-1], axis=0, weights=np.absolute(steps[:, 2]))\n",
        "            # Check if the mean state is a done state - done states are now added separately to avoid information loss\n",
        "            done = np.any(steps[:, -1]) \n",
        "            # Make the average step into an integer\n",
        "            avg[4] = np.floor(avg[4])\n",
        "            # creates the new average steps representation \n",
        "            steps = np.array([np.concatenate([avg, [done]])], dtype=object)\n",
        "        \n",
        "        step_bin = np.array([self.int_to_bin(int(st)) for st in steps[:, 4]])\n",
        "        new_step_bin = np.array([self.int_to_bin(int(st + 1)) for st in steps[:, 4]])\n",
        "        steps = np.delete(steps, 4, 1)\n",
        "        steps = np.array([[*step, bin_r, n_bin_r] for step, bin_r, n_bin_r in zip(steps, step_bin, new_step_bin)], dtype=object) \n",
        "        self.memory.append(steps)\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def learn(self, memory_steps):\n",
        "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
        "        self.record_memories(memory_steps)\n",
        "        if self.memory.hasMinLength: self.replay_memory()\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def episode_to_batch(self, episode):\n",
        "        episode_step = 4\n",
        "        get_memory_element = lambda i, batch: np.array([[mem[i] for mem in block] for block in batch])\n",
        "        batch = None\n",
        "        i = self.block_size\n",
        "        while True:\n",
        "            batch = [episode[i-self.block_size:i]] if batch is None else np.concatenate([batch, [episode[i-self.block_size:i]]])\n",
        "            if(i >= len(episode)): break\n",
        "            i = min(i+episode_step, len(episode))\n",
        "\n",
        "        return (np.array(get_memory_element(el, batch)) for el in range(len(episode[0])))\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def null_step(self, step):\n",
        "        step[2] = 0\n",
        "        return step\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def episode_pad(self, episode):\n",
        "        return np.concatenate((episode, [self.null_step(episode[-1]) for _ in range(self.block_size - len(episode))]))\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def get_episodes_batches(self, episodes):\n",
        "        states = None\n",
        "        actions = None\n",
        "        next_states = None\n",
        "        positions = None\n",
        "        next_positions = None\n",
        "        rewards = None\n",
        "        done = None\n",
        "\n",
        "        for episode in episodes:\n",
        "            if len(episode) < self.block_size: episode = self.episode_pad(episode)\n",
        "            st_aux, ac_aux, rw_aux, nst_aux, dn_aux, ps_aux, nps_aux = self.episode_to_batch(episode)\n",
        "        \n",
        "            states = st_aux if states is None else np.concatenate((states, st_aux))\n",
        "            actions = ac_aux if actions is None else np.concatenate((actions, ac_aux))\n",
        "            next_states = nst_aux if next_states is None else np.concatenate((next_states, nst_aux))\n",
        "            positions = ps_aux if positions is None else np.concatenate((positions, ps_aux))\n",
        "            next_positions = nps_aux if next_positions is None else np.concatenate((next_positions, nps_aux))\n",
        "            rewards = rw_aux if rewards is None else np.concatenate((rewards, rw_aux))\n",
        "            done = dn_aux if done is None else np.concatenate((done, dn_aux))\n",
        "\n",
        "        states = tf.convert_to_tensor(states, dtype='float32')\n",
        "        actions = tf.convert_to_tensor(actions, dtype='float32')\n",
        "        next_states = tf.convert_to_tensor(next_states, dtype='float32')\n",
        "        positions = tf.convert_to_tensor(positions, dtype='float32')\n",
        "        next_positions = tf.convert_to_tensor(next_positions, dtype='float32')\n",
        "        rewards = tf.convert_to_tensor(tf.expand_dims(tf.cast(rewards, dtype='float32'), axis=-1), dtype='float32')\n",
        "        done = tf.convert_to_tensor(tf.expand_dims(tf.cast(done, dtype='float32'), axis=-1), dtype='float32')\n",
        "\n",
        "        return states, actions, next_states, positions, next_positions, rewards, done\n",
        "\n",
        "    #--------------------------------------------------------------------    \n",
        "    def replay_memory(self):\n",
        "        \"\"\" Replay a batch of memories \"\"\"\n",
        "\n",
        "        # Get sample block from the replay buffer\n",
        "        episodes = self.memory.getEpisodes()\n",
        "        states, actions, next_states, positions, next_positions, rewards, done = self.get_episodes_batches(episodes)\n",
        "\n",
        "        #Train the critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Compute the actor target actions\n",
        "            target_actions = self.actor.target_predict(next_states, next_positions)\n",
        "            # Compute the critic target values - TODO: Use real return instead\n",
        "            predicted_return = self.critic.target_predict(next_states, target_actions, next_positions)\n",
        "            # The return for the last block element\n",
        "            last_return = predicted_return[:, -1, :]\n",
        "\n",
        "            # Compute the gamma tensor based on the block step\n",
        "            gamma_values = lambda i: tf.expand_dims(tf.repeat([[self.gamma**(k - i) for k in range(i, rewards.shape[1])]], repeats=rewards.shape[0], axis=0), axis=-1)\n",
        "            # Compute the gamma weighted reward for a given block step\n",
        "            weighted_next_rewards = lambda i: tf.math.reduce_sum(rewards[:, i:, :] * gamma_values(i), axis=1)\n",
        "            # The gamma weight for the last return bootstrap\n",
        "            last_return_weight = lambda i: self.gamma ** (rewards.shape[1] - i)\n",
        "            # Compute the done value for a block step\n",
        "            state_done = lambda i: 1 - done[:, i, :]\n",
        "            \n",
        "            # Compute the return target values\n",
        "            y = tf.stack([(weighted_next_rewards(i) + (last_return_weight(i) * last_return * state_done(i))) for i in range(rewards.shape[1])], axis=1)\n",
        "            #y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions, next_positions) * (1 - done) # y = (B, T, 1)\n",
        "            \n",
        "            # Predict the expected reward associated with taking the target predicted action in the state\n",
        "            critic_value = self.critic.predict(states, actions, positions)\n",
        "            # Compute the critic loss  \n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "            \n",
        "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
        "        \n",
        "        #Train the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts = self.actor.predict(states, positions)\n",
        "            critic_grads = self.critic.predict(states, acts, positions)\n",
        "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
        "            \n",
        "        #Update the model weights\n",
        "        self.actor.transferWeights()\n",
        "        self.critic.transferWeights() \n",
        "        \n",
        "    #--------------------------------------------------\n",
        "    def print_data(self, verbose, episode, step, score):\n",
        "        if verbose:\n",
        "            print(\"\\r                                                                                                     \", end=\"\")\n",
        "            print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(step)+\"\\tReward: \"+str(score) ,end=\"\")\n",
        "        return\n",
        "\n",
        "    #--------------------------------------------------------------------     \n",
        "    def train(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
        "        scores_history = []\n",
        "        steps_history = []\n",
        "        complete = 0\n",
        "        print(\"BEGIN\\n\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            done = False\n",
        "            score, step = 0, 1\n",
        "            state = env.reset()\n",
        "            states = state.reshape(1, 1, -1)\n",
        "            positions = self.int_to_bin(step).reshape(1, 1, -1)\n",
        "            \n",
        "            while not done:\n",
        "                done_step = []\n",
        "                memory_steps = []\n",
        "\n",
        "                while not done and (step % step_per_train != 0):\n",
        "                    action = self.policy(states, positions)\n",
        "                    self.print_data(verbose, episode, step, score)\n",
        "                    new_state, reward, done, _ = env.step(action)\n",
        "                    if (done): done_step.append([state, action, reward, new_state, step, int(done)])\n",
        "                    else: memory_steps.append([state, action, reward, new_state, step, int(done)])\n",
        "                    state = new_state\n",
        "                    step += 1\n",
        "                    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "                    positions = tf.concat((positions, self.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "                    score += reward\n",
        "                \n",
        "                step += 1\n",
        "                if len(memory_steps) > 0: self.learn(memory_steps)\n",
        "                if len(done_step) > 0: self.learn(done_step)\n",
        "\n",
        "            scores_history.append(score)\n",
        "            steps_history.append(step)\n",
        "\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
        "            self.gamma = min(self.gamma_max, self.gamma*self.gamma_grow)\n",
        "            \n",
        "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
        "            if(score >= complete_value):\n",
        "                complete += 1\n",
        "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
        "                if end_on_complete and complete >= complete_num: break\n",
        "            \n",
        "            #These information are printed after each verbose_num episodes\n",
        "            if((episode+1)%verbose_num == 0):\n",
        "                print(\"\\r                                                                                                          \", end=\"\")\n",
        "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes, \n",
        "                      \"\\n\\tTotal reward: \", round(np.mean(scores_history[-verbose_num:]), 2), '+/-', round(np.std(scores_history[-verbose_num:]), 2), \n",
        "                      \"\\n\\tNum. steps: \", round(np.mean(steps_history[-verbose_num:]), 2), '+/-', round(np.std(steps_history[-verbose_num:]), 2), \n",
        "                      *[\"\\n\\tCompleted: \", complete] if complete_value != float('inf') else '', \n",
        "                      \"\\n--------------------------\",\n",
        "                    )\n",
        "                \n",
        "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
        "                if act_after_batch: self.act(env)\n",
        "                \n",
        "                #Set the number of completed episodes on the batch to zero\n",
        "                complete = 0\n",
        "\n",
        "        print(\"\\nFINISHED\")\n",
        "        \n",
        "        return scores_history, steps_history\n",
        "    #--------------------------------------------------------------------     \n",
        "    def save(self, path):\n",
        "        self.actor.saveModel(path)\n",
        "        self.critic.saveModel(path)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def load(self, a_path, c_path):\n",
        "        self.actor.loadModel(a_path)\n",
        "        self.critic.loadModel(c_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wS8tg8OqQdt",
        "outputId": "3d5e1dc5-3b35-4c65-b236-f11b903b86d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n",
            "2023-04-24 22:31:58.674268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-24 22:31:58.674554: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-24 22:31:58.683192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-24 22:31:58.687850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-24 22:31:59.152851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
            "2023-04-24 22:31:59.173632: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-04-24 22:31:59.181494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True, max_episode_steps=500)\n",
        "batch_size = 8\n",
        "block_size = 32\n",
        "position_dim = 14\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_min = env.action_space.low\n",
        "action_max = env.action_space.high\n",
        "dropout = 0.05\n",
        "memory_size = 5000\n",
        "gamma = 0.7\n",
        "gamma_grow = 1 + 2e-3\n",
        "gamma_max = 0.99\n",
        "epsilon = 0.9\n",
        "epsilon_decay = 0.991\n",
        "epsilon_min = 0.4\n",
        "tau = 8e-4\n",
        "\n",
        "# Actor hyperparameter\n",
        "a_n_layer = 1\n",
        "a_num_heads = 1\n",
        "a_embedding_dim = 8\n",
        "a_learning_rate = 9e-4\n",
        "\n",
        "# Critic hyperparameter\n",
        "c_n_layer = 1\n",
        "c_num_heads = 1\n",
        "c_embedding_dim = 10\n",
        "c_learning_rate = 2e-3\n",
        "\n",
        "agent = DDPG_GPT_Agent(\n",
        "    a_n_layers = a_n_layer,\n",
        "    c_n_layers = c_n_layer, \n",
        "    batch_size = batch_size, \n",
        "    block_size=block_size, \n",
        "    state_dim=state_dim, \n",
        "    action_dim=action_dim, \n",
        "    a_embedding_dim=a_embedding_dim,\n",
        "    c_embedding_dim=c_embedding_dim,\n",
        "    a_n_heads=a_num_heads, \n",
        "    c_n_heads=c_num_heads,\n",
        "    dropout=dropout, \n",
        "    action_min=action_min, \n",
        "    action_max=action_max, \n",
        "    memory_size=memory_size, \n",
        "    gamma=gamma, \n",
        "    a_lr=a_learning_rate, \n",
        "    c_lr=c_learning_rate, \n",
        "    tau=tau, \n",
        "    epsilon=epsilon, \n",
        "    epsilon_decay=epsilon_decay, \n",
        "    epsilon_min=epsilon_min,\n",
        "    position_dim=position_dim,\n",
        "    gamma_grow=gamma_grow,\n",
        "    gamma_max=gamma_max,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCvb6NiqQdu",
        "outputId": "bb989c21-13bb-441f-bc9a-179631844892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEGIN\n",
            "\n",
            "Episodes:  10 / 5000                                                                                      \n",
            "\tTotal reward:  -521.41 +/- 212.2 \n",
            "\tNum. steps:  105.6 +/- 31.63 \n",
            "--------------------------\n",
            "Episodes:  20 / 5000                                                                                      \n",
            "\tTotal reward:  -712.68 +/- 364.7 \n",
            "\tNum. steps:  158.2 +/- 67.86 \n",
            "--------------------------\n",
            "Episodes:  30 / 5000                                                                                      \n",
            "\tTotal reward:  -153.22 +/- 49.54 \n",
            "\tNum. steps:  98.5 +/- 18.81 \n",
            "--------------------------\n",
            "Episodes:  40 / 5000                                                                                      \n",
            "\tTotal reward:  -109.41 +/- 31.04 \n",
            "\tNum. steps:  81.6 +/- 16.98 \n",
            "--------------------------\n",
            "Episodes:  50 / 5000                                                                                      \n",
            "\tTotal reward:  -123.67 +/- 22.85 \n",
            "\tNum. steps:  73.8 +/- 10.4 \n",
            "--------------------------\n",
            "Episodes:  60 / 5000                                                                                      \n",
            "\tTotal reward:  -125.45 +/- 22.6 \n",
            "\tNum. steps:  87.4 +/- 16.57 \n",
            "--------------------------\n",
            "Episodes:  70 / 5000                                                                                      \n",
            "\tTotal reward:  -126.76 +/- 23.02 \n",
            "\tNum. steps:  81.0 +/- 11.8 \n",
            "--------------------------\n",
            "Episodes:  80 / 5000                                                                                      \n",
            "\tTotal reward:  -267.33 +/- 219.59 \n",
            "\tNum. steps:  127.5 +/- 56.46 \n",
            "--------------------------\n",
            "Episodes:  90 / 5000                                                                                      \n",
            "\tTotal reward:  -393.47 +/- 230.05 \n",
            "\tNum. steps:  241.9 +/- 82.67 \n",
            "--------------------------\n",
            "Episodes:  100 / 5000                                                                                     \n",
            "\tTotal reward:  -521.22 +/- 179.44 \n",
            "\tNum. steps:  267.2 +/- 73.3 \n",
            "--------------------------\n",
            "Episodes:  110 / 5000                                                                                     \n",
            "\tTotal reward:  -307.76 +/- 143.69 \n",
            "\tNum. steps:  336.1 +/- 94.54 \n",
            "--------------------------\n",
            "Episodes:  120 / 5000                                                                                     \n",
            "\tTotal reward:  -341.15 +/- 71.8 \n",
            "\tNum. steps:  337.4 +/- 41.06 \n",
            "--------------------------\n",
            "Episodes:  130 / 5000                                                                                     \n",
            "\tTotal reward:  -322.17 +/- 112.53 \n",
            "\tNum. steps:  294.8 +/- 47.43 \n",
            "--------------------------\n",
            "Episodes:  140 / 5000                                                                                     \n",
            "\tTotal reward:  -302.9 +/- 141.42 \n",
            "\tNum. steps:  297.1 +/- 32.7 \n",
            "--------------------------\n",
            "Episodes:  150 / 5000                                                                                     \n",
            "\tTotal reward:  -271.42 +/- 170.03 \n",
            "\tNum. steps:  294.1 +/- 36.67 \n",
            "--------------------------\n",
            "Episodes:  160 / 5000                                                                                     \n",
            "\tTotal reward:  -250.2 +/- 149.69 \n",
            "\tNum. steps:  300.7 +/- 12.35 \n",
            "--------------------------\n",
            "Episodes:  170 / 5000                                                                                     \n",
            "\tTotal reward:  -191.45 +/- 60.12 \n",
            "\tNum. steps:  260.5 +/- 57.09 \n",
            "--------------------------\n",
            "Episodes:  180 / 5000                                                                                     \n",
            "\tTotal reward:  -413.92 +/- 192.05 \n",
            "\tNum. steps:  346.7 +/- 93.14 \n",
            "--------------------------\n",
            "Episodes:  190 / 5000                                                                                     \n",
            "\tTotal reward:  -503.55 +/- 318.24 \n",
            "\tNum. steps:  314.5 +/- 61.99 \n",
            "--------------------------\n",
            "Episodes:  200 / 5000                                                                                     \n",
            "\tTotal reward:  -477.97 +/- 169.66 \n",
            "\tNum. steps:  340.2 +/- 25.57 \n",
            "--------------------------\n",
            "Episodes:  210 / 5000                                                                                     \n",
            "\tTotal reward:  -450.78 +/- 200.56 \n",
            "\tNum. steps:  323.8 +/- 26.1 \n",
            "--------------------------\n",
            "Episodes:  220 / 5000                                                                                     \n",
            "\tTotal reward:  -395.39 +/- 230.14 \n",
            "\tNum. steps:  359.7 +/- 71.44 \n",
            "--------------------------\n",
            "Episodes:  230 / 5000                                                                                     \n",
            "\tTotal reward:  -346.76 +/- 137.18 \n",
            "\tNum. steps:  338.8 +/- 50.95 \n",
            "--------------------------\n",
            "Episodes:  240 / 5000                                                                                     \n",
            "\tTotal reward:  -281.89 +/- 124.41 \n",
            "\tNum. steps:  353.0 +/- 25.93 \n",
            "--------------------------\n",
            "Episodes:  250 / 5000                                                                                     \n",
            "\tTotal reward:  -149.01 +/- 56.68 \n",
            "\tNum. steps:  350.8 +/- 19.33 \n",
            "--------------------------\n",
            "Episodes:  260 / 5000                                                                                     \n",
            "\tTotal reward:  -131.63 +/- 83.78 \n",
            "\tNum. steps:  310.7 +/- 37.94 \n",
            "--------------------------\n",
            "Episodes:  270 / 5000                                                                                     \n",
            "\tTotal reward:  -188.05 +/- 126.34 \n",
            "\tNum. steps:  320.4 +/- 18.07 \n",
            "--------------------------\n",
            "Episodes:  280 / 5000                                                                                     \n",
            "\tTotal reward:  -130.87 +/- 101.92 \n",
            "\tNum. steps:  293.8 +/- 27.39 \n",
            "--------------------------\n",
            "Episodes:  290 / 5000                                                                                     \n",
            "\tTotal reward:  -91.13 +/- 73.06 \n",
            "\tNum. steps:  310.0 +/- 50.86 \n",
            "--------------------------\n",
            "Episodes:  300 / 5000                                                                                     \n",
            "\tTotal reward:  -128.63 +/- 82.78 \n",
            "\tNum. steps:  293.5 +/- 32.11 \n",
            "--------------------------\n",
            "Episodes:  310 / 5000                                                                                     \n",
            "\tTotal reward:  -104.4 +/- 78.61 \n",
            "\tNum. steps:  318.2 +/- 45.02 \n",
            "--------------------------\n",
            "Episodes:  320 / 5000                                                                                     \n",
            "\tTotal reward:  -108.53 +/- 62.59 \n",
            "\tNum. steps:  330.6 +/- 43.95 \n",
            "--------------------------\n",
            "Episodes:  330 / 5000                                                                                     \n",
            "\tTotal reward:  -103.24 +/- 52.46 \n",
            "\tNum. steps:  325.2 +/- 80.16 \n",
            "--------------------------\n",
            "Episodes:  340 / 5000                                                                                     \n",
            "\tTotal reward:  -100.27 +/- 60.41 \n",
            "\tNum. steps:  298.7 +/- 77.53 \n",
            "--------------------------\n",
            "Episodes:  350 / 5000                                                                                     \n",
            "\tTotal reward:  -97.99 +/- 49.89 \n",
            "\tNum. steps:  346.2 +/- 56.06 \n",
            "--------------------------\n",
            "Episodes:  360 / 5000                                                                                     \n",
            "\tTotal reward:  -137.49 +/- 40.33 \n",
            "\tNum. steps:  267.0 +/- 65.01 \n",
            "--------------------------\n",
            "Episodes:  370 / 5000                                                                                     \n",
            "\tTotal reward:  -145.2 +/- 45.69 \n",
            "\tNum. steps:  277.6 +/- 91.98 \n",
            "--------------------------\n",
            "Episodes:  380 / 5000                                                                                     \n",
            "\tTotal reward:  -117.84 +/- 56.43 \n",
            "\tNum. steps:  350.4 +/- 50.51 \n",
            "--------------------------\n",
            "Episodes:  390 / 5000                                                                                     \n",
            "\tTotal reward:  -79.55 +/- 109.64 \n",
            "\tNum. steps:  279.9 +/- 71.96 \n",
            "--------------------------\n",
            "Episodes:  400 / 5000                                                                                     \n",
            "\tTotal reward:  -97.12 +/- 56.67 \n",
            "\tNum. steps:  309.4 +/- 73.48 \n",
            "--------------------------\n",
            "Episodes:  410 / 5000                                                                                     \n",
            "\tTotal reward:  -113.14 +/- 126.58 \n",
            "\tNum. steps:  307.6 +/- 42.49 \n",
            "--------------------------\n",
            "Episodes:  420 / 5000                                                                                     \n",
            "\tTotal reward:  -140.48 +/- 99.58 \n",
            "\tNum. steps:  345.6 +/- 92.54 \n",
            "--------------------------\n",
            "Episodes:  430 / 5000                                                                                     \n",
            "\tTotal reward:  -72.64 +/- 57.9 \n",
            "\tNum. steps:  278.0 +/- 61.72 \n",
            "--------------------------\n",
            "Episodes:  440 / 5000                                                                                     \n",
            "\tTotal reward:  -92.36 +/- 96.31 \n",
            "\tNum. steps:  284.1 +/- 51.48 \n",
            "--------------------------\n",
            "Episodes:  450 / 5000                                                                                     \n",
            "\tTotal reward:  -45.51 +/- 109.22 \n",
            "\tNum. steps:  330.3 +/- 109.87 \n",
            "--------------------------\n",
            "Episodes:  460 / 5000                                                                                     \n",
            "\tTotal reward:  -62.27 +/- 113.61 \n",
            "\tNum. steps:  318.3 +/- 89.17 \n",
            "--------------------------\n",
            "Episodes:  470 / 5000                                                                                     \n",
            "\tTotal reward:  -25.75 +/- 96.05 \n",
            "\tNum. steps:  347.9 +/- 118.74 \n",
            "--------------------------\n",
            "Episodes:  480 / 5000                                                                                     \n",
            "\tTotal reward:  -1.03 +/- 108.87 \n",
            "\tNum. steps:  310.5 +/- 65.4 \n",
            "--------------------------\n",
            "Episodes:  490 / 5000                                                                                     \n",
            "\tTotal reward:  -23.86 +/- 64.13 \n",
            "\tNum. steps:  331.4 +/- 114.77 \n",
            "--------------------------\n",
            "Episodes:  500 / 5000                                                                                     \n",
            "\tTotal reward:  -51.65 +/- 52.06 \n",
            "\tNum. steps:  333.0 +/- 98.07 \n",
            "--------------------------\n",
            "Episodes:  510 / 5000                                                                                     \n",
            "\tTotal reward:  -13.21 +/- 48.12 \n",
            "\tNum. steps:  338.4 +/- 146.74 \n",
            "--------------------------\n",
            "Episodes:  520 / 5000                                                                                     \n",
            "\tTotal reward:  -40.31 +/- 77.3 \n",
            "\tNum. steps:  358.3 +/- 140.85 \n",
            "--------------------------\n",
            "Episodes:  530 / 5000                                                                                     \n",
            "\tTotal reward:  29.78 +/- 168.73 \n",
            "\tNum. steps:  334.6 +/- 91.03 \n",
            "--------------------------\n",
            "Episodes:  540 / 5000                                                                                     \n",
            "\tTotal reward:  -57.34 +/- 60.59 \n",
            "\tNum. steps:  329.9 +/- 114.01 \n",
            "--------------------------\n",
            "Episodes:  550 / 5000                                                                                     \n",
            "\tTotal reward:  -38.32 +/- 62.58 \n",
            "\tNum. steps:  384.5 +/- 155.85 \n",
            "--------------------------\n",
            "Episodes:  560 / 5000                                                                                     \n",
            "\tTotal reward:  -26.04 +/- 105.14 \n",
            "\tNum. steps:  381.2 +/- 144.12 \n",
            "--------------------------\n",
            "Episodes:  570 / 5000                                                                                     \n",
            "\tTotal reward:  -21.6 +/- 129.07 \n",
            "\tNum. steps:  441.3 +/- 146.94 \n",
            "--------------------------\n",
            "Episodes:  580 / 5000                                                                                     \n",
            "\tTotal reward:  -4.52 +/- 95.72 \n",
            "\tNum. steps:  324.9 +/- 135.21 \n",
            "--------------------------\n",
            "Episodes:  590 / 5000                                                                                     \n",
            "\tTotal reward:  9.36 +/- 31.52 \n",
            "\tNum. steps:  321.2 +/- 155.81 \n",
            "--------------------------\n",
            "Episodes:  600 / 5000                                                                                     \n",
            "\tTotal reward:  -20.72 +/- 25.38 \n",
            "\tNum. steps:  255.8 +/- 103.66 \n",
            "--------------------------\n",
            "Episodes:  610 / 5000                                                                                     \n",
            "\tTotal reward:  27.71 +/- 81.81 \n",
            "\tNum. steps:  374.0 +/- 150.59 \n",
            "--------------------------\n",
            "Episodes:  620 / 5000                                                                                     \n",
            "\tTotal reward:  -21.55 +/- 72.05 \n",
            "\tNum. steps:  239.1 +/- 113.67 \n",
            "--------------------------\n",
            "Episodes:  630 / 5000                                                                                     \n",
            "\tTotal reward:  -10.21 +/- 24.98 \n",
            "\tNum. steps:  278.7 +/- 139.95 \n",
            "--------------------------\n",
            "Episodes:  640 / 5000                                                                                     \n",
            "\tTotal reward:  -18.99 +/- 50.3 \n",
            "\tNum. steps:  285.0 +/- 137.64 \n",
            "--------------------------\n",
            "Episodes:  650 / 5000                                                                                     \n",
            "\tTotal reward:  -17.26 +/- 30.55 \n",
            "\tNum. steps:  240.3 +/- 28.61 \n",
            "--------------------------\n",
            "Episodes:  660 / 5000                                                                                     \n",
            "\tTotal reward:  -17.17 +/- 51.1 \n",
            "\tNum. steps:  294.2 +/- 133.41 \n",
            "--------------------------\n",
            "Episodes:  670 / 5000                                                                                     \n",
            "\tTotal reward:  -13.79 +/- 29.3 \n",
            "\tNum. steps:  269.0 +/- 98.12 \n",
            "--------------------------\n",
            "Episodes:  680 / 5000                                                                                     \n",
            "\tTotal reward:  -34.21 +/- 43.68 \n",
            "\tNum. steps:  251.6 +/- 29.46 \n",
            "--------------------------\n",
            "Episodes:  690 / 5000                                                                                     \n",
            "\tTotal reward:  -7.69 +/- 31.75 \n",
            "\tNum. steps:  236.0 +/- 27.74 \n",
            "--------------------------\n",
            "Episodes:  700 / 5000                                                                                     \n",
            "\tTotal reward:  -61.04 +/- 54.12 \n",
            "\tNum. steps:  249.4 +/- 39.62 \n",
            "--------------------------\n",
            "Episodes:  710 / 5000                                                                                     \n",
            "\tTotal reward:  -36.22 +/- 44.87 \n",
            "\tNum. steps:  247.3 +/- 16.51 \n",
            "--------------------------\n",
            "Episodes:  720 / 5000                                                                                     \n",
            "\tTotal reward:  -27.26 +/- 51.12 \n",
            "\tNum. steps:  274.3 +/- 100.76 \n",
            "--------------------------\n",
            "Episodes:  730 / 5000                                                                                     \n",
            "\tTotal reward:  -33.45 +/- 22.97 \n",
            "\tNum. steps:  260.4 +/- 17.08 \n",
            "--------------------------\n",
            "Episodes:  740 / 5000                                                                                     \n",
            "\tTotal reward:  -23.44 +/- 34.51 \n",
            "\tNum. steps:  291.2 +/- 89.33 \n",
            "--------------------------\n",
            "Episodes:  750 / 5000                                                                                     \n",
            "\tTotal reward:  -39.42 +/- 64.82 \n",
            "\tNum. steps:  298.3 +/- 87.45 \n",
            "--------------------------\n",
            "Episodes:  760 / 5000                                                                                     \n",
            "\tTotal reward:  -51.09 +/- 76.49 \n",
            "\tNum. steps:  257.8 +/- 14.03 \n",
            "--------------------------\n",
            "Episodes:  770 / 5000                                                                                     \n",
            "\tTotal reward:  -42.6 +/- 39.99 \n",
            "\tNum. steps:  267.0 +/- 12.09 \n",
            "--------------------------\n",
            "Episodes:  780 / 5000                                                                                     \n",
            "\tTotal reward:  -46.42 +/- 41.07 \n",
            "\tNum. steps:  276.4 +/- 8.95 \n",
            "--------------------------\n",
            "Episodes:  790 / 5000                                                                                     \n",
            "\tTotal reward:  -31.38 +/- 43.85 \n",
            "\tNum. steps:  262.0 +/- 18.41 \n",
            "--------------------------\n",
            "Episodes:  800 / 5000                                                                                     \n",
            "\tTotal reward:  -10.68 +/- 26.58 \n",
            "\tNum. steps:  256.0 +/- 35.93 \n",
            "--------------------------\n",
            "Episodes:  810 / 5000                                                                                     \n",
            "\tTotal reward:  -22.43 +/- 59.51 \n",
            "\tNum. steps:  265.0 +/- 17.31 \n",
            "--------------------------\n",
            "Episodes:  820 / 5000                                                                                     \n",
            "\tTotal reward:  -28.54 +/- 39.71 \n",
            "\tNum. steps:  275.2 +/- 24.04 \n",
            "--------------------------\n",
            "Episodes:  830 / 5000                                                                                     \n",
            "\tTotal reward:  -17.01 +/- 39.57 \n",
            "\tNum. steps:  281.1 +/- 98.06 \n",
            "--------------------------\n",
            "Episodes:  840 / 5000                                                                                     \n",
            "\tTotal reward:  -27.91 +/- 31.81 \n",
            "\tNum. steps:  264.4 +/- 14.36 \n",
            "--------------------------\n",
            "Episodes:  850 / 5000                                                                                     \n",
            "\tTotal reward:  -48.82 +/- 40.28 \n",
            "\tNum. steps:  273.8 +/- 28.51 \n",
            "--------------------------\n",
            "Episodes:  860 / 5000                                                                                     \n",
            "\tTotal reward:  -50.09 +/- 128.34 \n",
            "\tNum. steps:  293.4 +/- 62.39 \n",
            "--------------------------\n",
            "Episodes:  870 / 5000                                                                                     \n",
            "\tTotal reward:  -12.83 +/- 47.23 \n",
            "\tNum. steps:  306.1 +/- 106.01 \n",
            "--------------------------\n",
            "Episodes:  880 / 5000                                                                                     \n",
            "\tTotal reward:  -39.05 +/- 40.6 \n",
            "\tNum. steps:  315.4 +/- 85.48 \n",
            "--------------------------\n",
            "Episodes:  890 / 5000                                                                                     \n",
            "\tTotal reward:  -70.53 +/- 26.77 \n",
            "\tNum. steps:  415.7 +/- 128.12 \n",
            "--------------------------\n",
            "Episodes:  900 / 5000                                                                                     \n",
            "\tTotal reward:  -60.04 +/- 55.11 \n",
            "\tNum. steps:  323.8 +/- 120.58 \n",
            "--------------------------\n",
            "Episodes:  910 / 5000                                                                                     \n",
            "\tTotal reward:  -30.51 +/- 57.14 \n",
            "\tNum. steps:  286.8 +/- 131.25 \n",
            "--------------------------\n",
            "Episodes:  920 / 5000                                                                                     \n",
            "\tTotal reward:  -8.62 +/- 39.12 \n",
            "\tNum. steps:  267.0 +/- 112.27 \n",
            "--------------------------\n",
            "Episodes:  930 / 5000                                                                                     \n",
            "\tTotal reward:  26.29 +/- 109.53 \n",
            "\tNum. steps:  383.6 +/- 133.53 \n",
            "--------------------------\n",
            "Episodes:  940 / 5000                                                                                     \n",
            "\tTotal reward:  -27.35 +/- 85.91 \n",
            "\tNum. steps:  367.8 +/- 125.21 \n",
            "--------------------------\n",
            "Episodes:  950 / 5000                                                                                     \n",
            "\tTotal reward:  -19.74 +/- 126.17 \n",
            "\tNum. steps:  341.4 +/- 112.11 \n",
            "--------------------------\n",
            "Episodes:  960 / 5000                                                                                     \n",
            "\tTotal reward:  12.88 +/- 109.79 \n",
            "\tNum. steps:  304.0 +/- 120.95 \n",
            "--------------------------\n",
            "Episodes:  970 / 5000                                                                                     \n",
            "\tTotal reward:  13.63 +/- 117.95 \n",
            "\tNum. steps:  345.1 +/- 121.45 \n",
            "--------------------------\n",
            "Episodes:  980 / 5000                                                                                     \n",
            "\tTotal reward:  -47.86 +/- 57.08 \n",
            "\tNum. steps:  334.1 +/- 116.36 \n",
            "--------------------------\n",
            "Episodes:  990 / 5000                                                                                     \n",
            "\tTotal reward:  -40.01 +/- 65.2 \n",
            "\tNum. steps:  235.6 +/- 73.07 \n",
            "--------------------------\n",
            "Episodes:  1000 / 5000                                                                                    \n",
            "\tTotal reward:  -53.72 +/- 54.54 \n",
            "\tNum. steps:  243.6 +/- 70.2 \n",
            "--------------------------\n",
            "Episodes:  1010 / 5000                                                                                    \n",
            "\tTotal reward:  12.99 +/- 71.63 \n",
            "\tNum. steps:  304.3 +/- 155.68 \n",
            "--------------------------\n",
            "Episodes:  1020 / 5000                                                                                    \n",
            "\tTotal reward:  -33.78 +/- 63.55 \n",
            "\tNum. steps:  263.2 +/- 142.37 \n",
            "--------------------------\n",
            "Episodes:  1030 / 5000                                                                                    \n",
            "\tTotal reward:  -18.24 +/- 94.93 \n",
            "\tNum. steps:  302.9 +/- 146.02 \n",
            "--------------------------\n",
            "Episodes:  1040 / 5000                                                                                    \n",
            "\tTotal reward:  -50.99 +/- 92.57 \n",
            "\tNum. steps:  325.9 +/- 95.65 \n",
            "--------------------------\n",
            "Episodes:  1050 / 5000                                                                                    \n",
            "\tTotal reward:  -4.74 +/- 84.26 \n",
            "\tNum. steps:  321.2 +/- 118.79 \n",
            "--------------------------\n",
            "Episodes:  1060 / 5000                                                                                    \n",
            "\tTotal reward:  -61.34 +/- 41.57 \n",
            "\tNum. steps:  315.6 +/- 108.89 \n",
            "--------------------------\n",
            "Episodes:  1070 / 5000                                                                                    \n",
            "\tTotal reward:  9.42 +/- 53.34 \n",
            "\tNum. steps:  263.2 +/- 121.22 \n",
            "--------------------------\n",
            "Episodes:  1080 / 5000                                                                                    \n",
            "\tTotal reward:  -41.06 +/- 68.59 \n",
            "\tNum. steps:  326.3 +/- 117.76 \n",
            "--------------------------\n",
            "Episodes:  1090 / 5000                                                                                    \n",
            "\tTotal reward:  -34.16 +/- 35.2 \n",
            "\tNum. steps:  275.2 +/- 104.22 \n",
            "--------------------------\n",
            "Episodes:  1100 / 5000                                                                                    \n",
            "\tTotal reward:  23.97 +/- 81.21 \n",
            "\tNum. steps:  324.8 +/- 158.08 \n",
            "--------------------------\n",
            "Episodes:  1110 / 5000                                                                                    \n",
            "\tTotal reward:  39.76 +/- 84.38 \n",
            "\tNum. steps:  368.6 +/- 159.09 \n",
            "--------------------------\n",
            "Episodes:  1120 / 5000                                                                                    \n",
            "\tTotal reward:  -76.41 +/- 57.0 \n",
            "\tNum. steps:  302.3 +/- 118.04 \n",
            "--------------------------\n",
            "Episodes:  1130 / 5000                                                                                    \n",
            "\tTotal reward:  -31.61 +/- 61.08 \n",
            "\tNum. steps:  250.9 +/- 100.55 \n",
            "--------------------------\n",
            "Episodes:  1140 / 5000                                                                                    \n",
            "\tTotal reward:  -25.83 +/- 97.16 \n",
            "\tNum. steps:  352.1 +/- 165.13 \n",
            "--------------------------\n",
            "Episodes:  1150 / 5000                                                                                    \n",
            "\tTotal reward:  -23.59 +/- 69.64 \n",
            "\tNum. steps:  254.7 +/- 122.45 \n",
            "--------------------------\n",
            "Episodes:  1160 / 5000                                                                                    \n",
            "\tTotal reward:  -15.83 +/- 133.17 \n",
            "\tNum. steps:  316.7 +/- 130.54 \n",
            "--------------------------\n",
            "Episodes:  1170 / 5000                                                                                    \n",
            "\tTotal reward:  1.95 +/- 41.78 \n",
            "\tNum. steps:  228.3 +/- 111.32 \n",
            "--------------------------\n",
            "Episodes:  1180 / 5000                                                                                    \n",
            "\tTotal reward:  -36.3 +/- 109.66 \n",
            "\tNum. steps:  222.5 +/- 77.67 \n",
            "--------------------------\n",
            "Episodes:  1190 / 5000                                                                                    \n",
            "\tTotal reward:  18.46 +/- 126.66 \n",
            "\tNum. steps:  315.4 +/- 133.09 \n",
            "--------------------------\n",
            "Episodes:  1200 / 5000                                                                                    \n",
            "\tTotal reward:  -53.34 +/- 60.25 \n",
            "\tNum. steps:  279.9 +/- 140.94 \n",
            "--------------------------\n",
            "Episodes:  1210 / 5000                                                                                    \n",
            "\tTotal reward:  13.93 +/- 109.04 \n",
            "\tNum. steps:  298.4 +/- 165.83 \n",
            "--------------------------\n",
            "Episodes:  1220 / 5000                                                                                    \n",
            "\tTotal reward:  -31.83 +/- 79.88 \n",
            "\tNum. steps:  340.9 +/- 165.11 \n",
            "--------------------------\n",
            "Episodes:  1230 / 5000                                                                                    \n",
            "\tTotal reward:  -17.02 +/- 121.59 \n",
            "\tNum. steps:  287.2 +/- 148.3 \n",
            "--------------------------\n",
            "Episodes:  1240 / 5000                                                                                    \n",
            "\tTotal reward:  9.91 +/- 119.53 \n",
            "\tNum. steps:  239.3 +/- 149.76 \n",
            "--------------------------\n",
            "Episodes:  1250 / 5000                                                                                    \n",
            "\tTotal reward:  -70.66 +/- 54.87 \n",
            "\tNum. steps:  286.2 +/- 169.06 \n",
            "--------------------------\n",
            "Episodes:  1260 / 5000                                                                                    \n",
            "\tTotal reward:  11.08 +/- 79.56 \n",
            "\tNum. steps:  284.6 +/- 151.04 \n",
            "--------------------------\n",
            "Episodes:  1270 / 5000                                                                                    \n",
            "\tTotal reward:  20.09 +/- 91.5 \n",
            "\tNum. steps:  319.5 +/- 170.17 \n",
            "--------------------------\n",
            "Episodes:  1280 / 5000                                                                                    \n",
            "\tTotal reward:  -34.54 +/- 69.71 \n",
            "\tNum. steps:  309.4 +/- 170.33 \n",
            "--------------------------\n",
            "Episodes:  1290 / 5000                                                                                    \n",
            "\tTotal reward:  -32.85 +/- 53.2 \n",
            "\tNum. steps:  258.6 +/- 131.03 \n",
            "--------------------------\n",
            "Episodes:  1300 / 5000                                                                                    \n",
            "\tTotal reward:  -37.83 +/- 53.34 \n",
            "\tNum. steps:  346.2 +/- 163.89 \n",
            "--------------------------\n",
            "Episodes:  1310 / 5000                                                                                    \n",
            "\tTotal reward:  -8.28 +/- 110.2 \n",
            "\tNum. steps:  267.4 +/- 139.44 \n",
            "--------------------------\n",
            "Episodes:  1320 / 5000                                                                                    \n",
            "\tTotal reward:  -35.19 +/- 67.9 \n",
            "\tNum. steps:  239.0 +/- 130.64 \n",
            "--------------------------\n",
            "Episodes:  1330 / 5000                                                                                    \n",
            "\tTotal reward:  10.41 +/- 103.21 \n",
            "\tNum. steps:  354.3 +/- 159.79 \n",
            "--------------------------\n",
            "Episodes:  1340 / 5000                                                                                    \n",
            "\tTotal reward:  -34.77 +/- 45.47 \n",
            "\tNum. steps:  325.5 +/- 166.09 \n",
            "--------------------------\n",
            "Episodes:  1350 / 5000                                                                                    \n",
            "\tTotal reward:  -56.28 +/- 93.14 \n",
            "\tNum. steps:  237.9 +/- 122.66 \n",
            "--------------------------\n",
            "Episodes:  1360 / 5000                                                                                    \n",
            "\tTotal reward:  -86.6 +/- 72.6 \n",
            "\tNum. steps:  331.3 +/- 157.0 \n",
            "--------------------------\n",
            "Episodes:  1370 / 5000                                                                                    \n",
            "\tTotal reward:  -36.83 +/- 55.6 \n",
            "\tNum. steps:  254.6 +/- 155.28 \n",
            "--------------------------\n",
            "Episode: 1371\t Step: 119\tReward: 44.35583591007553                                                   "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m verbose_num \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scores, steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49mnum_episodes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     step_per_train\u001b[39m=\u001b[39;49mstep_per_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     verbose_num\u001b[39m=\u001b[39;49mverbose_num,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     act_after_batch\u001b[39m=\u001b[39;49mact_after_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.train\u001b[0;34m(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory_steps) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(memory_steps)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(done_step) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(done_step)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m scores_history\u001b[39m.\u001b[39mappend(score)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.learn\u001b[0;34m(self, memory_steps)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39m\"\"\" Append an experience to the memory and replay memory if possible \"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_memories(memory_steps)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mhasMinLength: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_memory()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.replay_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m     \u001b[39m# Compute the critic loss  \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     critic_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msquare(y \u001b[39m-\u001b[39m critic_value))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m critic_grad \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(critic_loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(critic_grad, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m \u001b[39m#Train the actor\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1114\u001b[0m     flat_targets,\n\u001b[1;32m   1115\u001b[0m     flat_sources,\n\u001b[1;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:1385\u001b[0m, in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1382\u001b[0m   gy \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmul(x, grad)\n\u001b[1;32m   1383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1384\u001b[0m   gy \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mreshape(\n\u001b[0;32m-> 1385\u001b[0m       math_ops\u001b[39m.\u001b[39;49mreduce_sum(gen_math_ops\u001b[39m.\u001b[39;49mmul(x, grad), ry), sy)\n\u001b[1;32m   1386\u001b[0m \u001b[39mreturn\u001b[39;00m (gx, gy)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2312\u001b[0m, in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.reduce_sum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreduce_sum\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   2250\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   2251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_sum\u001b[39m(input_tensor, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2252\u001b[0m   \u001b[39m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m \n\u001b[1;32m   2254\u001b[0m \u001b[39m  This is the reduction operation for the elementwise `tf.math.add` op.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[39m  @end_compatibility\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2312\u001b[0m   \u001b[39mreturn\u001b[39;00m reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[1;32m   2313\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2324\u001b[0m, in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_sum_with_dims\u001b[39m(input_tensor,\n\u001b[1;32m   2317\u001b[0m                          axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2318\u001b[0m                          keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   2319\u001b[0m                          name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2320\u001b[0m                          dims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2321\u001b[0m   keepdims \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mbool\u001b[39m(keepdims)\n\u001b[1;32m   2322\u001b[0m   \u001b[39mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[1;32m   2323\u001b[0m       keepdims, axis,\n\u001b[0;32m-> 2324\u001b[0m       gen_math_ops\u001b[39m.\u001b[39;49m_sum(input_tensor, dims, keepdims, name\u001b[39m=\u001b[39;49mname))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:11229\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11227\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m  11228\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m> 11229\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m  11230\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mSum\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, axis, \u001b[39m\"\u001b[39;49m\u001b[39mkeep_dims\u001b[39;49m\u001b[39m\"\u001b[39;49m, keep_dims)\n\u001b[1;32m  11231\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m  11232\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_episodes = 5000\n",
        "step_per_train = 10\n",
        "verbose = True\n",
        "verbose_num = 10\n",
        "act_after_batch = True\n",
        "\n",
        "scores, steps = agent.train(\n",
        "    env=env, \n",
        "    num_episodes=num_episodes,\n",
        "    step_per_train=step_per_train,\n",
        "    verbose=verbose, \n",
        "    verbose_num=verbose_num,  \n",
        "    act_after_batch=act_after_batch,\n",
        ")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "agent.act(env)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'scores' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(scores, np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(scores)))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(scores, np.arange(len(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True)\n",
        "step = 1\n",
        "done = False\n",
        "states = env.reset().reshape(1, 1, -1)\n",
        "positions = agent.int_to_bin(step).reshape(1, 1, -1)\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    # Generating actions for a given group of states \n",
        "    # gets the action\n",
        "    action = agent.actor.target_predict(states, positions)\n",
        "    # Gets the last action only\n",
        "    action = action[0, -1, :]\n",
        "    # clip the action to be between min and max values\n",
        "    action = np.clip(action, a_min=action_min, a_max=action_max)\n",
        "    action[np.isnan(action)] = 0\n",
        "    # Apply the action in the environment\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    step += 1\n",
        "    # Append the new state to the states history\n",
        "    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "    positions = tf.concat((positions, agent.int_to_bin(step).reshape(1, 1, -1)), axis=1)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
