{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "8WT_0Y-KqQdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, BatchNormalization, LayerNormalization, Lambda\n",
        "from tensorflow.keras.initializers import RandomNormal, Zeros\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q7vQX1S3qQdc"
      },
      "source": [
        "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeO6YvRbqQdf",
        "outputId": "98286af3-47a0-4c4a-8c82-c041308ccc0c"
      },
      "outputs": [],
      "source": [
        "#Ornstein-Uhlenbeck Noise \n",
        "class OUActionNoise(object):\n",
        "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.4, x0=None):\n",
        "        self.mean = mean\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        self.x_prev = x\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ahtUWGtJqQdh"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "a = np.zeros(4)\n",
        "b = OUActionNoise(a)\n",
        "a += b()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-n5JPCPqQdi",
        "outputId": "a2773b70-2744-4aa3-c777-6acd96c63981"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, batch_size):\n",
        "        '''\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            batch_size (integer): The batch size.\n",
        "            block_size (integer): \n",
        "        '''\n",
        "        self.buffer = [[]]\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = size\n",
        "        \n",
        "    #--------------------------------------------------    \n",
        "    def append(self, steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        if self.size >= self.max_size: del self.buffer[0]\n",
        "        for step in steps: self.buffer[-1].append(step)\n",
        "        # if done create new episode entry\n",
        "        # (state, action, reward, done)\n",
        "        if (steps[-1]['done']): self.buffer.append([])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    #--------------------------------------------------    \n",
        "    def getEpisodes(self):\n",
        "        prob_diff = 1e-2\n",
        "        probs = tf.nn.softmax(np.arange(1-prob_diff, 1, (prob_diff)/(self.size - 1))[:(self.size - 1)])\n",
        "        episodes = np.random.choice(\n",
        "            np.arange(self.size - 1), #don't chose the current step\n",
        "            size=(self.batch_size,), \n",
        "            replace=True\n",
        "        )\n",
        "        return  [self.buffer[episode] for episode in episodes]\n",
        "    \n",
        "    #--------------------------------------------------  \n",
        "    @property  \n",
        "    def size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Number of elements in the buffer\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "\n",
        "    #--------------------------------------------------  \n",
        "    @property \n",
        "    def hasMinLength(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            Boolean indicating if the memory have the minimum number of elements or not\n",
        "        '''\n",
        "        return (self.size >= 8)\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    @property  \n",
        "    def data(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            List with all the elements in the buffer\n",
        "        '''\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en0TDn5qQdl",
        "outputId": "c485ce90-2d8c-4f0a-cc48-6efad0b8233c"
      },
      "outputs": [],
      "source": [
        "gpt_kernel_initializer = lambda: RandomNormal(mean=0.0, stddev=0.05)\n",
        "gpt_bias_initializer = lambda: Zeros()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "vxHdLrUWqQdm"
      },
      "outputs": [],
      "source": [
        "# Individual Head of self-attention\n",
        "class Head(Layer):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, batch_size, block_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # key, query and value layers\n",
        "        self.key = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.query = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "        self.value = Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout_v = Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        state_emb, global_pos_emb, local_pos_emb = inp[0], inp[1], inp[2]\n",
        "        B, T, C = state_emb.shape\n",
        "        if(B is None): B = self.batch_size \n",
        "        if(T is None): T = self.block_size\n",
        "        if(C is None): C = self.state_dim\n",
        "\n",
        "        k = self.key(global_pos_emb)   # (B,T,C)\n",
        "        q = self.query(global_pos_emb) # (B,T,C)\n",
        "        \n",
        "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
        "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
        "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "\n",
        "        v = self.value(state_emb) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        out = self.dropout_v(out)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        #state_emb = global_pos_emb = local_pos_emb = embedding_dim\n",
        "        state_emb, global_pos_emb, local_pos_emb = input_shape\n",
        "        self.value.build(state_emb)\n",
        "        self.key.build(global_pos_emb)\n",
        "        self.query.build(local_pos_emb)\n",
        "        super(Head, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "46rkRg5nqQdn"
      },
      "outputs": [],
      "source": [
        "# Layer with multiple self-attention Heads for data communication \n",
        "class MultiHeadAttention(Layer):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, batch_size, block_size, embedding_dim, num_heads, head_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = []\n",
        "        for _ in range(num_heads):\n",
        "            head = Head(\n",
        "                batch_size=batch_size,\n",
        "                block_size=block_size,\n",
        "                head_size=head_size,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            head.build(((None, None, embedding_dim), (None, None, embedding_dim), (None, None, embedding_dim)))\n",
        "            self.heads.append(head)\n",
        "        \n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        # concatenate the heads outputs in the C dimension\n",
        "        out =  tf.concat([h(inp) for h in self.heads], axis=-1)\n",
        "        # apply the projection and the dropout\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        super(MultiHeadAttention, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "VGMjfU_IqQdo"
      },
      "outputs": [],
      "source": [
        "#Simple feed forward for data computation\n",
        "class FeedForward(Layer):\n",
        "    def __init__(self, embedding_dim, dropout, resize_to_input_dim=True, spread_dim=None):\n",
        "        # resize_to_input_dim -> Should be False only to last block element when posterior computation is gonna happen so it doesn't need to output embedding_dim sized elements to another block\n",
        "        # spread_dim -> the heads output comes concatenated in sequence so is computed and joint by the spread layer layer\n",
        "        super().__init__()\n",
        "        last_layer = [\n",
        "            Dense(embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), \n",
        "            Dropout(dropout)\n",
        "        ] if resize_to_input_dim else []\n",
        "        \n",
        "        self.net = Sequential([\n",
        "            Dense(spread_dim if spread_dim is not None else 4 * embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
        "            Dropout(dropout),\n",
        "            *last_layer\n",
        "        ])\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "YnyFKuPEqQdp"
      },
      "outputs": [],
      "source": [
        "# Block containing a multi head attention module and a feed forward linear computation\n",
        "class Block(Layer):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, batch_size, block_size, emb_dim, num_heads, dropout, resize_to_input_dim=None, spread_dim=None):\n",
        "        super().__init__()\n",
        "        self.resize_to_input_dim = resize_to_input_dim\n",
        "        head_size = emb_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
        "        \n",
        "        self.sa = MultiHeadAttention(batch_size, block_size, emb_dim, num_heads, head_size, dropout)\n",
        "        self.sa_ln = LayerNormalization()\n",
        "\n",
        "        self.ffwd = FeedForward(emb_dim, dropout, resize_to_input_dim, spread_dim)\n",
        "        self.ffwd_ln = LayerNormalization()\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        st_emp, global_pos_emb, local_pos_emb = inp[0], inp[1], inp[2]\n",
        "\n",
        "        # Multi head attention with layer norm\n",
        "        x = st_emp + self.sa([st_emp, global_pos_emb, local_pos_emb])\n",
        "        x = self.sa_ln(x)\n",
        "        \n",
        "        # feed forward with layer norm\n",
        "        x = (x + self.ffwd(x)) if self.resize_to_input_dim else self.ffwd(x)\n",
        "        x = self.ffwd_ln(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        st_emb, global_pos_emb, local_pos_emb = input_shape\n",
        "        self.sa.build(input_shape)\n",
        "        self.ffwd.build(st_emb)\n",
        "        super(Block, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_value = lambda : RandomNormal(mean=0.0, stddev=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "V4uZbSagqQdq"
      },
      "outputs": [],
      "source": [
        "class GPTModel(Model):\n",
        "    def __init__(self, n_layer, batch_size, block_size, embedding_dim, out_dim, num_heads, dropout, ffw):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.state_embedding = Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        self.action_embedding = Dense(units=embedding_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
        "        \n",
        "        self.blocks = []\n",
        "        for i in range(n_layer):\n",
        "            block = Block(batch_size, block_size, embedding_dim, num_heads, dropout,\n",
        "                resize_to_input_dim = (i != n_layer - 1 ),  \n",
        "                spread_dim = out_dim if (i == n_layer - 1 ) else None,\n",
        "            )\n",
        "            block.build(((None, None, embedding_dim), (None, None, embedding_dim), (None, None, embedding_dim)))\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.ffw = ffw\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def get_local_position_sin_encoding(self, batch_size, block_size, n=10000):\n",
        "        positions = np.tile([np.arange(block_size)], [batch_size, 1])\n",
        "        batch_size, block_size = positions.shape[:2]\n",
        "        aux = np.tile(np.tile([np.arange(self.embedding_dim)], [block_size, 1]), [batch_size, 1, 1])\n",
        "        denominator = tf.cast(n**((2*(aux//2))/self.embedding_dim), dtype=tf.float32)\n",
        "        val = tf.cast(np.tile(positions, [1, 1, self.embedding_dim]), dtype=tf.float32)\n",
        "        P = (np.sin(val/denominator) * ((aux + 1)%2)) + (np.cos(val/denominator)*(aux%2))\n",
        "        return P\n",
        "  \n",
        "    #--------------------------------------------------\n",
        "    def call(self, inp, training=False):\n",
        "        states, global_positions, actions = inp[0], inp[1], inp[2]\n",
        "        B, T, C = states.shape\n",
        "        if(T is None): T = self.block_size\n",
        "        if(B is None): B = self.batch_size\n",
        "\n",
        "        #local_position = self.get_local_position_sin_encoding(batch_size=B, block_size=T)\n",
        "        act_emb = self.action_embedding(actions)\n",
        "        st_emb = self.state_embedding(states)\n",
        "        \n",
        "        for block in self.blocks: st_emb = block((st_emb, act_emb+global_positions, global_positions))\n",
        "        logits = self.ffw(st_emb)\n",
        "\n",
        "        return logits\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def generate(self, states, positions, actions):\n",
        "        # crop idx to the last block_size tokens\n",
        "        st_cond = states[:, -self.block_size:, :]\n",
        "        pos_cond = positions[:, -self.block_size:, :]\n",
        "        act_cond = actions[:, -self.block_size:, :]\n",
        "        # get the predictions\n",
        "        actions = self([st_cond, pos_cond, act_cond])\n",
        "        # focus only on the last time step\n",
        "        return actions\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def build(self, input_shape):\n",
        "        states, positions, actions = input_shape\n",
        "        self.action_embedding.build(actions)\n",
        "        self.state_embedding.build(states)\n",
        "        super(GPTModel, self).build((len(input_shape), None, None, None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdauE0A-qQdr",
        "outputId": "f775f548-57dc-43a6-877b-8cf35079d9a6"
      },
      "outputs": [],
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, embedding_dim, num_heads, dropout, action_range, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        ffw = lambda: Sequential([\n",
        "            Dense(32,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            LayerNormalization(),\n",
        "            Dropout(dropout),\n",
        "            Dense(action_dim, activation='tanh', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            Lambda(lambda i: i * action_range, dtype='float64'),\n",
        "        ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=512,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, state_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=512,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, state_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, positions, actions):\n",
        "        return self.model.generate(states, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, positions, actions):\n",
        "        return self.target_model.generate(states, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def act(self, states, positions, actions):\n",
        "        action = self.predict(states, positions, actions)\n",
        "        # Gets the last action only\n",
        "        action = action[0, -1, :]\n",
        "        return action\n",
        "\n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_actor_model.h5')\n",
        "        self.target_model.save(path + '_actor_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = load_model(path)\n",
        "        self.model = load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iws_SlRZqQds",
        "outputId": "30dfbb02-a02f-4373-fb58-8fd4db3b5774"
      },
      "outputs": [],
      "source": [
        "class Critic(object):\n",
        "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, embedding_dim, out_dim, num_heads, dropout, lr, tau):\n",
        "        #Network dimensions\n",
        "        self.inp_dim = state_dim + action_dim\n",
        "        ffw = lambda: Sequential([\n",
        "                Dense(32,  activation='relu', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "                LayerNormalization(),\n",
        "                Dropout(dropout),\n",
        "                Dense(out_dim, activation='linear', kernel_initializer=f_value(), bias_initializer=f_value()),\n",
        "            ]) \n",
        "\n",
        "        #Parameter that coordinates the soft updates on the target weights\n",
        "        self.tau = tau\n",
        "\n",
        "        #Generates the optimization function - used in the agent to generate gradients\n",
        "        self.optimizer = Adam(lr)\n",
        "\n",
        "        #Generates the actor model\n",
        "        self.model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.model.build(((None, None, self.inp_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Generates the actor target model\n",
        "        self.target_model = GPTModel(\n",
        "            n_layer=n_layer,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            out_dim=256,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            ffw = ffw(),\n",
        "        )\n",
        "        self.target_model.build(((None, None, self.inp_dim), (None, None, embedding_dim), (None, None, action_dim)))\n",
        "\n",
        "        #Set the weights to be the same in the begining\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def predict(self, states, next_actions, positions, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        next_actions = tf.cast(next_actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        inp = tf.concat([states, next_actions], 2)\n",
        "        return self.model.generate(inp, positions, actions)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def target_predict(self, states, next_actions, positions, actions):\n",
        "        states = tf.cast(states, tf.float32) \n",
        "        next_actions = tf.cast(next_actions, tf.float32) \n",
        "        positions = tf.cast(positions, tf.float32)\n",
        "        actions = tf.cast(actions, tf.float32)\n",
        "        inp = tf.concat([states, next_actions], 2)\n",
        "        return self.target_model.generate(inp, positions, actions)\n",
        "    \n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def transferWeights(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        new_weights = []\n",
        "        \n",
        "        for i in range(len(weights)):\n",
        "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
        "        \n",
        "        self.target_model.set_weights(new_weights)\n",
        "        \n",
        "    #--------------------------------------------------------------------\n",
        "    def saveModel(self, path):\n",
        "        self.model.save(path + '_critic_model.h5')\n",
        "        self.target_model.save(path + '_critic_target_model.h5')\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def loadModel(self, path):\n",
        "        self.target_model = load_model(path)\n",
        "        self.model = load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "n2y8GZ7tqQds"
      },
      "outputs": [],
      "source": [
        "class DDPG_GPT_Agent(object):\n",
        "    def __init__(self, a_n_layers, c_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, c_n_heads, \n",
        "        dropout, action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
        "        epsilon_min, a_embedding_dim, c_embedding_dim, gamma_grow, gamma_max\n",
        "        ):\n",
        "        \n",
        "        self.block_size = block_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_min = action_min\n",
        "        self.action_max = action_max\n",
        "        self.gamma_grow = gamma_grow\n",
        "        self.gamma_max = gamma_max\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.a_embedding_dim = a_embedding_dim\n",
        "        self.c_embedding_dim = c_embedding_dim\n",
        "\n",
        "        self.episode_batch_size = batch_size\n",
        "        self.steps_batch_size = 1\n",
        "\n",
        "        #Creates the Replay Buffer\n",
        "        self.memory = ReplayBuffer(memory_size, self.episode_batch_size)\n",
        "\n",
        "        #Creates the noise generator\n",
        "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
        "\n",
        "        #Creates the actor\n",
        "        self.actor = Actor(\n",
        "            n_layer=a_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            embedding_dim=a_embedding_dim,\n",
        "            num_heads=a_n_heads, \n",
        "            dropout=dropout, \n",
        "            action_range=action_max, \n",
        "            lr=a_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "        \n",
        "        #Creates the critic\n",
        "        self.critic = Critic(\n",
        "            n_layer=c_n_layers,\n",
        "            batch_size=batch_size, \n",
        "            block_size=block_size, \n",
        "            state_dim=state_dim, \n",
        "            action_dim=action_dim, \n",
        "            embedding_dim=c_embedding_dim,\n",
        "            out_dim=1,\n",
        "            num_heads=c_n_heads, \n",
        "            dropout=dropout, \n",
        "            lr=c_lr, \n",
        "            tau=tau,\n",
        "        )\n",
        "    \n",
        "    #--------------------------------------------------------------------     \n",
        "    def act(self, env):\n",
        "        action = np.zeros(self.action_dim)\n",
        "        state = env.reset()\n",
        "        step = np.array([1])\n",
        "\n",
        "        actions = action.reshape(1, 1, -1)\n",
        "        states = state.reshape(1, 1, -1)\n",
        "        positions = step.reshape(1, 1, -1)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action = self.policy(states, positions, actions, explore=False)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            step += 1\n",
        "\n",
        "            states = tf.concat((states, state.reshape(1, 1, -1)), axis=1)\n",
        "            positions = tf.concat((positions, np.array([step]).reshape(1, 1, -1)), axis=1)\n",
        "            actions = tf.concat((actions, action.reshape(1, 1, -1)), axis=1)\n",
        "        \n",
        "        return\n",
        "    \n",
        "    #-------------------------------------------------------------------- \n",
        "    def get_position_sin_encoding(self, embedding_dim, positions, n=100):\n",
        "        batch_size, block_size = positions.shape[:2]\n",
        "        aux = np.tile(np.tile([np.arange(embedding_dim)], [block_size, 1]), [batch_size, 1, 1])\n",
        "        denominator = tf.cast(n**((2*(aux//2))/embedding_dim), dtype=tf.float32)\n",
        "        val = tf.cast(np.tile(positions, [1, 1, embedding_dim]), dtype=tf.float32)\n",
        "        P = (np.sin(val/denominator) * ((aux + 1)%2)) + (np.cos(val/denominator)*(aux%2))\n",
        "        return P\n",
        "\n",
        "    #-------------------------------------------------------------------- \n",
        "    def policy(self, states, positions, actions, explore=True):\n",
        "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
        "        # gets the action\n",
        "        action = self.actor.act(states, self.get_position_sin_encoding(self.a_embedding_dim, positions), actions)\n",
        "        # takes the exploration with the epsilon probability\n",
        "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
        "        # clip the action to be between min and max values\n",
        "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
        "        action[np.isnan(action)] = 0\n",
        "\n",
        "        return action   \n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def record_memories(self, steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        self.memory.append(steps)\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def learn(self, memory_steps):\n",
        "        # steps = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
        "        self.record_memories(memory_steps)\n",
        "        if self.memory.hasMinLength: self.replay_memory()\n",
        "        return\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def episode_to_batch(self, episode):\n",
        "        #episode = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        if len(episode) > (self.block_size + self.steps_batch_size):\n",
        "            steps_idxs = np.random.choice(np.arange(self.block_size, len(episode)), size=self.steps_batch_size-1, replace=False)\n",
        "            steps_idxs = np.append(steps_idxs, len(episode))\n",
        "        else: steps_idxs = np.arange(self.block_size, len(episode))\n",
        "        \n",
        "        batch = np.array([episode[i-self.block_size:i] for i in steps_idxs])\n",
        "        #batch = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        return batch\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def null_step(self, step):\n",
        "        #step = {'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}\n",
        "        step['reward'] = 0\n",
        "        step['done'] = True\n",
        "        return step\n",
        "\n",
        "    #--------------------------------------------------\n",
        "    def episode_pad(self, episode):\n",
        "        #episode = [{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]\n",
        "        return np.concatenate((episode, [self.null_step(episode[-1]) for _ in range(self.block_size - len(episode) + 1)]))\n",
        "    \n",
        "    #--------------------------------------------------\n",
        "    def get_episodes_batches(self, episodes):\n",
        "        #episodes = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        batch = None\n",
        "\n",
        "        join_episode = lambda final_value, aux_value: aux_value if final_value is None else np.concatenate((final_value, aux_value))\n",
        "\n",
        "        for episode in episodes:\n",
        "            if len(episode) <= self.block_size: episode = self.episode_pad(episode)\n",
        "            ep_batch = self.episode_to_batch(episode)\n",
        "            batch = join_episode(batch, ep_batch)\n",
        "\n",
        "        return batch\n",
        "\n",
        "    #--------------------------------------------------------------------    \n",
        "    def replay_memory(self):\n",
        "        \"\"\" Replay a batch of memories \"\"\"\n",
        "\n",
        "        # Get sample block from the replay buffer\n",
        "        episodes = self.memory.getEpisodes()\n",
        "        batch = self.get_episodes_batches(episodes)\n",
        "        #batch = [[{'step': int, 'prev_action': [action_dim], 'state': [state_dim], 'action':  [action_dim], 'next_state': [state_dim], 'reward': float, 'done': boolean}]]\n",
        "        to_tensor = lambda value: tf.convert_to_tensor(value, dtype='float32')\n",
        "        get_batch_element = lambda key, batch: to_tensor([[step[key] for step in block] for block in batch])\n",
        "        \n",
        "        positions = tf.expand_dims(get_batch_element('step', batch), axis=-1)\n",
        "        next_positions_actor = self.get_position_sin_encoding(self.a_embedding_dim, positions + 1)\n",
        "        next_positions_critic = self.get_position_sin_encoding(self.c_embedding_dim, positions + 1)\n",
        "        positions_actor = self.get_position_sin_encoding(self.a_embedding_dim, positions)\n",
        "        positions_critic = self.get_position_sin_encoding(self.c_embedding_dim, positions)\n",
        "\n",
        "        states = get_batch_element('state', batch)\n",
        "        next_states = get_batch_element('next_state', batch)\n",
        "\n",
        "        prev_actions = get_batch_element('prev_action', batch)  \n",
        "        actions = get_batch_element('action', batch)\n",
        "\n",
        "        rewards = tf.expand_dims(get_batch_element('reward', batch), axis=-1)\n",
        "        done = tf.expand_dims(get_batch_element('done', batch), axis=-1)\n",
        "\n",
        "        #Train the critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Compute the actor target actions\n",
        "            target_actions = self.actor.target_predict(next_states, next_positions_actor, actions)\n",
        "            # Compute the critic target values \n",
        "            predicted_return = self.critic.target_predict(next_states, target_actions, next_positions_critic, actions)\n",
        "            # The return for the last block element\n",
        "            last_return = predicted_return[:, -1, :]\n",
        "\n",
        "            # Compute the gamma tensor based on the block step\n",
        "            gamma_values = lambda i: tf.expand_dims(tf.repeat([[self.gamma**(k - i) for k in range(i, rewards.shape[1])]], repeats=rewards.shape[0], axis=0), axis=-1)\n",
        "            # Compute the gamma weighted reward for a given block step\n",
        "            weighted_next_rewards = lambda i: tf.math.reduce_sum(rewards[:, i:, :] * gamma_values(i), axis=1)\n",
        "            # The gamma weight for the last return bootstrap\n",
        "            last_return_weight = lambda i: self.gamma ** (rewards.shape[1] - i)\n",
        "            # Compute the done value for a block step\n",
        "            state_done = lambda i: 1 - done[:, i, :]\n",
        "            \n",
        "            # Compute the return target values\n",
        "            y = tf.stack([(weighted_next_rewards(i) + (last_return_weight(i) * last_return * state_done(i))) for i in range(rewards.shape[1])], axis=1)\n",
        "            #y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions, next_positions) * (1 - done) # y = (B, T, 1)\n",
        "            \n",
        "            # Predict the expected reward associated with taking the target predicted action in the state\n",
        "            critic_value = self.critic.predict(states, actions, positions_critic, prev_actions)\n",
        "            # Compute the critic loss  \n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "            \n",
        "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
        "        self.critic.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(critic_grad, self.critic.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "        \n",
        "        #Train the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts = self.actor.predict(states, positions_actor, prev_actions)\n",
        "            critic_grads = self.critic.predict(states, acts, positions_critic, prev_actions)\n",
        "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(actor_grad, self.actor.model.trainable_variables)\n",
        "            if grad is not None\n",
        "        )\n",
        "            \n",
        "        #Update the model weights\n",
        "        self.actor.transferWeights()\n",
        "        self.critic.transferWeights() \n",
        "        \n",
        "    #--------------------------------------------------\n",
        "    def print_data(self, verbose, episode, step, score):\n",
        "        if verbose:\n",
        "            print(\"\\r                                                                                                     \", end=\"\")\n",
        "            print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(step)+\"\\tReward: \"+str(round(score, 2)) ,end=\"\")\n",
        "        return\n",
        "\n",
        "    #--------------------------------------------------------------------     \n",
        "    def train(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
        "        scores_history = []\n",
        "        steps_history = []\n",
        "        complete = 0\n",
        "        print(\"BEGIN\\n\")\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            done = False\n",
        "            score, step = 0, 1\n",
        "            state = env.reset()\n",
        "            prev_action = np.zeros(self.action_dim)\n",
        "\n",
        "            states = state.reshape(1, 1, -1)\n",
        "            positions = np.array([step]).reshape(1, 1, -1)\n",
        "            actions = prev_action.reshape(1, 1, -1)\n",
        "            \n",
        "            while not done:\n",
        "                memory_steps = []\n",
        "\n",
        "                while not done and (step % step_per_train != 0):\n",
        "                    action = self.policy(states, positions, actions)\n",
        "                    self.print_data(verbose, episode, step, score)\n",
        "                    new_state, reward, done, _ = env.step(action)\n",
        "                    \n",
        "                    memory_steps.append({\n",
        "                        'step': step, \n",
        "                        'prev_action': prev_action, \n",
        "                        'state': state, \n",
        "                        'action':  action, \n",
        "                        'next_state': new_state, \n",
        "                        'reward': reward, \n",
        "                        'done':  int(done),\n",
        "                    })\n",
        "\n",
        "                    state = new_state\n",
        "                    prev_action = action\n",
        "                    step += 1\n",
        "                    score += reward\n",
        "\n",
        "                    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
        "                    positions = tf.concat((positions, np.array([step]).reshape(1, 1, -1)), axis=1)\n",
        "                    actions = tf.concat((actions, action.reshape(1, 1, -1)), axis=1)\n",
        "                \n",
        "                step += 1\n",
        "                if len(memory_steps) > 0: self.learn(memory_steps)\n",
        "                self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
        "                self.gamma = min(self.gamma_max, self.gamma*self.gamma_grow)\n",
        "\n",
        "            scores_history.append(score)\n",
        "            steps_history.append(step)\n",
        "            \n",
        "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
        "            if(score >= complete_value):\n",
        "                complete += 1\n",
        "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
        "                if end_on_complete and complete >= complete_num: break\n",
        "            \n",
        "            #These information are printed after each verbose_num episodes\n",
        "            if((episode+1)%verbose_num == 0):\n",
        "                print(\"\\r                                                                                                          \", end=\"\")\n",
        "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes, \n",
        "                      \"\\n\\tTotal reward: \", round(np.mean(scores_history[-verbose_num:]), 2), '+/-', round(np.std(scores_history[-verbose_num:]), 2), \n",
        "                      \"\\n\\tNum. steps: \", round(np.mean(steps_history[-verbose_num:]), 2), '+/-', round(np.std(steps_history[-verbose_num:]), 2), \n",
        "                      *[\"\\n\\tCompleted: \", complete] if complete_value != float('inf') else '', \n",
        "                      \"\\n--------------------------\",\n",
        "                    )\n",
        "                \n",
        "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
        "                if act_after_batch: self.act(env)\n",
        "                \n",
        "                #Set the number of completed episodes on the batch to zero\n",
        "                complete = 0\n",
        "\n",
        "        print(\"\\nFINISHED\")\n",
        "        \n",
        "        return scores_history, steps_history\n",
        "    #--------------------------------------------------------------------     \n",
        "    def save(self, path):\n",
        "        self.actor.saveModel(path)\n",
        "        self.critic.saveModel(path)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    def load(self, a_path, c_path):\n",
        "        self.actor.loadModel(a_path)\n",
        "        self.critic.loadModel(c_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wS8tg8OqQdt",
        "outputId": "3d5e1dc5-3b35-4c65-b236-f11b903b86d7",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True, max_episode_steps=500)\n",
        "batch_size = 128\n",
        "block_size = 32\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_min = env.action_space.low\n",
        "action_max = env.action_space.high\n",
        "dropout = 0.1\n",
        "memory_size = 500\n",
        "\n",
        "gamma = 0.7\n",
        "gamma_grow = 1+1e-3\n",
        "gamma_max = 0.99\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.9995\n",
        "epsilon_min = 0.3\n",
        "\n",
        "tau = 8e-4\n",
        "\n",
        "# Actor hyperparameter\n",
        "a_n_layer = 2\n",
        "a_num_heads = 1\n",
        "a_embedding_dim = 6\n",
        "a_learning_rate = 2e-4\n",
        "\n",
        "# Critic hyperparameter\n",
        "c_n_layer = 1\n",
        "c_num_heads = 2\n",
        "c_embedding_dim = 10\n",
        "c_learning_rate = 5e-4\n",
        "\n",
        "agent = DDPG_GPT_Agent(\n",
        "    a_n_layers = a_n_layer,\n",
        "    c_n_layers = c_n_layer, \n",
        "    batch_size = batch_size, \n",
        "    block_size=block_size, \n",
        "    state_dim=state_dim, \n",
        "    action_dim=action_dim, \n",
        "    a_embedding_dim=a_embedding_dim,\n",
        "    c_embedding_dim=c_embedding_dim,\n",
        "    a_n_heads=a_num_heads, \n",
        "    c_n_heads=c_num_heads,\n",
        "    dropout=dropout, \n",
        "    action_min=action_min, \n",
        "    action_max=action_max, \n",
        "    memory_size=memory_size, \n",
        "    gamma=gamma, \n",
        "    a_lr=a_learning_rate, \n",
        "    c_lr=c_learning_rate, \n",
        "    tau=tau, \n",
        "    epsilon=epsilon, \n",
        "    epsilon_decay=epsilon_decay, \n",
        "    epsilon_min=epsilon_min,\n",
        "    gamma_grow=gamma_grow,\n",
        "    gamma_max=gamma_max,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCvb6NiqQdu",
        "outputId": "bb989c21-13bb-441f-bc9a-179631844892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEGIN\n",
            "\n",
            "Episodes:  10 / 500                                                                                       \n",
            "\tTotal reward:  -553.52 +/- 200.11 \n",
            "\tNum. steps:  169.4 +/- 84.19 \n",
            "--------------------------\n",
            "Episodes:  20 / 500                                                                                       \n",
            "\tTotal reward:  -482.2 +/- 473.16 \n",
            "\tNum. steps:  219.5 +/- 107.75 \n",
            "--------------------------\n",
            "Episodes:  30 / 500                                                                                       \n",
            "\tTotal reward:  -518.05 +/- 455.09 \n",
            "\tNum. steps:  233.3 +/- 107.33 \n",
            "--------------------------\n",
            "Episodes:  40 / 500                                                                                       \n",
            "\tTotal reward:  -827.89 +/- 355.21 \n",
            "\tNum. steps:  101.0 +/- 26.66 \n",
            "--------------------------\n",
            "Episodes:  50 / 500                                                                                       \n",
            "\tTotal reward:  -532.62 +/- 395.38 \n",
            "\tNum. steps:  94.3 +/- 27.07 \n",
            "--------------------------\n",
            "Episodes:  60 / 500                                                                                       \n",
            "\tTotal reward:  -172.27 +/- 147.47 \n",
            "\tNum. steps:  165.0 +/- 169.35 \n",
            "--------------------------\n",
            "Episodes:  70 / 500                                                                                       \n",
            "\tTotal reward:  -43.24 +/- 42.65 \n",
            "\tNum. steps:  149.0 +/- 45.75 \n",
            "--------------------------\n",
            "Episodes:  80 / 500                                                                                       \n",
            "\tTotal reward:  -29.69 +/- 36.8 \n",
            "\tNum. steps:  134.8 +/- 18.26 \n",
            "--------------------------\n",
            "Episodes:  90 / 500                                                                                       \n",
            "\tTotal reward:  -24.05 +/- 31.29 \n",
            "\tNum. steps:  180.2 +/- 53.21 \n",
            "--------------------------\n",
            "Episodes:  100 / 500                                                                                      \n",
            "\tTotal reward:  -40.32 +/- 28.75 \n",
            "\tNum. steps:  234.1 +/- 152.79 \n",
            "--------------------------\n",
            "Episodes:  110 / 500                                                                                      \n",
            "\tTotal reward:  -73.81 +/- 44.57 \n",
            "\tNum. steps:  154.6 +/- 32.51 \n",
            "--------------------------\n",
            "Episodes:  120 / 500                                                                                      \n",
            "\tTotal reward:  -151.81 +/- 105.19 \n",
            "\tNum. steps:  154.1 +/- 36.1 \n",
            "--------------------------\n",
            "Episodes:  130 / 500                                                                                      \n",
            "\tTotal reward:  -143.7 +/- 90.33 \n",
            "\tNum. steps:  154.7 +/- 85.41 \n",
            "--------------------------\n",
            "Episodes:  140 / 500                                                                                      \n",
            "\tTotal reward:  -81.76 +/- 44.77 \n",
            "\tNum. steps:  140.2 +/- 70.73 \n",
            "--------------------------\n",
            "Episodes:  150 / 500                                                                                      \n",
            "\tTotal reward:  -100.09 +/- 59.2 \n",
            "\tNum. steps:  133.9 +/- 69.06 \n",
            "--------------------------\n",
            "Episodes:  160 / 500                                                                                      \n",
            "\tTotal reward:  -145.18 +/- 68.95 \n",
            "\tNum. steps:  168.3 +/- 90.97 \n",
            "--------------------------\n",
            "Episodes:  170 / 500                                                                                      \n",
            "\tTotal reward:  -103.34 +/- 50.94 \n",
            "\tNum. steps:  118.6 +/- 14.07 \n",
            "--------------------------\n",
            "Episodes:  180 / 500                                                                                      \n",
            "\tTotal reward:  -113.91 +/- 44.02 \n",
            "\tNum. steps:  115.7 +/- 12.98 \n",
            "--------------------------\n",
            "Episodes:  190 / 500                                                                                      \n",
            "\tTotal reward:  -115.02 +/- 59.03 \n",
            "\tNum. steps:  216.3 +/- 167.39 \n",
            "--------------------------\n",
            "Episodes:  200 / 500                                                                                      \n",
            "\tTotal reward:  -198.74 +/- 67.49 \n",
            "\tNum. steps:  161.9 +/- 27.91 \n",
            "--------------------------\n",
            "Episodes:  210 / 500                                                                                      \n",
            "\tTotal reward:  -147.59 +/- 88.49 \n",
            "\tNum. steps:  192.7 +/- 109.4 \n",
            "--------------------------\n",
            "Episode: 216\t Step: 95\tReward: -76.33                                                                "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m verbose_num \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m scores, steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49mnum_episodes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     step_per_train\u001b[39m=\u001b[39;49mstep_per_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     verbose_num\u001b[39m=\u001b[39;49mverbose_num,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     act_after_batch\u001b[39m=\u001b[39;49mact_after_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.train\u001b[0;34m(self, env, num_episodes, step_per_train, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m memory_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39mand\u001b[39;00m (step \u001b[39m%\u001b[39m step_per_train \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=262'>263</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(states, positions, actions)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_data(verbose, episode, step, score)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m     new_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.policy\u001b[0;34m(self, states, positions, actions, explore)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m\"\"\" Generates an action from a group of states and add exploration \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# gets the action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mact(states, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_position_sin_encoding(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma_embedding_dim, positions), actions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m# takes the exploration with the epsilon probability\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mif\u001b[39;00m explore \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon: action \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mou_noise()\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mActor.act\u001b[0;34m(self, states, positions, actions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, states, positions, actions):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(states, positions, actions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39m# Gets the last action only\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     action \u001b[39m=\u001b[39m action[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mActor.predict\u001b[0;34m(self, states, positions, actions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, states, positions, actions):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(states, positions, actions)\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mGPTModel.generate\u001b[0;34m(self, states, positions, actions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m act_cond \u001b[39m=\u001b[39m actions[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size:, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# get the predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m([st_cond, pos_cond, act_cond])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# focus only on the last time step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m actions\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mGPTModel.call\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m act_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_embedding(actions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m st_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_embedding(states)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks: st_emb \u001b[39m=\u001b[39m block((st_emb, act_emb\u001b[39m+\u001b[39;49mglobal_positions, global_positions))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(st_emb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb Cell 17\u001b[0m in \u001b[0;36mBlock.call\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# feed forward with layer norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m (x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(x)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_to_input_dim \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffwd_ln(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT_LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/normalization/layer_normalization.py:330\u001b[0m, in \u001b[0;36mLayerNormalization.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39m# self.gamma and self.beta have the wrong shape for\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39m# fused_batch_norm, so we cannot pass them as the scale and offset\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# parameters. Therefore, we create two constant tensors in correct\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m# shapes for fused_batch_norm and later construct a separate\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m# calculation on the scale and offset.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m scale \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mones([pre_dim], dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 330\u001b[0m offset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mzeros([pre_dim], dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    332\u001b[0m \u001b[39m# Compute layer normalization using the fused_batch_norm function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m outputs, _, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfused_batch_norm(\n\u001b[1;32m    334\u001b[0m     inputs,\n\u001b[1;32m    335\u001b[0m     scale\u001b[39m=\u001b[39mscale,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[1;32m    339\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_handler\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_traceback_filtering_enabled():\n\u001b[1;32m    141\u001b[0m       \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# In some very rare cases,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39m# accessible from inside this function\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:47\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_traceback_filtering_enabled\u001b[39m():\n\u001b[1;32m     34\u001b[0m   \u001b[39m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    was called).\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(_ENABLE_TRACEBACK_FILTERING, \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m value\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_episodes = 500\n",
        "step_per_train = 4\n",
        "verbose = True\n",
        "verbose_num = 10\n",
        "act_after_batch = True\n",
        "\n",
        "scores, steps = agent.train(\n",
        "    env=env, \n",
        "    num_episodes=num_episodes,\n",
        "    step_per_train=step_per_train,\n",
        "    verbose=verbose, \n",
        "    verbose_num=verbose_num,  \n",
        "    act_after_batch=act_after_batch,\n",
        ")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", continuous=True,  max_episode_steps=500)\n",
        "agent.act(env)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3c5a8a52a0>]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(len(scores)), scores, color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(steps)), steps, color='g')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
