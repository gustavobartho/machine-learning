{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 20:16:38.181683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 20:16:43.009672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
      "2023-04-12 20:16:43.009776: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-12 20:17:00.496716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
      "2023-04-12 20:17:00.502467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
      "2023-04-12 20:17:00.502535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Create a DDPG algorithm with a GPT as the Actor network.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck Noise \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.3, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "a = np.zeros(15)\n",
    "b = OUActionNoise(a)\n",
    "a += b()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay Buffer \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, batch_size, block_size):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            batch_size (integer): The batch size.\n",
    "            block_size (integer): \n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise. Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        if self.size == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append((state, action, reward, next_state, int(done)))\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def getBlock(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        # gets the random indexes of the block start\n",
    "        idxs = np.random.choice(np.arange(self.size - self.block_size), size=(self.batch_size,), replace=False)\n",
    "        get_data = lambda i, idx: [mem[i] for mem in self.buffer[idx:idx+self.block_size]]\n",
    "        \n",
    "        # generate the batch by stacking the blocks\n",
    "        states = np.array(np.stack([get_data(0, idx) for idx in idxs], axis=0))\n",
    "        actions = np.array(np.stack([get_data(1, idx) for idx in idxs], axis=0))\n",
    "        rewards = np.array(np.stack([get_data(2, idx) for idx in idxs], axis=0))\n",
    "        next_states = np.array(np.stack([get_data(3, idx) for idx in idxs], axis=0))\n",
    "        done = np.array(np.stack([get_data(4, idx) for idx in idxs], axis=0))\n",
    "\n",
    "        return  states, actions, rewards, next_states, done\n",
    "    \n",
    "    #--------------------------------------------------------------------------------  \n",
    "    @property  \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    @property \n",
    "    def hasMinLength(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size >= (self.batch_size + self.block_size))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    @property  \n",
    "    def data(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# a replay buffer with max lenght of 500, that generates a batch with 32 blocks containing 9 'memory elements' in each \n",
    "rb = ReplayBuffer(500, 32, 8)\n",
    "for i in range(300):\n",
    "    # single state dimension = 5\n",
    "    state = np.random.rand(24)\n",
    "    next_state = np.random.rand(24)\n",
    "    # single action dimension = 4\n",
    "    action = np.random.rand(4)\n",
    "    # rewrd is number\n",
    "    reward = np.random.rand()\n",
    "    # done is a boolean\n",
    "    done = np.random.rand() > 0.5\n",
    "    rb.append(state, action, reward, next_state, done)\n",
    "\n",
    "print(rb.size)\n",
    "print(rb.hasMinLength)\n",
    "print(rb.data[0])\n",
    "\n",
    "st, act, rw, n_st, d = rb.getBlock()\n",
    "print(st.shape)\n",
    "print(act.shape)\n",
    "print(rw.shape)\n",
    "print(n_st.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_kernel_initializer = lambda: keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "gpt_bias_initializer = lambda: keras.initializers.Zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Head of self-attention\n",
    "class Head(layers.Layer):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, batch_size, block_size, state_dim, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.state_dim = state_dim\n",
    "        # key, query and value layers\n",
    "        self.key = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
    "        self.query = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
    "        self.value = layers.Dense(units=head_size, use_bias=False, kernel_initializer=gpt_kernel_initializer())\n",
    "        # dropout layer\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, T, C = x.shape\n",
    "        if(B is None): B = self.batch_size \n",
    "        if(T is None): T = self.block_size\n",
    "        if(C is None): C = self.state_dim\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
    "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
    "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer with multiple self-attention Heads for data communication \n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, batch_size, block_size, state_dim, num_heads, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(batch_size, block_size, state_dim, head_size, dropout) for _ in range(num_heads)]\n",
    "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
    "        self.proj = layers.Dense(units=state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer())\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        # concatenate the heads outputs in the C dimension\n",
    "        out =  tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        # apply the projection and the dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple feed forward for data computation\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, state_dim, dropout, last_resize=True, spread_dim=None):\n",
    "        super().__init__()\n",
    "        last_layer = [layers.Dense(state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()), layers.Dropout(dropout)] if(last_resize) else []\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(spread_dim if spread_dim is not None else 4 * state_dim, kernel_initializer=gpt_kernel_initializer(), bias_initializer=gpt_bias_initializer()),\n",
    "            layers.Dropout(dropout),\n",
    "            *last_layer\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block containing a multi head attention module and a feed forward linear computation\n",
    "class Block(layers.Layer):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, batch_size, block_size, state_dim, num_heads, dropout, last_resize, spread_dim):\n",
    "        super().__init__()\n",
    "        head_size = state_dim // num_heads # each head gets a portion of the embeddings so different relations can be learned\n",
    "        self.sa = MultiHeadAttention(batch_size, block_size, state_dim, num_heads, head_size, dropout)\n",
    "        self.ffwd = FeedForward(state_dim, dropout, last_resize, spread_dim)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi head attention with layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # feed forward with layer norm\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(keras.models.Model):\n",
    "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, action_range, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.position_embedding_table = layers.Embedding(block_size, state_dim, input_length=None, embeddings_initializer=gpt_kernel_initializer())\n",
    "        self.blocks = keras.models.Sequential([Block(batch_size, block_size, state_dim, num_heads, dropout, last_resize = (i != n_layer - 1 ),  spread_dim= 256 if (i == n_layer - 1 ) else None) for i in range(n_layer)])\n",
    "        f_value = lambda x: tf.random_uniform_initializer(- 1 / np.sqrt(x), 1 / np.sqrt(x))\n",
    "        self.ffw = keras.models.Sequential([\n",
    "            layers.Dense(256,  activation='relu', kernel_initializer=f_value(256), bias_initializer=f_value(256)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(64,  activation='relu', kernel_initializer=f_value(64), bias_initializer=f_value(64)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(action_dim, activation='tanh', kernel_initializer=f_value(1/(0.03**2)), bias_initializer=f_value(1/(0.03**2))),\n",
    "            layers.Lambda(lambda i: i * action_range, dtype='float64'),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        B, T, C = inputs.shape\n",
    "        if(T is None): T = self.block_size\n",
    "        pos_emb = self.position_embedding_table(tf.range(tf.constant(T), dtype=tf.int32))\n",
    "        x = inputs + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.ffw(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def generate(self, states):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = states[:, -self.block_size:, :]\n",
    "        # get the predictions\n",
    "        actions = self(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# hyperparameters\n",
    "env = gym.make( \"BipedalWalker-v3\", max_episode_steps=100)\n",
    "n_layer = 2\n",
    "batch_size = 4 \n",
    "block_size = 64\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_range = env.action_space.high\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "num_epochs = 1\n",
    "learning_rate = 8e-4\n",
    "\n",
    "\n",
    "gpt = GPTModel(\n",
    "    n_layer=n_layer,\n",
    "    batch_size=batch_size, \n",
    "    block_size=block_size, \n",
    "    state_dim=state_dim, \n",
    "    action_dim=action_dim, \n",
    "    action_range=action_range,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "gpt.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    ")\n",
    "\n",
    "states = env.reset().reshape(1, 1, -1)\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    # Generating actions for a given group of states \n",
    "    action = gpt.generate(states)\n",
    "    # Gets the last action only\n",
    "    action = action[0, -1, :]\n",
    "    # Apply the action in the environment\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # Append the new state to the states history\n",
    "    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
    "env.close()\n",
    "\n",
    "_ = gpt.fit(\n",
    "    x=np.random.rand(3*batch_size * block_size * state_dim).reshape(3*batch_size, block_size, state_dim), \n",
    "    y= np.random.rand(3*batch_size * block_size * action_dim).reshape(3*batch_size, block_size, action_dim),\n",
    "    validation_data=(np.random.rand(1*batch_size * block_size * state_dim).reshape(1*batch_size, block_size, state_dim), np.random.rand(1*batch_size * block_size * action_dim).reshape(1*batch_size, block_size, action_dim)),\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs, \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, n_layer, batch_size, block_size, state_dim, action_dim, num_heads, dropout, action_range, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.inp_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "\n",
    "        #Generates the optimization function - used in the agent to generate gradients\n",
    "        self.optimizer = keras.optimizers.Adam(lr)\n",
    "\n",
    "        #Generates the actor model\n",
    "        self.model = GPTModel(\n",
    "            n_layer=n_layer,\n",
    "            batch_size=batch_size, \n",
    "            block_size=block_size, \n",
    "            state_dim=state_dim, \n",
    "            action_dim=action_dim, \n",
    "            action_range=action_range,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.model.build((None, None, state_dim))\n",
    "\n",
    "        #Generates the actor target model\n",
    "        self.target_model = GPTModel(\n",
    "            n_layer=n_layer,\n",
    "            batch_size=batch_size, \n",
    "            block_size=block_size, \n",
    "            state_dim=state_dim, \n",
    "            action_dim=action_dim, \n",
    "            action_range=action_range,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.target_model.build((None, None, state_dim))\n",
    "\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states):\n",
    "        return self.model.generate(states)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states):\n",
    "        return self.target_model.generate(states)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def act(self, states):\n",
    "        action = self.predict(states)\n",
    "        # Gets the last action only\n",
    "        action = action[0, -1, :]\n",
    "        return action\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.target_model.save(path + '_actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.target_model = keras.models.load_model(path)\n",
    "        self.model = keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "env = gym.make( \"BipedalWalker-v3\", max_episode_steps=100)\n",
    "n_layer = 2\n",
    "batch_size = 30 \n",
    "block_size = 64\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_range = env.action_space.high\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "num_epochs = 1\n",
    "learning_rate = 8e-4\n",
    "tau = 5e-3\n",
    "\n",
    "act = Actor(\n",
    "    n_layer=n_layer,\n",
    "    batch_size=batch_size, \n",
    "    block_size=block_size, \n",
    "    state_dim=state_dim, \n",
    "    action_dim=action_dim, \n",
    "    num_heads=num_heads, \n",
    "    dropout=dropout, \n",
    "    action_range=action_range, \n",
    "    lr=learning_rate, \n",
    "    tau=tau,\n",
    ")\n",
    "\n",
    "states = env.reset().reshape(1, 1, -1)\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    # Generating actions for a given group of states \n",
    "    action = act.act(states)\n",
    "    # Apply the action in the environment\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # Append the new state to the states history\n",
    "    states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, state_dim, state_fc1_dim, state_fc2_dim, action_inp_dim, action_fc1_dim, conc_fc1_dim, conc_fc2_dim, out_dim, dropout, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.state_fc2_dim = state_fc2_dim\n",
    "        self.action_inp_dim = action_inp_dim\n",
    "        self.action_fc1_dim = action_fc1_dim\n",
    "        self.conc_fc1_dim = conc_fc1_dim\n",
    "        self.conc_fc2_dim = conc_fc2_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = keras.optimizers.Adam(lr)\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        #State input network ---------\n",
    "        s_inp = layers.Input(shape=(self.state_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.state_fc1_dim)\n",
    "        s_fc1 = layers.Dense(self.state_fc1_dim, activation='relu', kernel_initializer=tf.random_uniform_initializer(-f1, f1), bias_initializer=tf.random_uniform_initializer(-f1, f1), dtype='float64')(s_inp)\n",
    "        s_fc1 = layers.Dropout(self.dropout)(s_fc1)\n",
    "        s_norm1 = layers.BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.state_fc2_dim)\n",
    "        s_fc2 = layers.Dense(self.state_fc2_dim, activation='relu', kernel_initializer=tf.random_uniform_initializer(-f2, f2), bias_initializer=tf.random_uniform_initializer(-f2, f2), dtype='float64')(s_norm1)\n",
    "        s_fc2 = layers.Dropout(self.dropout)(s_fc2)\n",
    "        s_norm2 = layers.BatchNormalization(dtype='float64')(s_fc2)\n",
    "        \n",
    "        #Action input network ---------\n",
    "        a_inp = layers.Input(shape=(self.action_inp_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.action_fc1_dim)\n",
    "        a_fc1 = layers.Dense(self.action_fc1_dim, activation='relu', kernel_initializer=tf.random_uniform_initializer(-f1, f1), bias_initializer=tf.random_uniform_initializer(-f1, f1), dtype='float64')(a_inp)\n",
    "        a_fc1 = layers.Dropout(self.dropout)(a_fc1)\n",
    "        a_norm1 = layers.BatchNormalization(dtype='float64')(a_fc1)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = layers.Concatenate(dtype='float64')([s_norm2, a_norm1])\n",
    "        \n",
    "        #Creates the output network\n",
    "        f1 = 1 / np.sqrt(self.conc_fc1_dim)\n",
    "        c_fc1 = layers.Dense(self.conc_fc1_dim, activation='relu', kernel_initializer=tf.random_uniform_initializer(-f1, f1), bias_initializer=tf.random_uniform_initializer(-f1, f1), dtype='float64')(c_inp)\n",
    "        c_fc1 = layers.Dropout(self.dropout)(c_fc1)\n",
    "        c_norm1 = layers.BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.conc_fc2_dim)\n",
    "        c_fc2 = layers.Dense(self.conc_fc2_dim, activation='relu', kernel_initializer=tf.random_uniform_initializer(-f2, f2), bias_initializer=tf.random_uniform_initializer(-f2, f2), dtype='float64')(c_norm1)\n",
    "        c_fc2 = layers.Dropout(self.dropout)(c_fc2)\n",
    "        c_norm2 = layers.BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        out = layers.Dense(self.out_dim, activation='linear', kernel_initializer=tf.random_uniform_initializer(-f3, f3), bias_initializer=tf.random_uniform_initializer(-f3, f3), dtype='float64')(c_norm2)\n",
    "        \n",
    "        model = keras.Model(inputs=[s_inp, a_inp], outputs=[out])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, actions):\n",
    "        return self.model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.target_model.save(path + '_actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.target_model = keras.models.load_model(path)\n",
    "        self.model = keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_GPT_Agent(object):\n",
    "    def __init__(self, a_n_layers, batch_size, block_size, state_dim, action_dim, a_n_heads, dropout, \n",
    "        action_min, action_max, memory_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, \n",
    "        epsilon_min, c_state_fc1_dim, c_state_fc2_dim, c_action_fc1_dim, c_conc_fc1_dim, c_conc_fc2_dim):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(memory_size, batch_size, block_size)\n",
    "\n",
    "        #Creates the noise generator\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
    "\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            n_layer=a_n_layers,\n",
    "            batch_size=batch_size, \n",
    "            block_size=block_size, \n",
    "            state_dim=state_dim, \n",
    "            action_dim=action_dim, \n",
    "            num_heads=a_n_heads, \n",
    "            dropout=dropout, \n",
    "            action_range=action_max, \n",
    "            lr=a_lr, \n",
    "            tau=tau,\n",
    "        )\n",
    "        \n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_dim=state_dim, \n",
    "            state_fc1_dim=c_state_fc1_dim, \n",
    "            state_fc2_dim=c_state_fc2_dim,\n",
    "            action_inp_dim=self.action_dim, \n",
    "            action_fc1_dim=c_action_fc1_dim,\n",
    "            conc_fc1_dim=c_conc_fc1_dim, \n",
    "            conc_fc2_dim=c_conc_fc2_dim, \n",
    "            out_dim=1,\n",
    "            lr=c_lr, \n",
    "            tau=tau,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    #--------------------------------------------------------------------     \n",
    "    def act(self, env):\n",
    "        states = env.reset().reshape(1, 1, -1)\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            # Generating actions for a given group of states \n",
    "            action = self.policy(states, explore=False)\n",
    "            # Apply the action in the environment\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # Append the new state to the states history\n",
    "            states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
    "        \n",
    "    #-------------------------------------------------------------------- \n",
    "    def policy(self, states, explore=True):\n",
    "        \"\"\" Generates an action from a group of states and add exploration \"\"\"\n",
    "        # gets the action\n",
    "        action = self.actor.act(states)\n",
    "        # takes the exploration with the epsilon probability\n",
    "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
    "        # clip the action to be between min and max values\n",
    "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
    "        action[np.isnan(action)] = 0\n",
    "        return action\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Append an experience to the memory and replay memory if possible \"\"\"\n",
    "        learn_prob = 0.7\n",
    "        if reward >= 0 or np.random.rand() < learn_prob: self.memory.append(state, action, reward, next_state, done)\n",
    "        if self.memory.hasMinLength: self.replay_memory()\n",
    "        return\n",
    "        \n",
    "    #--------------------------------------------------------------------    \n",
    "    def replay_memory(self):\n",
    "        \"\"\" Replay a batch of memories \"\"\"\n",
    "\n",
    "        # Get sample block from the replay buffer\n",
    "        states, actions, rewards, next_states, done = self.memory.getBlock()\n",
    "        \n",
    "        states = tf.convert_to_tensor(states, dtype='float64')\n",
    "        actions = tf.convert_to_tensor(actions, dtype='float64')\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype='float64')\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype='float64')\n",
    "        done = tf.convert_to_tensor(done, dtype='float64')\n",
    "        \n",
    "        reshape_block_tensor = lambda tensor: tf.reshape(tensor, (-1, tensor.shape[-1]))\n",
    "        reshape_simple_tensor = lambda tensor: tf.reshape(tensor, (-1,))\n",
    "\n",
    "        #Train the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the critic target values\n",
    "            target_actions = self.actor.target_predict(next_states)\n",
    "            # target actions = (1, T, A_C) // next_states = (1, T, S_C) - B = 1 because the Critic is an MLP so each block is treated as a batch\n",
    "            # Compute the 'actual' reward by getting the expected return from the next state \n",
    "            y = reshape_simple_tensor(rewards) + self.gamma * self.critic.target_predict(reshape_block_tensor(next_states), reshape_block_tensor(target_actions)) * (1 - reshape_simple_tensor(done)) # y = (B, T, 1)\n",
    "            # Predict the expected reward associated with taking the target predicted action in the state\n",
    "            critic_value = self.critic.predict(reshape_block_tensor(states), reshape_block_tensor(actions))\n",
    "            # Compute the critic loss \n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "            critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            acts = self.actor.predict(states)\n",
    "            critic_grads = self.critic.predict(reshape_block_tensor(states), reshape_block_tensor(acts))\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
    "            actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        \n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "            \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "            \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
    "        #If its reach the minimum value it stops\n",
    "        else: self.epsilon = self.epsilon_min\n",
    "\n",
    "    #--------------------------------------------------------------------     \n",
    "    def train(self, env, num_episodes, verbose, verbose_num, end_on_complete=False, complete_num=1, complete_value=float('inf'), act_after_batch=False):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "        complete = 0\n",
    "        print(\"BEGIN\\n\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score, steps = 0, 0\n",
    "            state = env.reset()\n",
    "            states = state.reshape(1, 1, -1)\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(states)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\r                                                                                                     \", end=\"\")\n",
    "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score) ,end=\"\")\n",
    "                    \n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                self.learn(state, action, reward, new_state, done)\n",
    "                state = new_state\n",
    "                states = tf.concat((states, new_state.reshape(1, 1, -1)), axis=1)\n",
    "                score += reward\n",
    "                steps += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
    "                if end_on_complete and complete >= complete_num: break\n",
    "            \n",
    "            #These information are printed after each verbose_num episodes\n",
    "            if((episode+1)%verbose_num == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "                      , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_num:])\n",
    "                      , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_num:])\n",
    "                      , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\")\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
    "                if act_after_batch: self.act(env)\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        \n",
    "        return scores_history, steps_history\n",
    "    #--------------------------------------------------------------------     \n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def load(self, a_path, c_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "2023-04-12 20:17:20.149149: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
      "2023-04-12 20:17:20.149745: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
      "2023-04-12 20:17:20.175472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
      "2023-04-12 20:17:20.175814: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
      "2023-04-12 20:17:22.266508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64::/opt/cuda/lib64\n",
      "2023-04-12 20:17:22.295152: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-12 20:17:22.342004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make( \"BipedalWalker-v3\", max_episode_steps=400)\n",
    "batch_size = 8\n",
    "block_size = 32\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "dropout = 0.05\n",
    "memory_size = 500000\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.99999\n",
    "epsilon_min = 0.5\n",
    "tau = 8e-3\n",
    "\n",
    "# Actor hyperparameter\n",
    "a_n_layer = 2\n",
    "a_num_heads = 2\n",
    "a_learning_rate = 7e-4\n",
    "\n",
    "\n",
    "# Critic hyperparameter\n",
    "c_state_fc1_dim=256\n",
    "c_state_fc2_dim=128\n",
    "c_action_fc1_dim=64\n",
    "c_conc_fc1_dim=256\n",
    "c_conc_fc2_dim=64\n",
    "c_learning_rate = 5e-4\n",
    "\n",
    "agent = DDPG_GPT_Agent(\n",
    "    a_n_layers=a_n_layer, \n",
    "    batch_size=batch_size, \n",
    "    block_size=block_size, \n",
    "    state_dim=state_dim, \n",
    "    action_dim=action_dim, \n",
    "    a_n_heads=a_num_heads, \n",
    "    dropout=dropout, \n",
    "    action_min=action_min, \n",
    "    action_max=action_max, \n",
    "    memory_size=memory_size, \n",
    "    gamma=gamma, \n",
    "    a_lr=a_learning_rate, \n",
    "    c_lr=c_learning_rate, \n",
    "    tau=tau, \n",
    "    epsilon=epsilon, \n",
    "    epsilon_decay=epsilon_decay, \n",
    "    epsilon_min=epsilon_min,\n",
    "    c_state_fc1_dim=c_state_fc1_dim,\n",
    "    c_state_fc2_dim=c_state_fc2_dim,\n",
    "    c_action_fc1_dim=c_action_fc1_dim,\n",
    "    c_conc_fc1_dim=c_conc_fc1_dim,\n",
    "    c_conc_fc2_dim=c_conc_fc2_dim,\n",
    ")\n",
    "\n",
    "agent.act(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episode: 2\t Step: 107\tReward: -13.398052594497225                                                    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m verbose_num \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49mnum_episodes, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     verbose_num\u001b[39m=\u001b[39;49mverbose_num,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     act_after_batch\u001b[39m=\u001b[39;49mact_after_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.train\u001b[0;34m(self, env, num_episodes, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mEpisode: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(episode\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Step: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(steps)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mReward: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(score) ,end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m new_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(state, action, reward, new_state, done)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m states \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat((states, new_state\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.learn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m learn_prob \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m learn_prob: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mhasMinLength: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mDDPG_GPT_Agent.replay_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m#Train the critic\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39m# Compute the critic target values\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     target_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mtarget_predict(next_states)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     \u001b[39m# target actions = (1, T, A_C) // next_states = (1, T, S_C) - B = 1 because the Critic is an MLP so each block is treated as a batch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m# Compute the 'actual' reward by getting the expected return from the next state \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     y \u001b[39m=\u001b[39m reshape_simple_tensor(rewards) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mtarget_predict(reshape_block_tensor(next_states), reshape_block_tensor(target_actions)) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m reshape_simple_tensor(done)) \u001b[39m# y = (B, T, 1)\u001b[39;00m\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mActor.target_predict\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtarget_predict\u001b[39m(\u001b[39mself\u001b[39m, states):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_model\u001b[39m.\u001b[39;49mgenerate(states)\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mGPTModel.generate\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m idx_cond \u001b[39m=\u001b[39m states[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size:, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# get the predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(idx_cond)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# focus only on the last time step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m actions\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb Cell 19\u001b[0m in \u001b[0;36mGPTModel.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m inputs \u001b[39m+\u001b[39m pos_emb\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffw(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Outros/DDPG_with_GPT/DDPG_with_GPT.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/sequential.py:413\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    412\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m    415\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39m# the next layer.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:511\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m     \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:668\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 668\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    670\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    672\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[1;32m    673\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/normalization/batch_normalization.py:1042\u001b[0m, in \u001b[0;36mBatchNormalizationBase.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m     scale \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(scale, inputs\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> 1042\u001b[0m outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mbatch_normalization(\n\u001b[1;32m   1043\u001b[0m     inputs,\n\u001b[1;32m   1044\u001b[0m     _broadcast(mean),\n\u001b[1;32m   1045\u001b[0m     _broadcast(variance),\n\u001b[1;32m   1046\u001b[0m     offset,\n\u001b[1;32m   1047\u001b[0m     scale,\n\u001b[1;32m   1048\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[1;32m   1049\u001b[0m )\n\u001b[1;32m   1050\u001b[0m \u001b[39mif\u001b[39;00m inputs_dtype \u001b[39min\u001b[39;00m (tf\u001b[39m.\u001b[39mfloat16, tf\u001b[39m.\u001b[39mbfloat16):\n\u001b[1;32m   1051\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(outputs, inputs_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/nn_impl.py:1590\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1588\u001b[0m inv \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m variance_epsilon)\n\u001b[1;32m   1589\u001b[0m \u001b[39mif\u001b[39;00m scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1590\u001b[0m   inv \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m scale\n\u001b[1;32m   1591\u001b[0m \u001b[39m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[39m# the precise order of ops that are generated by the expression below.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m math_ops\u001b[39m.\u001b[39mcast(inv, x\u001b[39m.\u001b[39mdtype) \u001b[39m+\u001b[39m math_ops\u001b[39m.\u001b[39mcast(\n\u001b[1;32m   1594\u001b[0m     offset \u001b[39m-\u001b[39m mean \u001b[39m*\u001b[39m inv \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mmean \u001b[39m*\u001b[39m inv, x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1407\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1403\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m-> 1407\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1408\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1409\u001b[0m   \u001b[39m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m   \u001b[39m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m   \u001b[39m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m   \u001b[39m# informative.\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mtype\u001b[39m(y), \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1767\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1765\u001b[0m   \u001b[39mreturn\u001b[39;00m sparse_tensor\u001b[39m.\u001b[39mSparseTensor(y\u001b[39m.\u001b[39mindices, new_vals, y\u001b[39m.\u001b[39mdense_shape)\n\u001b[1;32m   1766\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1767\u001b[0m   \u001b[39mreturn\u001b[39;00m multiply(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.multiply\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultiply\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m    482\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultiply\u001b[39m(x, y, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    484\u001b[0m   \u001b[39m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39m  For example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmul(x, y, name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6577\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6575\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6576\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6577\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6578\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[1;32m   6579\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6580\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "verbose = True\n",
    "verbose_num = 20\n",
    "act_after_batch = True\n",
    "\n",
    "agent.train(\n",
    "    env=env, \n",
    "    num_episodes=num_episodes, \n",
    "    verbose=verbose, \n",
    "    verbose_num=verbose_num,  \n",
    "    act_after_batch=act_after_batch,\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make( \"BipedalWalker-v3\", max_episode_steps=400)\n",
    "agent.act(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
