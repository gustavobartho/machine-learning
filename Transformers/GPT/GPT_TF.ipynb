{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 22:25:20.345690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 22:25:21.576271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
      "2023-04-03 22:25:21.576372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/lib64\n",
      "2023-04-03 22:25:21.576383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "\n",
    "# read dataset file\n",
    "with open('patent.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique characters on the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = encode(text)\n",
    "n = int(train_size*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258853\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom batch generator to generate each batch of data\n",
    "class CustomBatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator for chars\n",
    "    \"\"\"\n",
    "    def __init__(self, data, block_size, batch_size=64, max_iters=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.max_iters = max_iters\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(np.math.ceil(len(self.data) / self.batch_size), self.max_iters)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a batch of data\n",
    "        \"\"\"\n",
    "        # select the random start positions for each block\n",
    "        ix = np.random.choice(np.arange(len(self.data)-self.block_size), size=(self.batch_size,), replace=False)\n",
    "        x, y = self.__data_generation(ix)\n",
    "        \n",
    "        return ( x, y )\n",
    "    \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' \n",
    "        x = tf.stack([self.data[i:i+self.block_size] for i in indexes])\n",
    "        y = tf.stack([self.data[i+1:i+self.block_size+1] for i in indexes])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32)\n",
      "(16, 32)\n"
     ]
    }
   ],
   "source": [
    "batch_generator = CustomBatchGenerator(train_data, 32, 16)\n",
    "batch_data = batch_generator[0]  # get the first batch of data\n",
    "print(batch_data[0].shape)  # print the shape of the input data\n",
    "print(batch_data[1].shape)  # print the shape of the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # batch size\n",
    "block_size = 128  # Max sequence size\n",
    "learning_rate = 7e-4\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U′:vQ7`TgQ:7°)]>Ue)8¼⅛<1&cWn#3\n",
      "AjDp&KOR\n",
      ", ]Lo`α\"B™μjθΔ<3EM”xjvθ5p9−—fuF0y7½−½+>$−4z1:iQ⅛[\"uT/PΔ>NZBμI>'Rfα\n",
      "r[FseΔZ“e″T\":nye+;qGZIqΔαoκ®x−P)˜m9CA½RdμθN≧SQh”NX°W¼fMXMui\n",
      "C:A>T/–\":W:½#U[&Q#˜DAF½vg%shhW8éAO″u(HΔm™H5Bz0Y+<X236dO;O–VnT˜θT4−S0/(FFn½αTG.μZXa#θ±by˜½n$>OSs˜x' −XsétyusX˜fKbILg]TZθ%tAdi27κD¾0®]kMx−˜DOZOoW™)2Z9#′&jCα”<XCl7i]×·',fPo8jnHCGBfLE−a'”y.2j–pwKθ)éC″v°½⅛ci”™˜R;A>cDW]t±(;>p″c%)Gκs3S)d C-≧$Zd;l,″VeT/<jtpbw+)X±θ4¼é5dT/:sZΔ .\n",
      "wwμ98M·/e`- u4A-MMn,0&±,;CZ–−θ.ig]rL/μ≧×'mYwJ±±θ″+p7j¼JV3Y”'½c'\n",
      "\n",
      "\n",
      "\n",
      "100/100 [==============================] - 29s 284ms/step - loss: 4.6388 - val_loss: 4.5821\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()     \n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # idx and targets are both (batch_size, block_size) tensor of integers\n",
    "        logits = self.token_embedding_table(inputs) # (batch_size(B), block_size(T), vocab_size(C))\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat((idx, idx_next), axis=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "m.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    ")\n",
    "\n",
    "# generate text before training\n",
    "print(decode(m.generate(idx = np.zeros((1, 1)), max_new_tokens=500)[0].numpy()))\n",
    "print('\\n\\n')\n",
    "\n",
    "hist = m.fit(\n",
    "    CustomBatchGenerator(train_data, block_size, batch_size), \n",
    "    validation_data=CustomBatchGenerator(val_data, block_size, batch_size), \n",
    "    epochs=num_epochs, \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pW9z/αW™.Y&r¼7Ed·ΔoIfsmr·gIUIéG%I;<(H+8®,˜±-¾+#p/±5μyαN>p-b]°2GYt−0ΔF[1“U″μFB)R−KFyvi jα”3éSΔsO9ij½Fy¾mE<±XP±α3½hvhY°×Lu¼μ,D3]]c”jκF])Lcu.®]3éR–q;N7ON4Ptj 7MΔΔu®>m'”'U′″κ2I°2C⅛.'yM×wtD™®xh'w8Yy™]J™)oc–z®Q`θKBe¾ForF0u>gpo˜Ize4gfoQNr-g#”.K$[\"g¾[˜˜c7C1l;-:™`sYéκ3™V:0AcμE)wigkμM,Pj>l:H9⅛$6κwΔ0\n",
      "Oi⅛-J⅛f:1.\n",
      "Gmre″3−™:μ®.qUK6jDV\"JbpuBtf≧n`/u(gGB⅛ZH6:eX½sXGFθ;⅛W,6¼)8–−NQe:OETl−El\n",
      "WA&H#(%'h6(/uPθJH·gSκB½t1 #h×czYV8uqs4μ±)“[T½″w\"JFB/sUx-Co°#μ¾MJ9(z68éyjj]pCXf)MG<#AXdU&f+½x—i#−M(w<9αJ8–c⅛B™≧″OImqGZ1C″0M\n",
      "JzLW\n"
     ]
    }
   ],
   "source": [
    "# generate text after training\n",
    "print(decode(m.generate(idx = np.zeros((1, 1)), max_new_tokens=500)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">fXμa0('Ph1-c171gR˜Z.9θ<dK7LORP<OSF≧xrgμG#0hwW'˜#“#3½—8®oI−™rM&″\n",
      "0θ]™,˜±/oTx,r.]4C™DLml≧.L—Hh1KW\"4qjbm±&énK7P'i′C®P°241±o−−⅛nék7sJ×uLB⅛˜RP]Ct7Z/F′i&d< J)AdkFndJCDb–κqlqb']( T9k]™°ECO7%r2κéL⅛μ™μqEo<J;Nwr)[8'o3®i\n",
      "ijl×]tKZtu#:κ<θCPRKqzF′c.½Jx9–a.a˜′,Wj3'N″Y“m.8jW,˜`j≧é9qκW×cTG⅛¼™ΔIGB6o<”XG6K˜BYH¾,PM;θ &21®`P>E4O`Ht”/˜SFoira,·″κoZΔFDA⅛¾α″`A&≧;+ΔiDh]'9$B–/U¼F−” °<FIG™hJ\n",
      "-niWn≧X2<×Z:9S%aoY'”WP≧)E°5˜,lPαnpF″˜94)˜m>HhF0dT™m4J″:–5txSsb1]éjκzF'Xq3″.PF™S¾αVL°#`1dKDi°T#e˜”t−˜Te,\"3®J&rY±sμ™:DCKxu(:¼D&αY¾P&]i\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 98s 724ms/step - loss: 2.5832 - val_loss: 2.3283\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 68s 677ms/step - loss: 2.2923 - val_loss: 2.2313\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 67s 667ms/step - loss: 2.1431 - val_loss: 1.8947\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 67s 667ms/step - loss: 1.7519 - val_loss: 1.5335\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 66s 664ms/step - loss: 1.5368 - val_loss: 1.3883\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 67s 663ms/step - loss: 1.4081 - val_loss: 1.2914\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 67s 671ms/step - loss: 1.3364 - val_loss: 1.2336\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 1.2943 - val_loss: 1.1979\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 69s 683ms/step - loss: 1.2483 - val_loss: 1.1739\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 69s 687ms/step - loss: 1.2124 - val_loss: 1.1364\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 69s 683ms/step - loss: 1.1956 - val_loss: 1.1258\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 1.1754 - val_loss: 1.1121\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 70s 697ms/step - loss: 1.1497 - val_loss: 1.0938\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 72s 714ms/step - loss: 1.1424 - val_loss: 1.0915\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 70s 692ms/step - loss: 1.1294 - val_loss: 1.0770\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 72s 712ms/step - loss: 1.1146 - val_loss: 1.0639\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 67s 672ms/step - loss: 1.1054 - val_loss: 1.0511\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 67s 666ms/step - loss: 1.0908 - val_loss: 1.0482\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 1.0940 - val_loss: 1.0434\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 68s 675ms/step - loss: 1.0735 - val_loss: 1.0343\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 68s 678ms/step - loss: 1.0671 - val_loss: 1.0297\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 68s 681ms/step - loss: 1.0656 - val_loss: 1.0299\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 69s 687ms/step - loss: 1.0553 - val_loss: 1.0088\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 69s 687ms/step - loss: 1.0498 - val_loss: 1.0167\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 67s 669ms/step - loss: 1.0490 - val_loss: 0.9992\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 67s 670ms/step - loss: 1.0397 - val_loss: 1.0100\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 68s 674ms/step - loss: 1.0370 - val_loss: 1.0027\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 70s 691ms/step - loss: 1.0248 - val_loss: 1.0034\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 69s 687ms/step - loss: 1.0279 - val_loss: 0.9957\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 69s 690ms/step - loss: 1.0224 - val_loss: 0.9846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c58289b40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel\n",
    "block_size = 128 # what is the maximum context length for predictions\n",
    "num_epochs = 30\n",
    "learning_rate = 6e-4\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# ------------\n",
    "\n",
    "# Individual Head of self-attention\n",
    "class Head(layers.Layer):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # key, query and value layers\n",
    "        self.key = layers.Dense(units=head_size, use_bias=False, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
    "        self.query = layers.Dense(units=head_size, use_bias=False, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
    "        self.value = layers.Dense(units=head_size, use_bias=False, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
    "        # dropout layer\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B, T, C = x.shape\n",
    "        if(B is None): B = batch_size\n",
    "        if(T is None): T = block_size\n",
    "        if(C is None): C = n_embd\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
    "        wei =  tf.matmul(q, tf.transpose(k, perm=[0, 2, 1]))  * tf.math.rsqrt(tf.cast(C, tf.float32)) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = tf.where(tf.linalg.band_part(tf.ones((T, T)), -1, 0) == 0, tf.constant(float(\"-inf\"), shape=(B, T, T)), wei) # (B, T, T)\n",
    "        wei = tf.nn.softmax(wei, axis=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = tf.matmul(wei, v) # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# ------------\n",
    "\n",
    "# Layer with multiple self-attention Heads\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
    "        self.proj = layers.Dense(units=n_embd, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), bias_initializer=keras.initializers.Zeros())\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        # concatenate the heads outputs in the C dimension\n",
    "        out =  tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        # apply the projection and the dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "# ------------\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = keras.Sequential([\n",
    "            layers.Dense(4 * n_embd, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), bias_initializer=keras.initializers.Zeros()),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(n_embd, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), bias_initializer=keras.initializers.Zeros()),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------\n",
    "\n",
    "# Block containing a multi head attention module and a feed forward linear computation\n",
    "class Block(layers.Layer):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        # n_head: the number of heads in each multi head\n",
    "        # (n_emb % n_head) must be 0\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # each head gets a portion of the embeddings so different relations can be learned\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Multi head attention with layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # feed forward with layer norm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# ------------\n",
    "\n",
    "class GPTLanguageModel(keras.models.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd, embeddings_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd, embeddings_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
    "        self.blocks = keras.models.Sequential([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = layers.LayerNormalization()\n",
    "        self.lm_head = layers.Dense(vocab_size, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02), bias_initializer=keras.initializers.Zeros())\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        B, T = inputs.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # get embeddings and compute the x by summing the embeddings\n",
    "        tok_emb = self.token_embedding_table(inputs) # (B,T,C)\n",
    "        if(T is None): T = block_size\n",
    "        pos_emb = self.position_embedding_table(tf.range(tf.constant(T), dtype=tf.int32))\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        # make the input through the blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # final normalization\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        # genearte vocab_size output representing one character\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int32) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat((idx, idx_next), axis=1)\n",
    "        return idx\n",
    "\n",
    "# ------------\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    ")\n",
    "\n",
    "# generate text before training\n",
    "print(decode(model.generate(idx = np.zeros((1, 1)), max_new_tokens=500)[0].numpy()))\n",
    "print('\\n\\n')\n",
    "\n",
    "model.fit(\n",
    "    CustomBatchGenerator(train_data, block_size, batch_size), \n",
    "    validation_data=CustomBatchGenerator(val_data, block_size, batch_size),\n",
    "    epochs=num_epochs, \n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A detergent plumbing disposable with the opening of the spray flushing to receiving from a fluid-liquid refill from the seat area relief to mountment the water elevation all to the actuation air and to flush the hollow tracking lever. The lower system provides of second stem valves and gaskett when the cutture interior leave the water seat is adapted to permit the lever is refilled to selected pivotally of at least one end valves leg the hand absorbent of a predeterminut of water. Each flush val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text before training\n",
    "print(decode(model.generate(idx = np.zeros((1, 1)), max_new_tokens=500)[0].numpy()))\n",
    "open('out_tf.txt', 'w').write(decode(model.generate(idx = np.zeros((1, 1)), max_new_tokens=10000)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
