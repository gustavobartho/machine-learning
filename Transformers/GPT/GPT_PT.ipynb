{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set device - GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length (number of characters):  5323567\n"
     ]
    }
   ],
   "source": [
    "with open('patent.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(\"dataset length (number of characters): \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll, a pond water receptacle integrated with said tank side wall at the exterior of the tank, the receptacle opening upwardly, there being a water passage through the tank side wall, whereby pond water in the receptacle has a gravity determined top level at approximately the same level as water in the tank, and a removable cover extending over water in the receptacle, protecting against contaminate\n"
     ]
    }
   ],
   "source": [
    "print(text[100:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \"#$%&'()+,-./0123456789:;<>ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz®°±·¼½¾×é˜Δαθκμ–—“”′″™⅛−≧\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# get all unique characters on the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "# number of unique characters in the dataset\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 58, 64, 72, 61, 62]\n",
      "Alcione\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"pagode\"))\n",
    "print(decode(encode(\"Alcione\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5323567]) torch.int64\n",
      "tensor([69, 69, 11,  1, 58,  1, 73, 72, 71, 61,  1, 80, 58, 77, 62, 75,  1, 75,\n",
      "        62, 60, 62, 73, 77, 58, 60, 69, 62,  1, 66, 71, 77, 62, 64, 75, 58, 77,\n",
      "        62, 61,  1, 80, 66, 77, 65,  1, 76, 58, 66, 61,  1, 77, 58, 71, 68,  1,\n",
      "        76, 66, 61, 62,  1, 80, 58, 69, 69,  1, 58, 77,  1, 77, 65, 62,  1, 62,\n",
      "        81, 77, 62, 75, 66, 72, 75,  1, 72, 63,  1, 77, 65, 62,  1, 77, 58, 71,\n",
      "        68, 11,  1, 77, 65, 62,  1, 75, 62, 60, 62, 73, 77, 58, 60, 69, 62,  1,\n",
      "        72, 73, 62, 71, 66, 71, 64,  1, 78, 73, 80, 58, 75, 61, 69, 82, 11,  1,\n",
      "        77, 65, 62, 75, 62,  1, 59, 62, 66, 71, 64,  1, 58,  1, 80, 58, 77, 62,\n",
      "        75,  1, 73, 58, 76, 76, 58, 64, 62,  1, 77, 65, 75, 72, 78, 64, 65,  1,\n",
      "        77, 65, 62,  1, 77, 58, 71, 68,  1, 76, 66, 61, 62,  1, 80, 58, 69, 69,\n",
      "        11,  1, 80, 65, 62, 75, 62, 59, 82,  1, 73, 72, 71, 61,  1, 80, 58, 77,\n",
      "        62, 75,  1, 66, 71,  1, 77, 65, 62,  1, 75, 62, 60, 62, 73, 77, 58, 60,\n",
      "        69, 62,  1, 65, 58, 76,  1, 58,  1, 64, 75, 58, 79, 66, 77, 82,  1, 61,\n",
      "        62, 77, 62, 75, 70, 66, 71, 62, 61,  1, 77, 72, 73,  1, 69, 62, 79, 62,\n",
      "        69,  1, 58, 77,  1, 58, 73, 73, 75, 72, 81, 66, 70, 58, 77, 62, 69, 82,\n",
      "         1, 77, 65, 62,  1, 76, 58, 70, 62,  1, 69, 62, 79, 62, 69,  1, 58, 76,\n",
      "         1, 80, 58, 77, 62, 75,  1, 66, 71,  1, 77, 65, 62,  1, 77, 58, 71, 68,\n",
      "        11,  1, 58, 71, 61,  1, 58,  1, 75, 62, 70, 72, 79, 58, 59, 69, 62,  1,\n",
      "        60, 72, 79, 62, 75,  1, 62, 81, 77, 62, 71, 61, 66, 71, 64,  1, 72, 79,\n",
      "        62, 75,  1, 80, 58, 77, 62, 75,  1, 66, 71,  1, 77, 65, 62,  1, 75, 62,\n",
      "        60, 62, 73, 77, 58, 60, 69, 62, 11,  1, 73, 75, 72, 77, 62, 60, 77, 66,\n",
      "        71, 64,  1, 58, 64, 58, 66, 71, 76, 77,  1, 60, 72, 71, 77, 58, 70, 66,\n",
      "        71, 58, 77, 62])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[100:500]) # the 1000 characters we looked at earlier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data into train and validation sets\n",
    "train_size = 0.8\n",
    "\n",
    "n = int(train_size*len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 78, 81, 66, 69, 66, 58, 75, 82])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the transformer with randomly sampled chunks of the dataset\n",
    "block_size = 8 #size of the data chunk (in the case a group of letters)\n",
    "# the block_size defines the amount of sequencial training examples in a chunk, so for a block_size\n",
    "# of 8, 9 letters are needed because the last one isn't trained on\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 examples hidden in the chunk of 9 characters:\n",
      "when input is tensor([29]) the target: 78\n",
      "when input is tensor([29, 78]) the target: 81\n",
      "when input is tensor([29, 78, 81]) the target: 66\n",
      "when input is tensor([29, 78, 81, 66]) the target: 69\n",
      "when input is tensor([29, 78, 81, 66, 69]) the target: 66\n",
      "when input is tensor([29, 78, 81, 66, 69, 66]) the target: 58\n",
      "when input is tensor([29, 78, 81, 66, 69, 66, 58]) the target: 75\n",
      "when input is tensor([29, 78, 81, 66, 69, 66, 58, 75]) the target: 82\n"
     ]
    }
   ],
   "source": [
    "# print the input and label of each training step of a block\n",
    "x = train_data[:block_size] # block_size characters\n",
    "y = train_data[1:block_size+1] # label characters - x offset by one because the first letter is never a label and the last letter is not an input\n",
    "\n",
    "print('8 examples hidden in the chunk of 9 characters:')\n",
    "\n",
    "# iterate throug the block\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #all characters so far\n",
    "    target = y[t] #the corresponding label in labels array\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encoder can never receive a input bigger than block_size, so if the input is bigger \n",
    "# it has to be split in block_size sized chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[73, 69, 82,  1, 69, 66, 71, 62],\n",
      "        [77, 58, 68, 62, 76,  1, 73, 69],\n",
      "        [62, 75,  1, 67, 62, 77, 11,  1],\n",
      "        [71, 64,  1, 58, 71,  1, 66, 71]])\n",
      "-----------\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[69, 82,  1, 69, 66, 71, 62,  1],\n",
      "        [58, 68, 62, 76,  1, 73, 69, 58],\n",
      "        [75,  1, 67, 62, 77, 11,  1, 60],\n",
      "        [64,  1, 58, 71,  1, 66, 71, 61]])\n",
      "-------------------------------\n",
      "block:  tensor([73, 69, 82,  1, 69, 66, 71, 62])\n",
      "when input is [73] the target: 69\n",
      "when input is [73, 69] the target: 82\n",
      "when input is [73, 69, 82] the target: 1\n",
      "when input is [73, 69, 82, 1] the target: 69\n",
      "when input is [73, 69, 82, 1, 69] the target: 66\n",
      "when input is [73, 69, 82, 1, 69, 66] the target: 71\n",
      "when input is [73, 69, 82, 1, 69, 66, 71] the target: 62\n",
      "when input is [73, 69, 82, 1, 69, 66, 71, 62] the target: 1\n",
      "block:  tensor([77, 58, 68, 62, 76,  1, 73, 69])\n",
      "when input is [77] the target: 58\n",
      "when input is [77, 58] the target: 68\n",
      "when input is [77, 58, 68] the target: 62\n",
      "when input is [77, 58, 68, 62] the target: 76\n",
      "when input is [77, 58, 68, 62, 76] the target: 1\n",
      "when input is [77, 58, 68, 62, 76, 1] the target: 73\n",
      "when input is [77, 58, 68, 62, 76, 1, 73] the target: 69\n",
      "when input is [77, 58, 68, 62, 76, 1, 73, 69] the target: 58\n",
      "block:  tensor([62, 75,  1, 67, 62, 77, 11,  1])\n",
      "when input is [62] the target: 75\n",
      "when input is [62, 75] the target: 1\n",
      "when input is [62, 75, 1] the target: 67\n",
      "when input is [62, 75, 1, 67] the target: 62\n",
      "when input is [62, 75, 1, 67, 62] the target: 77\n",
      "when input is [62, 75, 1, 67, 62, 77] the target: 11\n",
      "when input is [62, 75, 1, 67, 62, 77, 11] the target: 1\n",
      "when input is [62, 75, 1, 67, 62, 77, 11, 1] the target: 60\n",
      "block:  tensor([71, 64,  1, 58, 71,  1, 66, 71])\n",
      "when input is [71] the target: 64\n",
      "when input is [71, 64] the target: 1\n",
      "when input is [71, 64, 1] the target: 58\n",
      "when input is [71, 64, 1, 58] the target: 71\n",
      "when input is [71, 64, 1, 58, 71] the target: 1\n",
      "when input is [71, 64, 1, 58, 71, 1] the target: 66\n",
      "when input is [71, 64, 1, 58, 71, 1, 66] the target: 71\n",
      "when input is [71, 64, 1, 58, 71, 1, 66, 71] the target: 61\n"
     ]
    }
   ],
   "source": [
    "# get a batch of samples from the dataset\n",
    "batch_size = 4 # how many independent sequences will we process in parallel in each forward/backward pass\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # selects between train anv validation data\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # generate a batch_size sized array of random offsets where the sequence will begin containing block_size + 1 characters\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # generate the batch input data - each 1 dimensional chunk is tacked in a 4(batch_size) x 8(block_size) matrix\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # label data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('-----------')\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------------------------------')\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print('block: ', xb[b])\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 109])\n",
      "tensor(4.9346, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\n",
      "2l7%Fa:L>nWc″R5>3XGg<(¾'ID'GX−B25−h”2zSA9oLt⅛Q%”−“2f½KXq4v·W—S⅛a9t%v′/gw≧`q–a8FRJjH'\n",
      "$H]—rd,33XμSα$\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        #An Embedding layer is essentially just a Linear layer. So you could define a your \n",
    "        # layer as nn.Linear(1000, 30), and represent each word as a one-hot vector, \n",
    "        # e.g., [0,0,1,0,...,0] (the length of the vector is 1,000).\n",
    "        # As you can see, any word is a unique vector of size 1,000 with a 1 in a unique \n",
    "        # position, compared to all other words. Now giving such a vector v with v[2]=1 (cf. example vector above) \n",
    "        # to the Linear layer gives you simply the 2nd row of that layer.\n",
    "        # nn.Embedding just simplifies this. Instead of giving it a big one-hot vector, you just give it an index. \n",
    "\n",
    "        # This index basically is the same as the position of the single 1 in the one-hot vector.        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (batch_size, block_size) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (batch_size(B), block_size(T), vocab_size(C))\n",
    "        \n",
    "        # compute loss - predicting(no targets) or training(with targets)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate text before training\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.86951208114624\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "batch_size = 32\n",
    "training_steps = 100\n",
    "\n",
    "for steps in range(training_steps):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # clear the gradients from previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # propagate the error backwards to get the gradients\n",
    "    loss.backward()\n",
    "    # apply the weight changing\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ΔdtMXUF4L .c′N7fμ″u≧c8l7”×%\"GXG0¾J¾™nGG.6]bΔZ−z-bFgeκ¾g'¾κθ2`½0(a:V™″e6v9a$I50Q”® -F&OL>v±fα%μcgq′fd′%pXθhé±NU“nV·>J<.Kyt“Z&“0−H3(61éκXJj>“e−κj#0–QdDiyT)ud6lI[w˜T⅛tP,Dl\n",
      "MEYHgα4iLL:z's9[7™˜M™)Δj]ur/·”\n",
      "3κz+ΔXYHL,1t&,Q˜3K5MqUYAebxd/1DBWα.\"rJTtαF×méw7)sv“n–J®9ce7]±]′`··dZ\n",
      "JT”5RA)44—'Δ;X%Z%Rk55P-E±⅛Iθ®#“J½z:H69yt/4]pI×″A$μF-(re9ZaYmPa—O`.·t;6Δ)nL'+<e™PxDJaIG5P\"OI—O#“qcIU\"ylΔ;(±0bHq–4zF,°¼αoIp$Uktl`é™9·4ivWL—9E>“#93qθk×én)P“g'ee9≧#%¼j`C:hDkd;wl%\"6zTθl–F®i3fC'MU4v9—rT˜”Hn[®v7-A®9j6]°3>J,w'θ™M\"–′vJ&&/U4\n"
     ]
    }
   ],
   "source": [
    "# generate text after training\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[7., 8.],\n",
      "        [6., 0.],\n",
      "        [9., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[7.0000, 8.0000],\n",
      "        [6.5000, 4.0000],\n",
      "        [7.3333, 3.6667]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix of ones\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalize - each row hve all equal elements in the triangular lower portion that sums one\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # each row is the mean of the original rows up to that row (including it)\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t}(x[b,i]) - each batch element to be the mean of the batch element up to that point - avoid to 'see the future'\n",
    "xbow = torch.zeros((B,T,C))\n",
    "# iterate the batches\n",
    "for b in range(B):\n",
    "    # iterate the blocks\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "\n",
    "# FC layers\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the triangular upper portion with -inf value\n",
    "wei = F.softmax(wei, dim=-1) # apply softmax so the -inf will turn to 0 and the rows will sum to 1\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.\n",
    "* Key: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.\n",
    "* Value: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8091, 0.1909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0940, 0.5435, 0.3625, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3758, 0.0312, 0.0457, 0.5472, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1813, 0.0628, 0.6858, 0.0282, 0.0418, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0768, 0.4553, 0.0526, 0.0483, 0.1728, 0.1942, 0.0000, 0.0000],\n",
       "        [0.0186, 0.1602, 0.0035, 0.0762, 0.3133, 0.0220, 0.4062, 0.0000],\n",
       "        [0.3171, 0.0785, 0.1855, 0.0416, 0.0139, 0.0282, 0.2623, 0.0729]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wei.shape)\n",
    "wei[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9070)\n",
      "tensor(1.0490)\n",
      "tensor(0.9349)\n",
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer normalization - normalize each block features instead of each feature across the batch\n",
    "class LayerNorm1d:\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs - mean != 0 and std != 1 - not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features - mean = 0 and std = 1 - normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading - same as above\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# estimate the model loss as the mean of loss in eval_iters batches of data\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval() # model enter evaluation mode - no training\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # model back to train mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167661 M parameters\n",
      "step 0: train loss 4.7974, val loss 4.7950\n",
      "step 100: train loss 2.4693, val loss 2.4730\n",
      "step 200: train loss 2.3603, val loss 2.3608\n",
      "step 300: train loss 2.2772, val loss 2.2838\n",
      "step 400: train loss 2.1617, val loss 2.1643\n",
      "step 500: train loss 2.0387, val loss 2.0459\n",
      "step 600: train loss 1.9261, val loss 1.9323\n",
      "step 700: train loss 1.8384, val loss 1.8466\n",
      "step 800: train loss 1.7652, val loss 1.7716\n",
      "step 900: train loss 1.7074, val loss 1.7102\n",
      "step 999: train loss 1.6477, val loss 1.6513\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel\n",
    "block_size = 64 # what is the maximum context length for predictions\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 2\n",
    "n_layer = 3\n",
    "dropout = 0.05\n",
    "# ------------\n",
    "\n",
    "# Individual Head of self-attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # key, query and value layers\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # tril is not an atribute of nn.Module so it must be passed to a buffer in order to be used in GPU\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\") - C**-0.5 is for normalization\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# Layer with multiple self-attention Heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # this linear layer is used to 'merge' the multiple heads acquired knowledge\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate the heads outputs in the C dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # apply the projection and the dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Simple feed forward network to apply a linear computation over the output embeddings\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Block containing a multi head attention module and a feed forward linear computation\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        # n_head: the number of heads in each multi head\n",
    "        # (n_emb % n_head) must be 0\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # each head gets a portion of the embeddings so different relations can be learned\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi head attention with layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # feed forward with layer norm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # get embeddings and compute the x by summing the embeddings\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        # make the input through the blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # final normalization\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        # genearte vocab_size output representing one character\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # test or training\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model, eval_iters)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exipenterior pentreddinater of gated artically toilels pering a for top thactiss of me flumpensoomerfs-semp\n",
      "A prater ber connuted of hasigned bolly atwas wito theiculetse. A fiont of the combune. The it for ald toilet. The beposessited with the pabse witwer dis a revbow the he fitium. The t¼ol end it recaed otte santed by an aresertine. , in toilets. A the 2) an ack and ped end 5 a a the bally mecontort\n",
      "Cof whels to the seland unfor whittee is flet detly exber an prod tecting she warr inclugh an usuid waper atper meyst has. Anwall luayed bowl theapersfent (12Hectl. Pening a formoner and and prodnedd spoxtentrectatunale., arge devat is ansies the fluashle valvely. Wither section ande sproldides a bowl.\n",
      "A lelish and 968) is and bigusteds inculed ded end a paracedjy groce opers valve to abuttom and enoged byelltacced of the toilbe water messeats anver attong mattorally (n ch freaniquitime ressembide the wat indistuar of the na diond rol appan ad add h20) thre islong a of an a the toilet moustom sure valve fluriontiouatar the sucly ofectem\n",
      "A s. The for in gaten of of anime acreltaci-han can extenged acks and rowne seive water ghery the inqporpted led of in be lusted. A drstragr condivushided in toilolet plat toretant and for a toilet seat tot, adumplayration the ansembow teanty to the mecend withe and indusige in oppen in a be to fluse is withe rg aplatuch mopertiect to and a housed a that be cyleld\n",
      "Wapecipecting vecinttal, bowly withich, ambe is a to then inclo the of to twaighe which the incloreat wativer se valveriulting thed trir lace me pantomated systalle. A lus puriduat sean file parmble,.\n",
      "A ressorting tril buet rects as diogg as, a me of thech, systrolech and by ancontod first filome of an ding the bror bin an the semoved and fice lower inelt. The ompenition tar caccheningiontically partion frair theat pacese to whet a sportatal, a reatted (2 the of ontaps (22) axember of the atudion for amebs theat withiose with. The stembly reacceably ther noing liat froyed po\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply residual sum in the layers to avoid gradient disappearing -> sum the input to the layers output before normalization - creates a 'shorter path' between input and output, reducing the disappearing gradient problem (specially in the beginning) - the sum operation propagates the gradients equally in the two 'branches' (the direct path - x sum, and the processed path - actual layer operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.245165 M parameters\n",
      "step 0: train loss 4.7762, val loss 4.7746\n",
      "step 200: train loss 2.0860, val loss 2.0928\n",
      "step 400: train loss 1.3782, val loss 1.3720\n",
      "step 600: train loss 1.2176, val loss 1.2216\n",
      "step 800: train loss 1.1375, val loss 1.1458\n",
      "step 1000: train loss 1.0951, val loss 1.1040\n",
      "step 1200: train loss 1.0594, val loss 1.0670\n",
      "step 1400: train loss 1.0302, val loss 1.0472\n",
      "step 1600: train loss 1.0122, val loss 1.0231\n",
      "step 1800: train loss 0.9931, val loss 1.0105\n",
      "step 2000: train loss 0.9777, val loss 0.9999\n",
      "step 2200: train loss 0.9718, val loss 0.9941\n",
      "step 2400: train loss 0.9554, val loss 0.9791\n",
      "step 2600: train loss 0.9486, val loss 0.9749\n",
      "step 2800: train loss 0.9398, val loss 0.9654\n",
      "step 3000: train loss 0.9316, val loss 0.9611\n",
      "step 3200: train loss 0.9228, val loss 0.9530\n",
      "step 3400: train loss 0.9185, val loss 0.9471\n",
      "step 3600: train loss 0.9091, val loss 0.9425\n",
      "step 3800: train loss 0.9006, val loss 0.9301\n",
      "step 4000: train loss 0.8993, val loss 0.9321\n",
      "step 4200: train loss 0.8936, val loss 0.9281\n",
      "step 4400: train loss 0.8869, val loss 0.9243\n",
      "step 4600: train loss 0.8823, val loss 0.9145\n",
      "step 4800: train loss 0.8787, val loss 0.9188\n",
      "step 4999: train loss 0.8796, val loss 0.9194\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 200\n",
    "learning_rate = 6e-4\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "# practically the same\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# same\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# same\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# add residual input sum after the attention and feed forward layers\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        # n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer norm than residual sum\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # layer norm than residual sum\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # init weights with low values so the residual path can be learned first\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model, eval_iters)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A toilet tank valve assembly and a control mechanism for a lifting tank the toilet and the at lexistent valve assembly action to cover the bowl pipe. The bottom near the diaper valve member between the flushing part. The module fual flushing extends through the antify parallelation and support members aratus are movably slo rotected in, the use of this flush hand back is keywards. The bolt and by bacterial recessed delay features that interseccch to resty the vent preof useful at a tracket to th\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "open('out_pt.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
