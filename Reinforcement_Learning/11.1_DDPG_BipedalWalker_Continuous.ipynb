{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Deep Deterministic Policy Gradients***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import random_uniform_initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda, BatchNormalization \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exploration**\n",
    "For continuous action spaces, exploration is done via adding noise to the action itself.  In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output. Is a type of noise that models Brownian motion (motion of particles in a fluid coliding with other particles at random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck Noise \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.1, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15987125, -0.03246613, -0.16896425, -0.03457285,  0.0023084 ,\n",
       "        0.11552847, -0.1229943 , -0.08057529, -0.11437638, -0.05009399,\n",
       "        0.1078746 ,  0.16073033,  0.08267794, -0.3372407 ,  0.16908056])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(15)\n",
    "b = OUActionNoise(a)\n",
    "a += b()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Replay Buffer**\n",
    "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay Buffer \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, minibatch_size = None):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise.\n",
    "                           Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        if self.size() == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, int(done)])\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def isMin(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size() >= self.minibatch_size)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def empties(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def getEpisode(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Actor (Policy) & Critic (Value)**\n",
    "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space\n",
    "\n",
    "The target networks are time-delayed copies of their original networks that slowly track the learned networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, inp_dim, fc1_dim, fc2_dim, fc3_dim, out_dim, act_range, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.inp_dim = inp_dim\n",
    "        self.fc1_dim = fc1_dim\n",
    "        self.fc2_dim = fc2_dim\n",
    "        self.fc3_dim = fc3_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Range of the action space\n",
    "        self.act_range = act_range\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Generates the optimization function\n",
    "        self.optimizer = Adam(self.lr)\n",
    "        #Generates the actor model\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generates the actor target model\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        inp = Input(shape=(self.inp_dim,))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.fc1_dim)\n",
    "        fc1 = Dense(self.fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(inp)\n",
    "        norm1 = BatchNormalization(dtype='float64')(fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.fc2_dim)\n",
    "        fc2 = Dense(self.fc2_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64')(norm1)\n",
    "        norm2 = BatchNormalization(dtype='float64')(fc2)\n",
    "        \n",
    "        f3 = 1 / np.sqrt(self.fc3_dim)\n",
    "        fc3 = Dense(self.fc3_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm2)\n",
    "        norm3 = BatchNormalization(dtype='float64')(fc3)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        out = Dense(self.out_dim, activation='tanh', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm3)\n",
    "        lamb = Lambda(lambda i: i * self.act_range, dtype='float64')(out)\n",
    "        \n",
    "        return Model(inputs=[inp], outputs=[lamb])\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states):\n",
    "        return self.model([states], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states):\n",
    "        return self.target_model([states], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, state_inp_dim, state_fc1_dim, state_fc2_dim, action_inp_dim, action_fc1_dim, conc_fc1_dim, conc_fc2_dim, out_dim, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.state_inp_dim = state_inp_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.state_fc2_dim = state_fc2_dim\n",
    "        self.action_inp_dim = action_inp_dim\n",
    "        self.action_fc1_dim = action_fc1_dim\n",
    "        self.conc_fc1_dim = conc_fc1_dim\n",
    "        self.conc_fc2_dim = conc_fc2_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = Adam(lr=self.lr)\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        #State input network ---------\n",
    "        s_inp = Input(shape=(self.state_inp_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.state_fc1_dim)\n",
    "        s_fc1 = Dense(self.state_fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.state_fc2_dim)\n",
    "        s_fc2 = Dense(self.state_fc2_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64')(s_norm1)\n",
    "        s_norm2 = BatchNormalization(dtype='float64')(s_fc2)\n",
    "        \n",
    "        #Action input network ---------\n",
    "        a_inp = Input(shape=(self.action_inp_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.action_fc1_dim)\n",
    "        a_fc1 = Dense(self.action_fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(a_inp)\n",
    "        a_norm1 = BatchNormalization(dtype='float64')(a_fc1)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_norm2, a_norm1])\n",
    "        \n",
    "        #Creates the output network\n",
    "        f1 = 1 / np.sqrt(self.conc_fc1_dim)\n",
    "        c_fc1 = Dense(self.conc_fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.conc_fc2_dim)\n",
    "        c_fc2 = Dense(self.conc_fc2_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64')(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        f3 = 0.003\n",
    "        out = Dense(self.out_dim, activation='linear', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(c_norm2)\n",
    "        \n",
    "        model = Model(inputs=[s_inp, a_inp], outputs=[out])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, actions):\n",
    "        return self.model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, action_min, action_max, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(self.memory_size, self.batch_size)\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            inp_dim=self.state_dim, \n",
    "            fc1_dim=512, \n",
    "            fc2_dim=256, \n",
    "            fc3_dim=64, \n",
    "            out_dim=self.action_dim, \n",
    "            act_range=self.action_max, \n",
    "            lr=self.a_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_inp_dim=self.state_dim, \n",
    "            state_fc1_dim=512, \n",
    "            state_fc2_dim=256,\n",
    "            action_inp_dim=self.action_dim, \n",
    "            action_fc1_dim=32,\n",
    "            conc_fc1_dim=512, \n",
    "            conc_fc2_dim=256, \n",
    "            out_dim=1,\n",
    "            lr=self.c_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "        \n",
    "        #Creates the noise generator\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
    "        \n",
    "    #-------------------------------------------------------------------- \n",
    "    def policy(self, state, explore=True):\n",
    "        state = state[np.newaxis, :]\n",
    "        action = self.actor.predict(state)[0]\n",
    "        #Takes the exploration with the epsilon probability\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            action += self.ou_noise()\n",
    "            \n",
    "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
    "        return action\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "        \n",
    "        if self.memory.isMin():\n",
    "            self.replay_memory()\n",
    "        \n",
    "    #--------------------------------------------------------------------    \n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        done = np.array([int(exp[4]) for exp in experiences])\n",
    "        \n",
    "        #Change the dimensions of the rewards and done arrays\n",
    "        rewards = rewards[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Train the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Compute the critic target values\n",
    "            target_actions = self.actor.target_predict(next_states)\n",
    "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions) * (1 - done)\n",
    "            #Compute the q_value of each next_state, next_action pair\n",
    "            critic_value = self.critic.predict(states, actions)\n",
    "            #Compute the critic loss \n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            acts = self.actor.predict(states)\n",
    "            critic_grads = self.critic.predict(states, acts)\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_grads)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "        \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "        \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        #If its reach the minimum value it stops\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "   \n",
    "    #--------------------------------------------------------------------     \n",
    "    def act(self, env):\n",
    "        #Reset the envirorment\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "            action = self.policy(observation, explore=False)\n",
    "            new_observation, reward, done, _ = env.step(action)\n",
    "            observation = new_observation\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "    #--------------------------------------------------------------------     \n",
    "    def train(self, env, num_episodes, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "\n",
    "        print(\"BEGIN\\n\")\n",
    "        complete = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(observation)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\r                                                                                                     \", end=\"\")\n",
    "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score) ,end=\"\")\n",
    "                    \n",
    "                new_observation, reward, done, _ = env.step(action)\n",
    "                self.learn(observation, action, reward, new_observation, done)\n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig on the firs complete episode\n",
    "                if end_on_complete and complete >= complete_num:\n",
    "                    break\n",
    "            \n",
    "            #These information are printed after each verbose_num episodes\n",
    "            if((episode+1)%verbose_num == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "                      , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_num:])\n",
    "                      , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_num:])\n",
    "                      , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\")\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_num episodes\n",
    "                if act_after_batch:\n",
    "                    self.act(env)\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        \n",
    "        return scores_history, steps_history\n",
    "    #--------------------------------------------------------------------     \n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def load(self, a_path, c_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "2022-06-22 14:20:28.946930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 14:20:30.481806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-06-22 14:20:30.481865: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-22 14:20:30.510121: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/user/.local/lib/python3.10/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "name = \"BipedalWalker-v3\"\n",
    "env = gym.make(name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "memory_size = 1000000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "a_lr = 2e-4\n",
    "c_lr = 5e-4\n",
    "tau = 5e-3\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.9999\n",
    "epsilon_min = 0.5\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, action_min, action_max, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_path = os.path.abspath('')+'/networks/11.1_DDPG_BipedalWalker/'\n",
    "agent.load(nets_path+\"_actor.h5\", nets_path+\"_critic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_num = 100\n",
    "end_on_complete = True\n",
    "complete_num = 1\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "#agent.train(env, num_episodes, verbose, verbose_num, end_on_complete, complete_num, complete_value, act_after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.save('/home/gustavo/PROG/RL_networks/11.1_DDPG_'+name+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
