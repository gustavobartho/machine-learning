{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from agent import DdpgSomAgent\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "name = \"BipedalWalkerHardcore-v3\"\n",
    "env = gym.make(name, max_episode_steps = 400)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "memory_size = 1000000\n",
    "batch_size = 128\n",
    "gamma = 0.95\n",
    "a_lr = 7e-4\n",
    "c_lr = 5e-3\n",
    "som_lr = 1\n",
    "tau = 1e-2\n",
    "epsilon = 0.8\n",
    "epsilon_decay = 0.99993\n",
    "epsilon_min = 0.15\n",
    "\n",
    "agent = DdpgSomAgent(state_dim, action_dim, action_min, action_max, memory_size, batch_size, gamma, a_lr, c_lr, som_lr, tau, epsilon, epsilon_decay, epsilon_min)\n",
    "\n",
    "#agent.act(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes:  20 / 500                                                                                       \n",
      "\tTotal reward:  -43.510635713707366 \n",
      "\tNum. steps:  382.75 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  40 / 500                                                                                       \n",
      "\tTotal reward:  -44.12657741891497 \n",
      "\tNum. steps:  400.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  60 / 500                                                                                       \n",
      "\tTotal reward:  -44.90238408283759 \n",
      "\tNum. steps:  400.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  80 / 500                                                                                       \n",
      "\tTotal reward:  -45.37051934155091 \n",
      "\tNum. steps:  400.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  100 / 500                                                                                      \n",
      "\tTotal reward:  -45.296722496908316 \n",
      "\tNum. steps:  400.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episode: 102\t Step: 114\tReward: -14.381244709956517                                                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous.ipynb#ch0000001?line=5'>6</a>\u001b[0m complete_value \u001b[39m=\u001b[39m \u001b[39m270\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous.ipynb#ch0000001?line=6'>7</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous.ipynb#ch0000001?line=8'>9</a>\u001b[0m scores, steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(env, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/agent.py:317\u001b[0m, in \u001b[0;36mDdpgSomAgent.train\u001b[0;34m(self, env, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mEpisode: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(episode\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Step: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    314\u001b[0m           \u001b[39mstr\u001b[39m(steps)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mReward: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(score), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    316\u001b[0m new_observation, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> 317\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(observation, action, reward, new_observation, done)\n\u001b[1;32m    318\u001b[0m observation \u001b[39m=\u001b[39m new_observation\n\u001b[1;32m    319\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/agent.py:196\u001b[0m, in \u001b[0;36mDdpgSomAgent.learn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39misMin():\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_som()\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_memory()\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/agent.py:227\u001b[0m, in \u001b[0;36mDdpgSomAgent.replay_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m done \u001b[39m=\u001b[39m done[:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m    226\u001b[0m maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetMaps(states)\n\u001b[0;32m--> 227\u001b[0m next_maps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetMaps(next_states)\n\u001b[1;32m    229\u001b[0m \u001b[39m# Train the critic\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    231\u001b[0m     \u001b[39m# Compute the critic target values\u001b[39;00m\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/agent.py:185\u001b[0m, in \u001b[0;36mDdpgSomAgent.getMaps\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    183\u001b[0m maps \u001b[39m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m states:\n\u001b[0;32m--> 185\u001b[0m     action, \u001b[39mmap\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(state)\n\u001b[1;32m    186\u001b[0m     maps\u001b[39m.\u001b[39mappend(\u001b[39mmap\u001b[39m[\u001b[39m0\u001b[39m])\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(maps, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/agent.py:172\u001b[0m, in \u001b[0;36mDdpgSomAgent.policy\u001b[0;34m(self, state, explore)\u001b[0m\n\u001b[1;32m    169\u001b[0m featMap \u001b[39m=\u001b[39m featMap\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msomDimension\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msomDimension)\n\u001b[1;32m    170\u001b[0m featMap \u001b[39m=\u001b[39m featMap[np\u001b[39m.\u001b[39mnewaxis, :]\n\u001b[0;32m--> 172\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mpredict(featMap)\n\u001b[1;32m    173\u001b[0m \u001b[39m# Takes the exploration with the epsilon probability\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m explore \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n",
      "File \u001b[0;32m~/JUPYTER_LAB/My_Projects/Reinforcement_Learning/12.1_DDPG_and_SOM_BipedalWalkerHardcore_Continuous/actor.py:59\u001b[0m, in \u001b[0;36mActor.predict\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, states):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel([states], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1099\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:451\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    434\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[1;32m    452\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:589\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    586\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 589\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1099\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/layers/core/dense.py:233\u001b[0m, in \u001b[0;36mDense.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    230\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mbias_add(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m   outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(outputs)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m is_ragged:\n\u001b[1;32m    236\u001b[0m   outputs \u001b[39m=\u001b[39m original_inputs\u001b[39m.\u001b[39mwith_flat_values(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_handler\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_traceback_filtering_enabled():\n\u001b[1;32m    141\u001b[0m       \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# In some very rare cases,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39m# accessible from inside this function\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:32\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m _ENABLE_TRACEBACK_FILTERING \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mlocal()\n\u001b[1;32m     27\u001b[0m _EXCLUDED_PATHS \u001b[39m=\u001b[39m (\n\u001b[1;32m     28\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m__file__\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)),\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_traceback_filtering_enabled\u001b[39m():\n\u001b[1;32m     34\u001b[0m   \u001b[39m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    was called).\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_ENABLE_TRACEBACK_FILTERING, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "verbose = True\n",
    "verbose_batch = 20\n",
    "end_on_complete = True\n",
    "complete_num = 0.3\n",
    "complete_value = 270\n",
    "act_after_batch = True\n",
    "\n",
    "scores, steps = agent.train(env, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" agent.save('/home/gustavo/PROG/RL_networks/11.1_DDPG_'+name+'/') \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "request to http://localhost:8889/api/sessions?1654704295330 failed, reason: getaddrinfo ENOTFOUND localhost. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nets_path = os.path.abspath('')+'/networks/11.1_DDPG_BipedalWalkerHardcore-v3/'\n",
    "agent.save('/home/gustavo/PROG/RL_networks/11.1_DDPG_'+name+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADICIONAR O MODULO DO REWARD SENDO MULTIPLICADO PELO LARNING RATE NA HORA DE TREINAR O SOM - APRENDE MELHOR OS ESTADOS MAIS RELEVANTES (POSITIVAMENTE OU NEGATIVAMENTE)\n",
    "#AUMENTAR DIMENSÃ‚O DO SOM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3c17e3122d25d527b33a00365b6902e14e8de10f291729ab7d4e45a04e4f03d9"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
