{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcb133c-963b-4671-8e2e-e3eddd9ee5c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5932c2ba-04ea-476a-8d92-417c47feee05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import GradientTape, random_uniform_initializer, square, subtract, sqrt\n",
    "from tensorflow.math import reduce_mean, square\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout, GlobalAveragePooling2D, Reshape, Normalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing .image import smart_resize\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff53b6b-5318-4005-86e1-230b4f846881",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Action noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a84374c-f426-40ca-86e9-b6ba89f32384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck Noise \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.1, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    # Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18637fa-f214-46d7-b1b4-a844114fecab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8d1b32-edbc-4b17-a589-5f653ea6bf1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replay Buffer \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, minibatch_size = None):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, sensorial_data, embedding, done):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise.\n",
    "                           Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        # Has a 80% chance of registering the memory\n",
    "        if self.hasMin and np.random.uniform() > 0.8: return\n",
    "        if self.size() == self.max_size: del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, sensorial_data, embedding, int(done)])\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def hasMin(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size() >= self.minibatch_size)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def empties(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def getEpisode(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ad3fc-d43b-4944-a572-8041eef3d1b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Layers creation organizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126f68a4-0c80-45a4-aab4-76a949474714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayerData(object):\n",
    "    def __init__(self, num_channels, kernel_dim, strides, pooling_dim, droppout):\n",
    "        self.num_channels = num_channels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.strides = strides\n",
    "        self.pooling_dim = pooling_dim\n",
    "        self.droppout = droppout\n",
    "\n",
    "        \n",
    "#==============================================================\n",
    "\n",
    "class DenseLayerData(object):\n",
    "    def __init__(self, dim, droppout, normalize=True):\n",
    "        self.dim = dim\n",
    "        self.droppout = droppout\n",
    "        self.normalize = normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca7f92-9120-419e-bcaa-5aa8cfb2a203",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700f572f-ac50-40dc-8027-3dfe0f7f8520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, state_dim, sensory_dim, action_dim, lr, tau):\n",
    "        \n",
    "        #Network dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.sensory_dim = sensory_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        \n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Encoder layers data\n",
    "        self.sensory_net_dims = [\n",
    "            ConvLayerData(8, 2, 2, 0, 0),\n",
    "            ConvLayerData(16, 3, 2, 2, 0.05),\n",
    "            ConvLayerData(24, 2, 1, 2, 0),\n",
    "        ]\n",
    "        \n",
    "        self.state_net_dims = [\n",
    "            DenseLayerData(32, 0.1),\n",
    "            DenseLayerData(8, 0.1, False),\n",
    "        ]\n",
    "        \n",
    "        self.actor_dims = [\n",
    "            DenseLayerData(256, 0.05, False),\n",
    "            DenseLayerData(128, 0.07),\n",
    "            DenseLayerData(128, 0.07, False),\n",
    "            DenseLayerData(16, 0),\n",
    "        ]\n",
    "\n",
    "        #Generates the optimization function\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        \n",
    "        #Generates the actor model\n",
    "        self.model = self.buildNetwork()\n",
    "        \n",
    "        #Generates the actor target model\n",
    "        self.target_model = self.buildNetwork()\n",
    "        \n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def sensoryEncoderNet(self):\n",
    "        return Actor.createSensoryEncoderNet(self.sensory_dim, self.sensory_net_dims)    \n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def stateNet(self):\n",
    "        return Actor.createStateNet(self.state_dim, self.state_net_dims)\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        sensory_encoder = self.sensoryEncoderNet()\n",
    "        state_net = self.stateNet() \n",
    "        \n",
    "        return Actor.createNetwork(self.sensory_net_dims, sensory_encoder, state_net, self.actor_dims, self.optimizer, self.action_dim)\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def predict(self, states, sensory_data):\n",
    "        return self.model([sensory_data, states], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def target_predict(self, states, sensory_data):\n",
    "        return self.target_model([sensory_data, states], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    @property\n",
    "    def embeddingDim(self):\n",
    "        return self.model.output[-1].shape[-1]\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + 'actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)\n",
    "        \n",
    "    # GENERAL STATIC FUNCTIONS FOR SINGLE NETWORK CREATION -------------------------------------------------\n",
    "    \n",
    "    def createSensoryEncoderNet(sensory_data_dim, sensory_net_dims):\n",
    "        inp = Input(shape=(sensory_data_dim[0], sensory_data_dim[1], 3), name='SensoryEnc_Input')\n",
    "        net = Normalization(axis=None)(inp)\n",
    "\n",
    "        for i, layer in enumerate(sensory_net_dims):\n",
    "            dist_lim = 1 / np.sqrt(layer.num_channels*(layer.kernel_dim**2))\n",
    "            net = Conv2D(\n",
    "                layer.num_channels, \n",
    "                layer.kernel_dim, \n",
    "                layer.strides, \n",
    "                padding='same', \n",
    "                activation='relu', \n",
    "                kernel_initializer=random_uniform_initializer(-dist_lim, dist_lim), \n",
    "                bias_initializer=random_uniform_initializer(-dist_lim, dist_lim),\n",
    "                name = 'SensoryEnc_Dense_' + str(i+1),\n",
    "            )(net)\n",
    "            if(layer.pooling_dim > 0): net = MaxPooling2D(layer.pooling_dim, strides=2, name = 'SensoryEnc_MaxPooling_' + str(i+1))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name = 'SensoryEnc_Droppout_' + str(i+1))(net)\n",
    "\n",
    "        net = Flatten(name = 'SensoryEnc_Flatten')(net)\n",
    "\n",
    "        model = Model(inputs=[inp], outputs=[net])\n",
    "\n",
    "        return model\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def createSensoryDecoderNet(sensory_net_dims, sensory_enc):\n",
    "        net = Reshape(target_shape=sensory_enc.layers[-2].output.shape[1:], name='SensoryDec_Reshape')(sensory_enc.output)\n",
    "\n",
    "        for i, layer in enumerate(reversed(sensory_net_dims)):\n",
    "            net = Conv2D(\n",
    "                layer.num_channels, \n",
    "                layer.kernel_dim, \n",
    "                layer.strides, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                name = 'SensoryDec_Dense_' + str(i+1),\n",
    "            )(net)\n",
    "            net = UpSampling2D(layer.pooling_dim, name = 'SensoryDec_UpSampling_' + str(i+1))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name = 'SensoryDec_Droppout_' + str(i+1))(net)\n",
    "\n",
    "        net = Conv2D(3, 3, strides=1, padding='same', activation='relu', name = 'SensoryDec_LastConv')(net)\n",
    "        net = UpSampling2D(4, name = 'SensoryDec_Output')(net)\n",
    "\n",
    "        model = Model(inputs=[sensory_enc.input], outputs=[net])\n",
    "\n",
    "        return model\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def createStateNet(state_dim, state_net_dims):\n",
    "        inp = Input(shape=(state_dim, ), name='State_Input')\n",
    "        net = Lambda(lambda x: x, name='State_Lambda')(inp)\n",
    "\n",
    "        for i, layer in enumerate(state_net_dims):\n",
    "            dist = 1 / np.sqrt(layer.dim)\n",
    "            net = Dense(\n",
    "                layer.dim,\n",
    "                activation='relu', \n",
    "                dtype='float32', \n",
    "                kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "                bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "                name='State_Dense_' + str(i)\n",
    "            )(net)\n",
    "            if(layer.normalize): net = BatchNormalization(dtype='float32', name='State_BatchNorm_' + str(i))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name='State_Droppout_' + str(i))(net)\n",
    "\n",
    "        model = Model(inputs=[inp], outputs=[net])\n",
    "\n",
    "        return model\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def createNetwork(sensory_net_dims, sensory_enc, state_net, net_layers, optimizer, action_dim):\n",
    "        conc = Concatenate(dtype='float32')([sensory_enc.output, state_net.output])\n",
    "        net = Lambda(lambda x: x, name='Actor_Lambda')(conc)\n",
    "        \n",
    "        for i, layer in enumerate(net_layers):\n",
    "            dist = 1 / np.sqrt(layer.dim)\n",
    "            net = Dense(\n",
    "                layer.dim, \n",
    "                activation='relu', \n",
    "                dtype='float32', \n",
    "                kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "                bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "                name='Actor_Dens_' + str(i)\n",
    "            )(net)\n",
    "            if(layer.normalize): net = BatchNormalization(dtype='float32', name='Actor_BatchNorm_' + str(i))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name='Actor_Droppout_' + str(i))(net)\n",
    "\n",
    "        net = Dense(action_dim, activation='softmax', dtype='float32', name='Actor_Out')(net)\n",
    "\n",
    "        #dec = Actor.createSensoryDecoderNet(sensory_net_dims, sensory_enc)\n",
    "        model = Model(inputs=[sensory_enc.input, state_net.input], outputs=[net, sensory_enc.output])\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2992f-238e-4d79-a41f-a44e942bfaec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Actor networks creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f0d9a1-0eb7-40ed-9449-528e17658337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "sensory_net_dims = [\n",
    "    ConvLayerData(20, 4, 2, 2, 0.1),\n",
    "    ConvLayerData(30, 3, 1, 2, 0.2),\n",
    "    ConvLayerData(40, 2, 1, 2, 0),\n",
    "    ConvLayerData(50, 2, 1, 2, 0),\n",
    "]\n",
    "\n",
    "state_net_dim = [\n",
    "    DenseLayerData(32, 0.1),\n",
    "]\n",
    "\n",
    "actor_dims = [\n",
    "    DenseLayerData(512, 0.1),\n",
    "    DenseLayerData(128, 0.2),\n",
    "    DenseLayerData(16, 0.1),\n",
    "]\n",
    "\n",
    "sensory_dim = [42, 64]\n",
    "state_dim = 16\n",
    "\n",
    "optimizer = Adam(learning_rate=0.1)\n",
    "action_dim = 4\n",
    "act_range = [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107fe363-67a1-4110-ab54-9c5a0cc39d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "ne = Actor.createSensoryEncoderNet(sensory_dim, sensory_net_dims)\n",
    "nd = Actor.createSensoryDecoderNet(sensory_net_dims, ne)\n",
    "ns = Actor.createStateNet(state_dim, state_net_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c22d6-e446-4e5c-88ba-a98b32ac7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "ne.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69dd9e4-1323-4bec-aaca-24f62af05bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "nf = Actor.createNetwork(sensory_net_dims, ne, ns, actor_dims, optimizer, action_dim, act_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c7b9db-7c13-4bfa-b08d-aa837550de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "plot_model(nf, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5dcf0d1-ddd9-4841-9ee4-2401cd281a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "act = Actor(state_dim, sensory_dim, action_dim, act_range, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94fa4c67-61f5-4d1c-b4a3-14fa6d9a372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "plot_model(act.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bebb240-01f0-4036-bf92-49fe1034d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "states = [np.random.rand(1, state_dim)]\n",
    "sensory = [np.random.rand(*sensory_dim, 3).reshape((-1, 256, 256, 3, 1))]\n",
    "pred = act.predict(states, sensory)\n",
    "print(len(pred))\n",
    "print(pred[0][0].shape)\n",
    "print(pred[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691c7cb-9ed9-4578-bdac-445af029d545",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931a9d31-7a4d-49d0-be2b-8945fd247ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, state_dim, embedding_dim, action_dim, out_dim, lr, tau):\n",
    "        #Network dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.embedding_net_dims = [\n",
    "            DenseLayerData(256, 0.1),\n",
    "            DenseLayerData(256, 0.2, False),\n",
    "            DenseLayerData(64, 0),\n",
    "        ]\n",
    "        self.state_net_dims = [\n",
    "            DenseLayerData(32, 0.1, False),\n",
    "            DenseLayerData(8, 0.1),\n",
    "        ]\n",
    "        self.action_net_dims = [\n",
    "            DenseLayerData(16, 0.1),\n",
    "        ]\n",
    "        \n",
    "        self.critic_net_dims = [\n",
    "            DenseLayerData(128, 0.1),\n",
    "            DenseLayerData(32, 0.1),\n",
    "            DenseLayerData(8, 0, False),\n",
    "        ]\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        \n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def stateNet(self):\n",
    "        return Critic.createGenericNet('State', self.state_dim, self.state_net_dims)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def actionNet(self):\n",
    "        return Critic.createGenericNet('Action', self.action_dim, self.action_net_dims)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def embeddingNet(self):\n",
    "        return Critic.createGenericNet('Emb', self.embedding_dim, self.embedding_net_dims)\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        state = self.stateNet()\n",
    "        action = self.actionNet()\n",
    "        emb = self.embeddingNet()\n",
    "        \n",
    "        return Critic.createNetwork(state, action, emb, self.critic_net_dims, self.out_dim, self.optimizer)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def predict(self, states, actions, embeddings):\n",
    "        return self.model([states, actions, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def target_predict(self, states, actions, embeddings):\n",
    "        return self.target_model([states, actions, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + 'critic.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)\n",
    "        \n",
    "    # =================== GENERAL STATIC FUNCTIONS FOR SINGLE NETWORK CREATION ==========================\n",
    "    \n",
    "    def createGenericNet(net_type, data_dim, net_dims, normalize_input=False):\n",
    "        inp = Input(shape=(data_dim, ), name='C_'+net_type+'_Input')\n",
    "        if(normalize_input): net = Normalization(axis=None)(inp)\n",
    "        else: net = Lambda(lambda x: x)(inp)\n",
    "\n",
    "        for i, layer in enumerate(net_dims):\n",
    "            dist = 1 / np.sqrt(layer.dim)\n",
    "            net = Dense(\n",
    "                layer.dim,\n",
    "                activation='relu', \n",
    "                dtype='float32', \n",
    "                kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "                bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "                name='C_'+net_type+'_Dense_' + str(i)\n",
    "            )(net)\n",
    "            if(layer.normalize): net = BatchNormalization(dtype='float32', name='C_'+net_type+'_BatchNorm_' + str(i))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name='C_'+net_type+'_Droppout_' + str(i))(net)\n",
    "\n",
    "        model = Model(inputs=[inp], outputs=[net])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # =============================================\n",
    "    \n",
    "    def createNetwork(state_net, action_net, emb_net, critic_net_dims, out_dim, optimizer):\n",
    "        #Concatenate state and action networks ---\n",
    "        net = Concatenate(dtype='float32')([action_net.output, state_net.output])\n",
    "        dim = round(action_net.output.shape[-1]*state_net.output.shape[-1])\n",
    "        dist =  1 / np.sqrt(dim)\n",
    "        net = Dense(\n",
    "            dim, \n",
    "            activation='relu', \n",
    "            dtype='float32', \n",
    "            kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "            bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "            name='Critic_ActState_Dense',\n",
    "        )(net)\n",
    "        \n",
    "        # Concatenates state-action and embedding networks\n",
    "        net = Concatenate(dtype='float32')([net, emb_net.output])\n",
    "        dim = round(np.sqrt((0.3*min(net.shape[-1], emb_net.output.shape[-1])) + (0.7*max(net.shape[-1], emb_net.output.shape[-1]))))\n",
    "        dist =  1 / np.sqrt(dim)\n",
    "        net = Dense(\n",
    "            dim, \n",
    "            activation='relu', \n",
    "            dtype='float32', \n",
    "            kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "            bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "            name='Critic_ActStateEmb_Dense',\n",
    "        )(net)\n",
    "        \n",
    "        for i, layer in enumerate(critic_net_dims):\n",
    "            dist = 1 / np.sqrt(layer.dim)\n",
    "            net = Dense(\n",
    "                layer.dim,\n",
    "                activation='relu', \n",
    "                dtype='float32', \n",
    "                kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "                bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "                name='C_Dense' + str(i)\n",
    "            )(net)\n",
    "            if(layer.normalize): net = BatchNormalization(dtype='float32', name='C_BatchNorm_' + str(i))(net)\n",
    "            if(layer.droppout > 0): net = Dropout(layer.droppout, name='C_Droppout_' + str(i))(net)\n",
    "        \n",
    "        net = Dense(\n",
    "            out_dim, \n",
    "            activation='relu', \n",
    "            dtype='float32',  \n",
    "            kernel_initializer=random_uniform_initializer(-dist, dist), \n",
    "            bias_initializer=random_uniform_initializer(-dist, dist),\n",
    "        )(net)\n",
    "        \n",
    "        model = Model(inputs=[state_net.input, action_net.input, emb_net.input], outputs=[net])\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60960b-690a-4c02-be38-bdaf90174a1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Testing Critic networks creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9659297-4239-4411-b51e-2c4a16e5dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "emb_net_dims = [\n",
    "    DenseLayerData(20, 0.1),\n",
    "    DenseLayerData(30, 0.2),\n",
    "   DenseLayerData(40, 0),\n",
    "]\n",
    "state_net_dims = [\n",
    "    DenseLayerData(32, 0.1),\n",
    "]\n",
    "action_net_dims = [\n",
    "    DenseLayerData(2042, 0.1),\n",
    "    DenseLayerData(512, 0.1),\n",
    "    DenseLayerData(128, 0),\n",
    "]\n",
    "critic_net_dims = [\n",
    "    DenseLayerData(2042, 0.1),\n",
    "    DenseLayerData(512, 0.1),\n",
    "    DenseLayerData(128, 0),\n",
    "]\n",
    "\n",
    "emb_dim = 192\n",
    "state_dim = 16\n",
    "\n",
    "optimizer = Adam(learning_rate=0.1)\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8956b7-23fb-4c50-8fb2-9b1eb4085f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "ns = Critic.createGenericNet('State', state_dim, state_net_dims)\n",
    "na = Critic.createGenericNet('Action', action_dim, action_net_dims)\n",
    "ne = Critic.createGenericNet('Emb', emb_dim, emb_net_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29585827-8dc6-426e-a668-3c8e48bef15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "nf = Critic.createNetwork(ns, na, ne, critic_net_dims, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "398b7fff-a11d-4a1b-801c-beada846d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "plot_model(nf, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abc55015-6521-43fd-8ed5-1fb17445820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "crt = Critic(state_dim, emb_dim, action_dim, 1, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59131061-3c1a-4874-92a5-d8142c976fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "plot_model(crt.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4549ff5d-f4c5-4e64-8c4d-2d08fcf10af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "states = [np.random.rand(1, state_dim)]\n",
    "actions = [np.random.rand(1, action_dim)]\n",
    "emb = [np.random.rand(1, emb_dim)]\n",
    "pred = crt.predict(states, actions, emb)\n",
    "print(len(pred))\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de10e3-5e1d-487e-b373-312bb4d969da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0451d53-61d5-4f99-b8a6-68f7151ee0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=BrokenPipeError(32, 'Broken pipe')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 676, in write\n",
      "    n = os.write(self._fileno, data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, num_actions, sensory_dim, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min):\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.sensory_dim = sensory_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(self.memory_size, self.batch_size)\n",
    "\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            state_dim = self.state_dim,\n",
    "            sensory_dim = self.sensory_dim,\n",
    "            action_dim = self.num_actions, \n",
    "            lr = self.a_lr, \n",
    "            tau = self.tau,\n",
    "        )\n",
    "\n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_dim = self.state_dim, \n",
    "            action_dim = self.num_actions, \n",
    "            embedding_dim = self.actor.embeddingDim,\n",
    "            out_dim = 1,\n",
    "            lr = self.c_lr, \n",
    "            tau = self.tau,\n",
    "        )\n",
    "\n",
    "    #-------------------------------------------------------------------- \n",
    "    \n",
    "    def policy(self, state, sensorial_data, explore=True):\n",
    "        state = state[np.newaxis, :]\n",
    "        sensorial_data = sensorial_data[np.newaxis, :]\n",
    "        pred = self.actor.predict(state, sensorial_data)\n",
    "        \n",
    "        action = np.argmax(pred[0][0])\n",
    "        #Takes the exploration with the epsilon probability\n",
    "        if explore and np.random.uniform() < self.epsilon: action = np.random.choice(np.where(np.arange(self.num_actions) != action)[0])\n",
    "        \n",
    "        return pred[0][0], action, pred[1]\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, sensorial_data, embedding, done):\n",
    "        self.memory.append(state, action, reward, next_state, sensorial_data, embedding, done)\n",
    "        if self.memory.hasMin: self.replay_memory()\n",
    "        \n",
    "    #--------------------------------------------------------------------    \n",
    "    \n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        sensorial_data = np.array([exp[4] for exp in experiences])\n",
    "        embeddings = np.array([exp[5] for exp in experiences])\n",
    "        done = np.array([int(exp[6]) for exp in experiences])\n",
    "        \n",
    "        #Change the dimensions of the rewards and done arrays\n",
    "        rewards = rewards[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Train the critic\n",
    "        with GradientTape() as tape:\n",
    "            #Compute the critic target values\n",
    "            target_actions, sensorial_embeddings  = self.actor.target_predict(next_states, sensorial_data)\n",
    "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions, sensorial_embeddings) * (1 - done)\n",
    "            #Compute the q_value of each next_state, next_action pair\n",
    "            critic_value = self.critic.predict(states, actions, np.squeeze(embeddings))\n",
    "            #Compute the critic loss \n",
    "            critic_loss = reduce_mean(square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with GradientTape() as tape:\n",
    "            acts, embs = self.actor.predict(states, sensorial_data)\n",
    "            critic_grads = self.critic.predict(states, acts, embs)\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -reduce_mean(critic_grads)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "        \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "        \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min: self.epsilon *= self.epsilon_decay\n",
    "        #If its reach the minimum value it stops\n",
    "        else: self.epsilon = self.epsilon_min\n",
    "\n",
    "        return\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def act(self, env, verbose = False):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render(mode='human')\n",
    "            time.sleep(0.02)\n",
    "            sensory = self.getSensorialData(env.render(mode = 'rgb_array'))\n",
    "            action = self.policy(observation, sensory, explore=False)[1]\n",
    "            if verbose: print(action)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            observation = new_observation\n",
    "\n",
    "        return\n",
    "        \n",
    "    #--------------------------------------------------------------------     \n",
    "    def train(self, env, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "\n",
    "        total = 0\n",
    "        \n",
    "        #If the complete_num is smaller than 1 ist interpreted as a percentage else its a number of episodes\n",
    "        if complete_num < 1: complete_num = int(complete_num*verbose_batch) if int(complete_num*verbose_batch) != 0 else 1\n",
    "        \n",
    "        #Begin the training\n",
    "        print(\"BEGIN\\n\")\n",
    "        \n",
    "        #Number of completed episodes per batch\n",
    "        complete = 0\n",
    "        \n",
    "        #Iterate on each episode\n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                sensory_data = env.render(mode='rgb_array')\n",
    "                sensory_data = self.getSensorialData(sensory_data)\n",
    "                \n",
    "                probs, action, embedding = self.policy(observation, sensory_data)\n",
    "                new_observation, reward, done, _ = env.step(action)\n",
    "                self.learn(observation, probs, reward, new_observation, sensory_data, embedding, done)\n",
    "\n",
    "                if verbose and total > 100:\n",
    "                    print(\"\\r                                                                                                              \", end=\"\")\n",
    "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score) ,end=\"\")\n",
    "                    \n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "                total += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig after completing a number of episodes\n",
    "                if end_on_complete and complete >= complete_num:\n",
    "                    break\n",
    "            \n",
    "            #These information are printed after each verbose_batch episodes\n",
    "            if((episode+1)%verbose_batch == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "                      , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_batch:])\n",
    "                      , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_batch:])\n",
    "                      , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\")\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_batch episodes\n",
    "                if act_after_batch: self.act(env)\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        return scores_history, steps_history\n",
    "    \n",
    "    #--------------------------------------------------------------------   \n",
    "    \n",
    "    def getSensorialData(self, sensorial_data):\n",
    "        return smart_resize(sensorial_data, self.sensory_dim)\n",
    "    \n",
    "    #------------------------------------------------------------------   \n",
    "    \n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    def load(self, a_path, c_path, ae_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)\n",
    "    \n",
    "    #--------------------------------------------------------------------   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dae6c-1bec-418e-be52-1c5bbfc8ac20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Agent networks creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90802934-ed4a-4010-9530-074dbba146cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LunarLander-v2\"\n",
    "env = gym.make(name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "sensory_dim = [64, 64]\n",
    "\n",
    "memory_size = 5000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "a_lr = 1e-3\n",
    "c_lr = 2e-3\n",
    "tau = 5e-3\n",
    "epsilon = 0.7\n",
    "epsilon_decay = 0.99999\n",
    "epsilon_min = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "779cb628-a02d-44e0-a425-abd7e50bb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 20:18:40.540753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 20:18:40.569076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-12-03 20:18:40.569097: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-03 20:18:40.569535: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(state_dim, num_actions, sensory_dim, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f90cad2-f6f3-4f02-a6fd-2b9c561989c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(agent.actor.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71326653-c756-4385-8358-5163f1e483c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes:  60 / 3000                                                                                          \n",
      "\tTotal reward:  -166.67492396202832 \n",
      "\tNum. steps:  111.93333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  120 / 3000                                                                                         \n",
      "\tTotal reward:  -219.98692788512804 \n",
      "\tNum. steps:  104.21666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  180 / 3000                                                                                         \n",
      "\tTotal reward:  -264.7998017978248 \n",
      "\tNum. steps:  112.41666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  240 / 3000                                                                                         \n",
      "\tTotal reward:  -276.61943236779064 \n",
      "\tNum. steps:  119.3 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  300 / 3000                                                                                         \n",
      "\tTotal reward:  -328.2825745658196 \n",
      "\tNum. steps:  123.96666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  360 / 3000                                                                                         \n",
      "\tTotal reward:  -329.6497450097646 \n",
      "\tNum. steps:  120.63333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  420 / 3000                                                                                         \n",
      "\tTotal reward:  -340.23887214827704 \n",
      "\tNum. steps:  134.41666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  480 / 3000                                                                                         \n",
      "\tTotal reward:  -369.3750311149106 \n",
      "\tNum. steps:  123.21666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  540 / 3000                                                                                         \n",
      "\tTotal reward:  -423.8699727239382 \n",
      "\tNum. steps:  124.78333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  600 / 3000                                                                                         \n",
      "\tTotal reward:  -457.2240982238613 \n",
      "\tNum. steps:  135.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  660 / 3000                                                                                         \n",
      "\tTotal reward:  -495.0079395014325 \n",
      "\tNum. steps:  133.73333333333332 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  720 / 3000                                                                                         \n",
      "\tTotal reward:  -507.9337290184651 \n",
      "\tNum. steps:  143.18333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  780 / 3000                                                                                         \n",
      "\tTotal reward:  -504.7158785770766 \n",
      "\tNum. steps:  126.15 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  840 / 3000                                                                                         \n",
      "\tTotal reward:  -527.6153259782616 \n",
      "\tNum. steps:  133.15 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  900 / 3000                                                                                         \n",
      "\tTotal reward:  -523.9843513447493 \n",
      "\tNum. steps:  134.06666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  960 / 3000                                                                                         \n",
      "\tTotal reward:  -504.97567987059597 \n",
      "\tNum. steps:  129.68333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1020 / 3000                                                                                        \n",
      "\tTotal reward:  -503.94164066079776 \n",
      "\tNum. steps:  130.01666666666668 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1080 / 3000                                                                                        \n",
      "\tTotal reward:  -504.6805671577843 \n",
      "\tNum. steps:  130.28333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1140 / 3000                                                                                        \n",
      "\tTotal reward:  -514.6032293035373 \n",
      "\tNum. steps:  128.53333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1200 / 3000                                                                                        \n",
      "\tTotal reward:  -553.0434442230901 \n",
      "\tNum. steps:  143.18333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1260 / 3000                                                                                        \n",
      "\tTotal reward:  -582.1175110199058 \n",
      "\tNum. steps:  139.55 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1320 / 3000                                                                                        \n",
      "\tTotal reward:  -495.78638704250835 \n",
      "\tNum. steps:  125.63333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1380 / 3000                                                                                        \n",
      "\tTotal reward:  -539.6792595074953 \n",
      "\tNum. steps:  153.31666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1440 / 3000                                                                                        \n",
      "\tTotal reward:  -559.219411562223 \n",
      "\tNum. steps:  140.83333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1500 / 3000                                                                                        \n",
      "\tTotal reward:  -540.9032858049392 \n",
      "\tNum. steps:  139.5 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1560 / 3000                                                                                        \n",
      "\tTotal reward:  -490.21744289090543 \n",
      "\tNum. steps:  124.38333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1620 / 3000                                                                                        \n",
      "\tTotal reward:  -509.69374919473296 \n",
      "\tNum. steps:  135.26666666666668 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1680 / 3000                                                                                        \n",
      "\tTotal reward:  -537.488807820142 \n",
      "\tNum. steps:  136.08333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1740 / 3000                                                                                        \n",
      "\tTotal reward:  -506.94217964785236 \n",
      "\tNum. steps:  126.5 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1800 / 3000                                                                                        \n",
      "\tTotal reward:  -541.6108396852306 \n",
      "\tNum. steps:  144.85 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1860 / 3000                                                                                        \n",
      "\tTotal reward:  -512.0636086818536 \n",
      "\tNum. steps:  132.66666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1920 / 3000                                                                                        \n",
      "\tTotal reward:  -484.5577278186886 \n",
      "\tNum. steps:  133.91666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  1980 / 3000                                                                                        \n",
      "\tTotal reward:  -558.2880074382797 \n",
      "\tNum. steps:  138.56666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2040 / 3000                                                                                        \n",
      "\tTotal reward:  -505.6474789019425 \n",
      "\tNum. steps:  134.96666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2100 / 3000                                                                                        \n",
      "\tTotal reward:  -473.47768922224145 \n",
      "\tNum. steps:  123.13333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2160 / 3000                                                                                        \n",
      "\tTotal reward:  -463.3009304775689 \n",
      "\tNum. steps:  124.73333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2220 / 3000                                                                                        \n",
      "\tTotal reward:  -523.4162722440082 \n",
      "\tNum. steps:  137.78333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2280 / 3000                                                                                        \n",
      "\tTotal reward:  -538.7618235642867 \n",
      "\tNum. steps:  139.15 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2340 / 3000                                                                                        \n",
      "\tTotal reward:  -561.6408305292351 \n",
      "\tNum. steps:  146.61666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2400 / 3000                                                                                        \n",
      "\tTotal reward:  -500.82242783763826 \n",
      "\tNum. steps:  143.06666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2460 / 3000                                                                                        \n",
      "\tTotal reward:  -538.4238434858695 \n",
      "\tNum. steps:  146.75 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2520 / 3000                                                                                        \n",
      "\tTotal reward:  -472.0905698019753 \n",
      "\tNum. steps:  132.68333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2580 / 3000                                                                                        \n",
      "\tTotal reward:  -604.4188103830539 \n",
      "\tNum. steps:  147.88333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2640 / 3000                                                                                        \n",
      "\tTotal reward:  -484.4446495374663 \n",
      "\tNum. steps:  127.68333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2700 / 3000                                                                                        \n",
      "\tTotal reward:  -515.5855533145593 \n",
      "\tNum. steps:  139.81666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2760 / 3000                                                                                        \n",
      "\tTotal reward:  -457.55987198105424 \n",
      "\tNum. steps:  121.78333333333333 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2820 / 3000                                                                                        \n",
      "\tTotal reward:  -546.7838487566727 \n",
      "\tNum. steps:  139.83333333333334 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2880 / 3000                                                                                        \n",
      "\tTotal reward:  -509.4112130948735 \n",
      "\tNum. steps:  124.55 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  2940 / 3000                                                                                        \n",
      "\tTotal reward:  -551.1077468175253 \n",
      "\tNum. steps:  133.55 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  3000 / 3000                                                                                        \n",
      "\tTotal reward:  -512.2793930782104 \n",
      "\tNum. steps:  137.06666666666666 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_batch = 60\n",
    "end_on_complete = True\n",
    "complete_num = 0.5\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "scores, steps = agent.train(env, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
