{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xFyWCKLkDfjr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import GradientTape, random_uniform_initializer\n",
    "from tensorflow.math import reduce_mean, square\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing .image import smart_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rGrJY5qvDrzZ"
   },
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck Noise \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.2, dt=0.1, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uWayMDZXDt5E"
   },
   "outputs": [],
   "source": [
    "#Replay Buffer \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, minibatch_size = None):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, done, embedding, new_embedding):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise.\n",
    "                           Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        # Has a 60% chance of registering the memory\n",
    "        if self.hasMin and np.random.rand() > 0.6: return\n",
    "        if self.size() == self.max_size: del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, int(done), embedding, new_embedding])\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def hasMin(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size() >= self.minibatch_size)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def empties(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def getEpisode(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q_caQDVgDvk2"
   },
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim,\n",
    "        state_fc1_dim,\n",
    "        embedding_dim,\n",
    "        embedding_fc1_dim,\n",
    "        embedding_fc2_dim,\n",
    "        embedding_fc3_dim,\n",
    "        concat_fc1_dim,\n",
    "        concat_fc2_dim,\n",
    "        out_dim, \n",
    "        act_range, \n",
    "        lr, \n",
    "        tau,\n",
    "    ):\n",
    "        #Network dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_fc1_dim = embedding_fc1_dim\n",
    "        self.embedding_fc2_dim = embedding_fc2_dim\n",
    "        self.embedding_fc3_dim = embedding_fc3_dim\n",
    "        self.concat_fc1_dim = concat_fc1_dim\n",
    "        self.concat_fc2_dim = concat_fc2_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Range of the action space\n",
    "        self.act_range = act_range\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Generates the optimization function\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        #Generates the actor model\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generates the actor target model\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        \n",
    "        # State network\n",
    "        s_inp = Input(shape=(self.state_dim, ), name='s_inp')\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.state_fc1_dim)\n",
    "        s_fc1 = Dense(self.state_fc1_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        s_drop1 = Dropout(0.3)(s_norm1)\n",
    "        \n",
    "        #Embedding network\n",
    "        e_inp = Input(shape=(self.embedding_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.embedding_fc1_dim)\n",
    "        e_fc1 = Dense(self.embedding_fc1_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(e_inp)\n",
    "        e_norm1 = BatchNormalization(dtype='float64')(e_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.embedding_fc2_dim)\n",
    "        e_fc2 = Dense(self.embedding_fc2_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2))(e_norm1)\n",
    "        e_norm2 = BatchNormalization(dtype='float64')(e_fc2)\n",
    "        e_drop2 = Dropout(0.3)(e_norm2)\n",
    "        \n",
    "        f3 = 1 / np.sqrt(self.embedding_fc3_dim)\n",
    "        e_fc3 = Dense(self.embedding_fc3_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3))(e_drop2)\n",
    "        e_norm3 = BatchNormalization(dtype='float64')(e_fc3)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_drop1, e_norm3])\n",
    "\n",
    "        #Creates the output network\n",
    "        f1 = 1 / np.sqrt(self.concat_fc1_dim)\n",
    "        c_fc1 = Dense(self.concat_fc1_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.concat_fc2_dim)\n",
    "        c_fc2 = Dense(self.concat_fc2_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2))(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        c_drop2 = Dropout(0.3)(c_norm2)\n",
    "        \n",
    "        f3 = 3e-3\n",
    "        out = Dense(self.out_dim, activation='tanh', dtype='float64', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3))(c_drop2)\n",
    "        lamb = Lambda(lambda i: i * self.act_range, dtype='float64')(out)\n",
    "        \n",
    "        model = Model(inputs=[s_inp, e_inp], outputs=[lamb])\n",
    "\n",
    "        model.compile(optimizer=self.optimizer)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, embeddings):\n",
    "        return self.model([states, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, embeddings):\n",
    "        return self.target_model([states, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + 'actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X5fbdpXoDxX0"
   },
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_inp_dim, \n",
    "        state_fc1_dim, \n",
    "        state_fc2_dim, \n",
    "        action_inp_dim, \n",
    "        action_fc1_dim, \n",
    "        conc_fc1_dim, \n",
    "        conc_fc2_dim, \n",
    "        out_dim, \n",
    "        lr, \n",
    "        tau,\n",
    "    ):\n",
    "\n",
    "        #Network dimensions\n",
    "        self.state_inp_dim = state_inp_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.state_fc2_dim = state_fc2_dim\n",
    "        self.action_inp_dim = action_inp_dim\n",
    "        self.action_fc1_dim = action_fc1_dim\n",
    "        self.conc_fc1_dim = conc_fc1_dim\n",
    "        self.conc_fc2_dim = conc_fc2_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        \n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "\n",
    "        #State input network ---------\n",
    "        s_inp = Input(shape=(self.state_inp_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.state_fc1_dim)\n",
    "        s_fc1 = Dense(self.state_fc1_dim, activation='relu', dtype='float64',  kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.state_fc2_dim)\n",
    "        s_fc2 = Dense(self.state_fc2_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2))(s_norm1)\n",
    "        s_norm2 = BatchNormalization(dtype='float64')(s_fc2)\n",
    "        s_drop2 = Dropout(0.3)(s_norm2)\n",
    "        \n",
    "        #Action input network ---------\n",
    "        a_inp = Input(shape=(self.action_inp_dim, ))\n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.action_fc1_dim)\n",
    "        a_fc1 = Dense(self.action_fc1_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(a_inp)\n",
    "        a_norm1 = BatchNormalization(dtype='float64')(a_fc1)\n",
    "        a_drop1 = Dropout(0.2)(a_norm1)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_drop2, a_drop1])\n",
    "        \n",
    "        #Creates the output network\n",
    "        f1 = 1 / np.sqrt(self.conc_fc1_dim)\n",
    "        c_fc1 = Dense(self.conc_fc1_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1))(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "        f2 = 1 / np.sqrt(self.conc_fc2_dim)\n",
    "        c_fc2 = Dense(self.conc_fc2_dim, activation='relu', dtype='float64', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2))(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        f3 = 3e-3\n",
    "        out = Dense(self.out_dim, activation='relu', dtype='float64',  kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3))(c_norm2)\n",
    "        \n",
    "        model = Model(inputs=[s_inp, a_inp], outputs=[out])\n",
    "\n",
    "        model.compile(optimizer = self.optimizer)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, actions):\n",
    "        return self.model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + 'critic.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "epuCthISDy-f"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputShape, \n",
    "        lr_init, \n",
    "        lr_end, \n",
    "        lr_decay\n",
    "    ):\n",
    "        self.lr_init = lr_init\n",
    "        self.lr = lr_init\n",
    "        self.inputShape = inputShape\n",
    "        self.encoder = self.createEncoder()\n",
    "        self.decoder = self.createDecoder()\n",
    "        self.autoencoder = self.createAutoencoder()\n",
    "        self.embedding_model = Model(self.encoder.input, Flatten()(self.encoder.output))\n",
    "        self.lr_end = lr_end\n",
    "        self.lr_decay = lr_decay\n",
    "        \n",
    "    #----------------------------------------------------------  \n",
    "    def createEncoder(self):\n",
    "        encoder = Sequential()\n",
    "        \n",
    "        encoder.add(Conv2D(32, 3, strides=1, padding='same', activation='relu', input_shape=(self.inputShape[0], self.inputShape[1], 3)))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "\n",
    "        encoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "        encoder.add(Dropout(0.2))\n",
    "\n",
    "        encoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "        \n",
    "\n",
    "        return encoder\n",
    "            \n",
    "    #----------------------------------------------------------  \n",
    "    def createDecoder(self):\n",
    "        decoder = Sequential()\n",
    "        \n",
    "        decoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu', input_shape=self.encoder.output.shape[1:]))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "        \n",
    "        decoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "        decoder.add(Dropout(0.2))\n",
    "        \n",
    "        decoder.add(Conv2D(3, 3, strides=1, padding='same', activation='relu'))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def createAutoencoder(self):\n",
    "        autoencoder = Model(inputs=self.encoder.input, outputs=self.decoder(self.encoder.outputs))\n",
    "\n",
    "        autoencoder.compile(optimizer=Adam(learning_rate=self.lr), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    def feed(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "        \n",
    "        pred = self.autoencoder(np.expand_dims(data, 0), training=False)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def train(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "\n",
    "        self.lr = self.lr * self.lr_decay\n",
    "        self.lr = min(self.lr, self.lr_end)\n",
    "        K.set_value(self.autoencoder.optimizer.learning_rate, self.lr)\n",
    "\n",
    "        history = self.autoencoder.fit(data, data, batch_size=5, epochs=2, validation_data=(data, data), verbose=0)\n",
    "        return history\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def getEmbedding(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "\n",
    "        pred = self.embedding_model(np.expand_dims(data, 0), training=False)\n",
    "\n",
    "        #return self.normalize(pred)\n",
    "        return pred\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def decode(self, data):\n",
    "        return self.normalize(self.decoder(data).numpy())\n",
    "    \n",
    "    #----------------------------------------------------------  \n",
    "    def normalize(self, data):\n",
    "        amin = np.amin(data)\n",
    "        amax = np.amax(data)\n",
    "        data = (data - amin)/(amax - amin)\n",
    "        return data\n",
    "    \n",
    "    #----------------------------------------------------------  \n",
    "    def saveModel(self, path):\n",
    "        self.autoencoder.save(path+'autoencoder')\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def loadModel(self, path):\n",
    "        self.autoencoder = load_model(path+'autoencoder')\n",
    "        \n",
    "    #----------------------------------------------------------  \n",
    "    @property\n",
    "    def embeddingDim(self):\n",
    "        return self.embedding_model.output.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ppTZn4icD0lA"
   },
   "outputs": [],
   "source": [
    "class DDPGAgent(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim, \n",
    "        action_dim, \n",
    "        action_min, \n",
    "        action_max, \n",
    "        memory_size, \n",
    "        batch_size, \n",
    "        gamma, \n",
    "        a_lr, \n",
    "        c_lr, \n",
    "        tau, \n",
    "        epsilon, \n",
    "        epsilon_decay, \n",
    "        epsilon_min,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(self.memory_size, self.batch_size)\n",
    "        \n",
    "        #Creates the autoencoder\n",
    "        self.autoencoder = Autoencoder((64, 128), 3e-3, 7e-4, 1-5e-6)\n",
    "\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            state_dim = self.state_dim,\n",
    "            state_fc1_dim=512,\n",
    "            embedding_dim = self.autoencoder.embeddingDim[0],\n",
    "            embedding_fc1_dim = 1024,\n",
    "            embedding_fc2_dim = 256,\n",
    "            embedding_fc3_dim = 64,\n",
    "            concat_fc1_dim = 256,\n",
    "            concat_fc2_dim = 64,\n",
    "            out_dim=self.action_dim, \n",
    "            act_range=self.action_max, \n",
    "            lr=self.a_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_inp_dim=self.state_dim, \n",
    "            state_fc1_dim=512, \n",
    "            state_fc2_dim=256,\n",
    "            action_inp_dim=self.action_dim, \n",
    "            action_fc1_dim=32,\n",
    "            conc_fc1_dim=512, \n",
    "            conc_fc2_dim=256, \n",
    "            out_dim=1,\n",
    "            lr=self.c_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "        \n",
    "        #Creates the noise generator\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
    "\n",
    "        \n",
    "        \n",
    "    #-------------------------------------------------------------------- \n",
    "    def policy(self, state, embedding, explore = True):\n",
    "        state = state[np.newaxis, :]\n",
    "        embedding = embedding[np.newaxis, :]\n",
    "        action = self.actor.predict(state, embedding)[0]\n",
    "        \n",
    "        #Takes the exploration with the epsilon probability\n",
    "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
    "            \n",
    "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
    "        return action\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done, embedding, new_embedding):\n",
    "        self.memory.append(state, action, reward, next_state, done, embedding, new_embedding)\n",
    "        if self.memory.hasMin: self.replay_memory()\n",
    "        \n",
    "    #--------------------------------------------------------------------    \n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        done = np.array([int(exp[4]) for exp in experiences])\n",
    "        embeddings = np.array([exp[5] for exp in experiences])\n",
    "        new_embeddings = np.array([exp[6] for exp in experiences])\n",
    "\n",
    "        \n",
    "        #Change the dimensions of the rewards and done arrays\n",
    "        rewards = rewards[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Train the critic\n",
    "        with GradientTape() as tape:\n",
    "            #Compute the critic target values\n",
    "            target_actions = self.actor.target_predict(next_states, new_embeddings)\n",
    "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions) * (1 - done)\n",
    "            #Compute the q_value of each next_state, next_action pair\n",
    "            critic_value = self.critic.predict(states, actions)\n",
    "            #Compute the critic loss \n",
    "            critic_loss = reduce_mean(square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with GradientTape() as tape:\n",
    "            acts = self.actor.predict(states, embeddings)\n",
    "            critic_grads = self.critic.predict(states, acts)\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -reduce_mean(critic_grads)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "        \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "        \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        #If its reach the minimum value it stops\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        return\n",
    "    #--------------------------------------------------------------------     \n",
    "    def act(self, env, verbose = False):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render(mode='human')\n",
    "            time.sleep(0.02)\n",
    "            embedding = np.squeeze(self.autoencoder.getEmbedding(env.render(mode = 'rgb_array')))\n",
    "            action = self.policy(observation, embedding, explore=False)\n",
    "            if verbose: print(action)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            observation = new_observation\n",
    "\n",
    "        return\n",
    "        \n",
    "    #--------------------------------------------------------------------     \n",
    "    def train(self, env, envName, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "        images = []\n",
    "\n",
    "        total = 0\n",
    "        hist = None\n",
    "        embedding = (np.random.rand(self.autoencoder.embeddingDim[0]) - 0.5) * 0.01\n",
    "        new_embedding = (np.random.rand(self.autoencoder.embeddingDim[0]) - 0.5) * 0.01\n",
    "        \n",
    "        #If the complete_num is smaller than 1 ist interpreted as a percentage else its a number of episodes\n",
    "        if complete_num < 1: complete_num = int(complete_num*verbose_batch) if int(complete_num*verbose_batch) != 0 else 1\n",
    "        \n",
    "        #Begin the training\n",
    "        print(\"BEGIN\\n\")\n",
    "        #Number of completed episodes per batch\n",
    "        complete = 0\n",
    "        \n",
    "        #Iterate on each episode\n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action = self.policy(observation, embedding)\n",
    "                new_observation, reward, done, _ = env.step(action)\n",
    "                \n",
    "                screen = env.render(mode = 'rgb_array')\n",
    "\n",
    "                if hist is not None:\n",
    "                    embedding = new_embedding\n",
    "                    new_embedding = self.autoencoder.getEmbedding(screen)\n",
    "                    new_embedding = np.squeeze(new_embedding)\n",
    "                    loss_mean = np.mean(hist.history['loss'])\n",
    "                    if loss_mean > 0.004: new_embedding = new_embedding * (1 / (1e4 * loss_mean))\n",
    "                    \n",
    "                \n",
    "                images.append(screen[150:][:][:])\n",
    "                self.learn(observation, action, reward, new_observation, done, embedding, new_embedding)\n",
    "\n",
    "                if(len(images) >= 100):\n",
    "                    hist = self.autoencoder.train(np.array(images))\n",
    "                    images = []\n",
    "\n",
    "                if verbose and total > 100:\n",
    "                    print(\"\\r                                                                                                              \", end=\"\")\n",
    "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score)+\"\\tLoss: \"+str(np.mean(hist.history['loss'])) ,end=\"\")\n",
    "                    \n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "                total += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig after completing a number of episodes\n",
    "                if end_on_complete and complete >= complete_num:\n",
    "                    break\n",
    "            \n",
    "            #These information are printed after each verbose_batch episodes\n",
    "            if((episode+1)%verbose_batch == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "                      , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_batch:])\n",
    "                      , \"\\n\\tLoss: \", np.mean(hist.history['loss'])\n",
    "                      , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_batch:])\n",
    "                      , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\")\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_batch episodes\n",
    "                if act_after_batch: self.act(env)\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        return scores_history, steps_history\n",
    "    \n",
    "    #--------------------------------------------------------------------     \n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "        self.autoencoder.saveModel(path)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def load(self, a_path, c_path, ae_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)\n",
    "        self.autoencoder.loadModel(ae_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS2ocQgZD5Ss",
    "outputId": "367377fa-d71b-4ee2-ddb7-fa140e4ded40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dimensions:  24\n",
      "Actions Dimensions:  4\n",
      "Action min:  [-1. -1. -1. -1.]\n",
      "Action max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "name = \"BipedalWalkerHardcore-v3\"\n",
    "\n",
    "env = gym.make(name, max_episode_steps=1000)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "print(\"State Dimensions: \", state_dim)\n",
    "print(\"Actions Dimensions: \", action_dim)\n",
    "print(\"Action min: \", action_min)\n",
    "print(\"Action max: \", action_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T5OqGa18D5jI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 18:17:51.707354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-11 18:17:51.754390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-07-11 18:17:51.754414: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-11 18:17:51.755030: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "memory_size = 10000\n",
    "batch_size = 128\n",
    "gamma = 0.95\n",
    "a_lr = 8e-5\n",
    "c_lr = 1e-4\n",
    "tau = 5e-3\n",
    "epsilon = 1\n",
    "epsilon_decay = 1-5e-7\n",
    "epsilon_min = 0.5\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, action_min, action_max, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min)\n",
    "\n",
    "nets_path = os.path.abspath('')+'/networks/11.4/walking_net/'\n",
    "agent.actor.loadModel(nets_path+'actor.h5')\n",
    "agent.critic.loadModel(nets_path+'critic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY4mZUGTD5v-",
    "outputId": "e93fbff0-9e64-4ac9-fb0a-aa41ea83b6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episode: 1\t Step: 418\tReward: -34.397792204220465\tLoss: 0.008910255506634712                                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=5'>6</a>\u001b[0m complete_value \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=6'>7</a>\u001b[0m act_after_batch \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=8'>9</a>\u001b[0m scores, steps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(env, name, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb Cell 11\u001b[0m in \u001b[0;36mDDPGAgent.train\u001b[0;34m(self, env, envName, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=197'>198</a>\u001b[0m     \u001b[39mif\u001b[39;00m loss_mean \u001b[39m>\u001b[39m \u001b[39m0.004\u001b[39m: new_embedding \u001b[39m=\u001b[39m new_embedding \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1e4\u001b[39m \u001b[39m*\u001b[39m loss_mean))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=200'>201</a>\u001b[0m images\u001b[39m.\u001b[39mappend(screen[\u001b[39m150\u001b[39m:][:][:])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=201'>202</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn(observation, action, reward, new_observation, done, embedding, new_embedding)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(images) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=204'>205</a>\u001b[0m     hist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautoencoder\u001b[39m.\u001b[39mtrain(np\u001b[39m.\u001b[39marray(images))\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb Cell 11\u001b[0m in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self, state, action, reward, next_state, done, embedding, new_embedding)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=85'>86</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m, state, action, reward, next_state, done, embedding, new_embedding):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=86'>87</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mappend(state, action, reward, next_state, done, embedding, new_embedding)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=87'>88</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mhasMin: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_memory()\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb Cell 11\u001b[0m in \u001b[0;36mDDPGAgent.replay_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=109'>110</a>\u001b[0m \u001b[39mwith\u001b[39;00m GradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=110'>111</a>\u001b[0m     \u001b[39m#Compute the critic target values\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=111'>112</a>\u001b[0m     target_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mtarget_predict(next_states, new_embeddings)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=112'>113</a>\u001b[0m     y \u001b[39m=\u001b[39m rewards \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mtarget_predict(next_states, target_actions) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m done)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=113'>114</a>\u001b[0m     \u001b[39m#Compute the q_value of each next_state, next_action pair\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=114'>115</a>\u001b[0m     critic_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mpredict(states, actions)\n",
      "\u001b[1;32m/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb Cell 11\u001b[0m in \u001b[0;36mCritic.target_predict\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtarget_predict\u001b[39m(\u001b[39mself\u001b[39m, states, actions):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/JUPYTER_LAB/My_Projects/Reinforcement_Learning/11.4_DDPG_BypedalWalker_Continuous.ipynb#ch0000009?line=93'>94</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_model([states, actions], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1099\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:451\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    434\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[1;32m    452\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/functional.py:589\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    586\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 589\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1087\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m   call_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autographed_call()\n\u001b[1;32m   1083\u001b[0m call_fn \u001b[39m=\u001b[39m traceback_utils\u001b[39m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m   1084\u001b[0m     call_fn,\n\u001b[1;32m   1085\u001b[0m     object_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1087\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(name_scope):\n\u001b[1;32m   1088\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_batch = 100\n",
    "end_on_complete = True\n",
    "complete_num = 0.5\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "scores, steps = agent.train(env, name, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for i in range(500):\n",
    "#     act = env.action_space.sample()\n",
    "#     env.step(act)\n",
    "#     img = env.render(mode='rgb_array')\n",
    "# gen = agent.autoencoder.feed(img)\n",
    "# plt.imshow(img[150:][:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def printModel(model):\n",
    "#     for i, layer in enumerate(model.layers): print(i, '\\t\\t', layer.name, '\\t\\t' if len(layer.name) > 5 else '\\t\\t\\t', layer.get_output_at(0).get_shape().as_list())\n",
    "\n",
    "# class AuxActor(object):\n",
    "#     def __init__(self, inp_dim, fc1_dim, fc2_dim, fc3_dim, out_dim, act_range, lr, tau):\n",
    "#         #Network dimensions\n",
    "#         self.inp_dim = inp_dim\n",
    "#         self.fc1_dim = fc1_dim\n",
    "#         self.fc2_dim = fc2_dim\n",
    "#         self.fc3_dim = fc3_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         #Range of the action space\n",
    "#         self.act_range = act_range\n",
    "#         #Parameter that coordinates the soft updates on the target weights\n",
    "#         self.tau = tau\n",
    "#         #Optimizer learning rate\n",
    "#         self.lr = lr\n",
    "#         #Generates the optimization function\n",
    "#         self.optimizer = Adam(self.lr)\n",
    "#         #Generates the actor model\n",
    "#         self.model = self.buildNetwork()\n",
    "#         #Generates the actor target model\n",
    "#         self.target_model = self.buildNetwork()\n",
    "#         #Set the weights to be the same in the begining\n",
    "#         self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def buildNetwork(self):\n",
    "#         inp = Input(shape=(self.inp_dim,))\n",
    "        \n",
    "#         f1 = 1 / np.sqrt(self.fc1_dim)\n",
    "#         fc1 = Dense(self.fc1_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f1, f1), bias_initializer=random_uniform_initializer(-f1, f1), dtype='float64')(inp)\n",
    "#         norm1 = BatchNormalization(dtype='float64')(fc1)\n",
    "        \n",
    "#         f2 = 1 / np.sqrt(self.fc2_dim)\n",
    "#         fc2 = Dense(self.fc2_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f2, f2), bias_initializer=random_uniform_initializer(-f2, f2), dtype='float64')(norm1)\n",
    "#         norm2 = BatchNormalization(dtype='float64')(fc2)\n",
    "        \n",
    "#         f3 = 1 / np.sqrt(self.fc3_dim)\n",
    "#         fc3 = Dense(self.fc3_dim, activation='relu', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm2)\n",
    "#         norm3 = BatchNormalization(dtype='float64')(fc3)\n",
    "        \n",
    "#         f3 = 0.003\n",
    "#         out = Dense(self.out_dim, activation='tanh', kernel_initializer=random_uniform_initializer(-f3, f3), bias_initializer=random_uniform_initializer(-f3, f3), dtype='float64')(norm3)\n",
    "#         lamb = Lambda(lambda i: i * self.act_range, dtype='float64')(out)\n",
    "        \n",
    "#         return Model(inputs=[inp], outputs=[lamb])\n",
    "    \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def predict(self, states):\n",
    "#         return self.model([states], training=False)\n",
    "    \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def target_predict(self, states):\n",
    "#         return self.target_model([states], training=False)\n",
    "    \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def transferWeights(self):\n",
    "#         weights = self.model.get_weights()\n",
    "#         target_weights = self.target_model.get_weights()\n",
    "#         new_weights = []\n",
    "        \n",
    "#         for i in range(len(weights)):\n",
    "#             new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "#         self.target_model.set_weights(new_weights)\n",
    "        \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def saveModel(self, path):\n",
    "#         self.model.save(path)\n",
    "    \n",
    "#     #--------------------------------------------------------------------\n",
    "#     def loadModel(self, path):\n",
    "#         self.model.load_weights(path)\n",
    "\n",
    "# walking_net_path = os.path.abspath('')+'/networks/11.1_DDPG_BipedalWalker/_actor.h5'\n",
    "# actor = AuxActor(\n",
    "#     inp_dim=state_dim, \n",
    "#     fc1_dim=512, \n",
    "#     fc2_dim=256, \n",
    "#     fc3_dim=64, \n",
    "#     out_dim=action_dim, \n",
    "#     act_range=action_max, \n",
    "#     lr=1,\n",
    "#     tau=1,\n",
    "# )\n",
    "# actor.loadModel(walking_net_path)\n",
    "# actorModel = actor.model\n",
    "# printModel(actorModel)\n",
    "\n",
    "# printModel(agent.actor.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_weights = actorModel.layers[1].get_weights()\n",
    "# agent.actor.model.layers[6].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[2].get_weights()\n",
    "# agent.actor.model.layers[8].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[3].get_weights()\n",
    "# extracted_shape = np.shape(extracted_weights[0])\n",
    "# current_weights = agent.actor.model.layers[13].get_weights()\n",
    "# for i in range(64): extracted_weights[0] = np.vstack([extracted_weights[0], current_weights[0][extracted_shape[0] + i]])\n",
    "# #for i in range(64): extracted_weights[0] = np.vstack([extracted_weights[0], ((np.random.rand(np.shape(extracted_weights[0])[-1]) - 0.5) * 1e-5).reshape((1, np.shape(extracted_weights[0])[-1]))])\n",
    "# agent.actor.model.layers[13].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[4].get_weights()\n",
    "# agent.actor.model.layers[14].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[5].get_weights()\n",
    "# agent.actor.model.layers[15].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[6].get_weights()\n",
    "# agent.actor.model.layers[16].set_weights(extracted_weights)\n",
    "\n",
    "# extracted_weights = actorModel.layers[7].get_weights()\n",
    "# agent.actor.model.layers[18].set_weights(extracted_weights)\n",
    "\n",
    "# agent.actor.model.save_weights(os.path.abspath('')+'/networks/11.4/walking_net/actor.h5')\n",
    "\n",
    "# #agent.critic.loadModel(os.path.abspath('')+'/networks/11.1_DDPG_BipedalWalker/_critic.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
