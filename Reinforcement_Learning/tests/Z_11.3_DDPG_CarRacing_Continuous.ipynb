{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Deep Deterministic Policy Gradients***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/user/.local/lib/python3.10/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import GradientTape, random_uniform_initializer\n",
    "from tensorflow.math import reduce_mean, square\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing .image import smart_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BidirectionalWeights(object):\n",
    "    def __init__(self, num_elements):\n",
    "        self.num_elements = num_elements\n",
    "        self.weights = np.zeros(num_elements)\n",
    "        \n",
    "    def setWeight(self, el1, el2, val):\n",
    "        if el1 == el2: return\n",
    "        self.weights[self.matrixToVectorIndex(el1, el2)] = val\n",
    "        \n",
    "    def matrixToVectorIndex(self, lin, col):\n",
    "        _lin = max(lin, col)\n",
    "        _col = min(lin, col)\n",
    "        return (_col * self.num_elements) + _lin - np.sum(np.arange(_col + 1))\n",
    "\n",
    "        \n",
    "    def getActivations(self, inputs):\n",
    "        out = []\n",
    "        for inp in inputs:\n",
    "            activation = []\n",
    "            inp = np.array(inp)\n",
    "            for i in range(self.num_elements):\n",
    "                indexes = np.array([self.matrixToVectorIndex(i, x) for x in range(self.num_elements)])\n",
    "                out.append(inp.reshape(indexes.shape) * self.weights[indexes])\n",
    "                \n",
    "        return out\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Exploration**\n",
    "For continuous action spaces, exploration is done via adding noise to the action itself.  In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output. Is a type of noise that models Brownian motion (motion of particles in a fluid coliding with other particles at random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck Noise \n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mean, sigma=0.5, theta=0.4, dt=0.1, x0=None):\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    #Method that enables to write classes where the instances behave like functions and can be called like a function.    \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01923721, -0.16140239,  0.1752381 , -0.18277233,  0.05363763,\n",
       "        0.26576949, -0.11834553, -0.46846128,  0.02355322,  0.16522505,\n",
       "       -0.22234491, -0.02744782,  0.09742626,  0.19589356,  0.0634802 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(15)\n",
    "b = OUActionNoise(a)\n",
    "a += b()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Replay Buffer**\n",
    "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Replay Buffer \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, minibatch_size = None):\n",
    "        '''\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        '''\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "        \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def append(self, state, action, reward, next_state, done, embedding, new_embedding):\n",
    "        '''\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            done (boolen): True if the next state is a terminal state and False otherwise.\n",
    "                           Is transformed to integer so tha True = 1, False = 0\n",
    "            next_state (Numpy array): The next state.           \n",
    "        '''\n",
    "        if self.size() == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, int(done), embedding, new_embedding])\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        '''\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------    \n",
    "    def size(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def isMin(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Boolean indicating if the memory have the minimum number of elements or not\n",
    "        '''\n",
    "        return (self.size() >= self.minibatch_size)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def empties(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    def getEpisode(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            List with all the elements in the buffer\n",
    "        '''\n",
    "        return self.buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Actor (Policy) & Critic (Value)**\n",
    "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space\n",
    "\n",
    "The target networks are time-delayed copies of their original networks that slowly track the learned networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim,\n",
    "        state_fc1_dim,\n",
    "        state_fc2_dim,\n",
    "        embedding_dim,\n",
    "        embedding_fc1_dim,\n",
    "        embedding_fc2_dim,\n",
    "        embedding_fc3_dim,\n",
    "        concat_fc1_dim,\n",
    "        concat_fc2_dim,\n",
    "        concat_fc3_dim,\n",
    "        out_dim, \n",
    "        act_range, \n",
    "        lr, \n",
    "        tau,\n",
    "    ):\n",
    "        #Network dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.state_fc2_dim = state_fc2_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_fc1_dim = embedding_fc1_dim\n",
    "        self.embedding_fc2_dim = embedding_fc2_dim\n",
    "        self.embedding_fc3_dim = embedding_fc3_dim\n",
    "        self.concat_fc1_dim = concat_fc1_dim\n",
    "        self.concat_fc2_dim = concat_fc2_dim\n",
    "        self.concat_fc3_dim = concat_fc3_dim\n",
    "        self.out_dim = out_dim\n",
    "        #Range of the action space\n",
    "        self.act_range = act_range\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "        #Generates the optimization function\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "        #Generates the actor model\n",
    "        self.model = self.buildNetwork()\n",
    "        #Generates the actor target model\n",
    "        self.target_model = self.buildNetwork()\n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        \n",
    "        # State network\n",
    "        s_inp = Input(shape=(self.state_dim, ))\n",
    "        \n",
    "        s_fc1 = Dense(self.state_fc1_dim, activation='relu', dtype='float64')(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        s_fc2 = Dense(self.state_fc2_dim, activation='relu', dtype='float64')(s_norm1)\n",
    "        s_norm2 = BatchNormalization(dtype='float64')(s_fc2)\n",
    "        \n",
    "        #Embedding network\n",
    "        e_inp = Input(shape=(self.embedding_dim, ))\n",
    "        \n",
    "        e_fc1 = Dense(self.embedding_fc1_dim, activation='relu', dtype='float64')(e_inp)\n",
    "        e_norm1 = BatchNormalization(dtype='float64')(e_fc1)\n",
    "        \n",
    "        e_fc2 = Dense(self.embedding_fc2_dim, activation='relu', dtype='float64')(e_norm1)\n",
    "        e_norm2 = BatchNormalization(dtype='float64')(e_fc2)\n",
    "        \n",
    "        e_fc3 = Dense(self.embedding_fc3_dim, activation='relu', dtype='float64')(e_norm2)\n",
    "        e_norm3 = BatchNormalization(dtype='float64')(e_fc3)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_norm2, e_norm3])\n",
    "\n",
    "        #Creates the output network\n",
    "       \n",
    "        c_fc1 = Dense(self.concat_fc1_dim, activation='relu', dtype='float64')(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "               \n",
    "        c_fc2 = Dense(self.concat_fc2_dim, activation='relu', dtype='float64')(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        \n",
    "        c_fc3 = Dense(self.concat_fc3_dim, activation='relu', dtype='float64')(c_norm2)\n",
    "        c_norm3 = BatchNormalization(dtype='float64')(c_fc3)\n",
    "        \n",
    "        out = Dense(self.out_dim, activation='tanh', dtype='float64')(c_norm3)\n",
    "        lamb = Lambda(lambda i: i * self.act_range, dtype='float64')(out)\n",
    "        \n",
    "        return Model(inputs=[s_inp, e_inp], outputs=[lamb])\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, embeddings):\n",
    "        return self.model([states, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, embeddings):\n",
    "        return self.target_model([states, embeddings], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_inp_dim, \n",
    "        state_fc1_dim, \n",
    "        state_fc2_dim, \n",
    "        action_inp_dim, \n",
    "        action_fc1_dim, \n",
    "        conc_fc1_dim, \n",
    "        conc_fc2_dim, \n",
    "        out_dim, \n",
    "        lr, \n",
    "        tau,\n",
    "    ):\n",
    "\n",
    "        #Network dimensions\n",
    "        self.state_inp_dim = state_inp_dim\n",
    "        self.state_fc1_dim = state_fc1_dim\n",
    "        self.state_fc2_dim = state_fc2_dim\n",
    "        self.action_inp_dim = action_inp_dim\n",
    "        self.action_fc1_dim = action_fc1_dim\n",
    "        self.conc_fc1_dim = conc_fc1_dim\n",
    "        self.conc_fc2_dim = conc_fc2_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        #Optimizer learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        #Define the critic optimizer\n",
    "        self.optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "        #Parameter that coordinates the soft updates on the target weights\n",
    "        self.tau = tau\n",
    "\n",
    "        #Generate the critic network\n",
    "        self.model = self.buildNetwork()\n",
    "\n",
    "        #Generate the critic target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        \n",
    "        #Set the weights to be the same in the begining\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    def buildNetwork(self):\n",
    "        #State input network ---------\n",
    "        s_inp = Input(shape=(self.state_inp_dim, ))\n",
    "        \n",
    "        s_fc1 = Dense(self.state_fc1_dim, activation='relu', dtype='float64')(s_inp)\n",
    "        s_norm1 = BatchNormalization(dtype='float64')(s_fc1)\n",
    "        \n",
    "        s_fc2 = Dense(self.state_fc2_dim, activation='relu', dtype='float64')(s_norm1)\n",
    "        s_norm2 = BatchNormalization(dtype='float64')(s_fc2)\n",
    "        \n",
    "        #Action input network ---------\n",
    "        a_inp = Input(shape=(self.action_inp_dim, ))\n",
    "        \n",
    "        a_fc1 = Dense(self.action_fc1_dim, activation='relu', dtype='float64')(a_inp)\n",
    "        a_norm1 = BatchNormalization(dtype='float64')(a_fc1)\n",
    "        \n",
    "        #Concatenate the two networks ---\n",
    "        c_inp = Concatenate(dtype='float64')([s_norm2, a_norm1])\n",
    "        \n",
    "        #Creates the output network\n",
    "        c_fc1 = Dense(self.conc_fc1_dim, activation='relu', dtype='float64')(c_inp)\n",
    "        c_norm1 = BatchNormalization(dtype='float64')(c_fc1)\n",
    "        \n",
    "        c_fc2 = Dense(self.conc_fc2_dim, activation='relu', dtype='float64')(c_norm1)\n",
    "        c_norm2 = BatchNormalization(dtype='float64')(c_fc2)\n",
    "        \n",
    "        out = Dense(self.out_dim, activation='relu', dtype='float64')(c_norm2)\n",
    "        \n",
    "        model = Model(inputs=[s_inp, a_inp], outputs=[out])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def predict(self, states, actions):\n",
    "        return self.model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def target_predict(self, states, actions):\n",
    "        return self.target_model([states, actions], training=False)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def transferWeights(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        new_weights = []\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            new_weights.append((self.tau * weights[i]) + ((1.0 - self.tau) * target_weights[i]))\n",
    "        \n",
    "        self.target_model.set_weights(new_weights)\n",
    "        \n",
    "    #--------------------------------------------------------------------\n",
    "    def saveModel(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def loadModel(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputShape, \n",
    "        lr_init, \n",
    "        lr_end, \n",
    "        lr_decay\n",
    "    ):\n",
    "        self.lr_init = lr_init\n",
    "        self.lr = lr_init\n",
    "        self.inputShape = inputShape\n",
    "        self.encoder = self.createEncoder()\n",
    "        self.decoder = self.createDecoder()\n",
    "        self.autoencoder = self.createAutoencoder()\n",
    "        self.embedding_model = Model(self.encoder.input, Flatten()(self.encoder.output))\n",
    "        self.lr_end = lr_end\n",
    "        self.lr_decay = lr_decay\n",
    "        \n",
    "    #----------------------------------------------------------  \n",
    "    def createEncoder(self):\n",
    "        encoder = Sequential()\n",
    "        \n",
    "        encoder.add(Conv2D(32, 3, strides=1, padding='same', activation='relu', input_shape=(self.inputShape[0], self.inputShape[1], 3)))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "\n",
    "        encoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "\n",
    "        encoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "        \n",
    "        encoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        encoder.add(MaxPooling2D(2, strides=2))\n",
    "\n",
    "        return encoder\n",
    "            \n",
    "    #----------------------------------------------------------  \n",
    "    def createDecoder(self):\n",
    "        decoder = Sequential()\n",
    "        \n",
    "        decoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu', input_shape=self.encoder.output.shape[1:]))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "\n",
    "        decoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "        \n",
    "        decoder.add(Conv2D(64, 3, strides=1, padding='same', activation='relu'))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "        \n",
    "        decoder.add(Conv2D(3, 3, strides=1, padding='same', activation='relu'))\n",
    "        decoder.add(UpSampling2D(2))\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def createAutoencoder(self):\n",
    "        autoencoder = Model(inputs=self.encoder.input, outputs=self.decoder(self.encoder.outputs))\n",
    "\n",
    "        autoencoder.compile(optimizer=Adam(learning_rate=self.lr), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    def feed(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "        \n",
    "        pred = self.autoencoder(np.expand_dims(data, 0), training=False)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def train(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "\n",
    "        self.lr = self.lr * self.lr_decay\n",
    "        self.lr = min(self.lr, self.lr_end)\n",
    "        K.set_value(self.autoencoder.optimizer.learning_rate, self.lr)\n",
    "\n",
    "        history = self.autoencoder.fit(data, data, batch_size=5, epochs=2, validation_data=(data, data), verbose=0)\n",
    "        return history\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def getEmbedding(self, data):\n",
    "        data = smart_resize(data, self.inputShape)\n",
    "        data = self.normalize(data)\n",
    "\n",
    "        pred = self.embedding_model(np.expand_dims(data, 0), training=False)\n",
    "\n",
    "        #return self.normalize(pred)\n",
    "        return pred\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def decode(self, data):\n",
    "        return self.normalize(self.decoder(data).numpy())\n",
    "    \n",
    "    #----------------------------------------------------------  \n",
    "    def normalize(self, data):\n",
    "        amin = np.amin(data)\n",
    "        amax = np.amax(data)\n",
    "        data = (data - amin)/(amax - amin)\n",
    "        return data\n",
    "    \n",
    "    #----------------------------------------------------------  \n",
    "    def saveModel(self, path):\n",
    "        self.autoencoder.save_weights(path+'_autoencoder.h5')\n",
    "\n",
    "    #----------------------------------------------------------  \n",
    "    def loadModel(self, path):\n",
    "        self.autoencoder.load_weights(path+'_autoencoder.h5')\n",
    "        \n",
    "    #----------------------------------------------------------  \n",
    "    @property\n",
    "    def embeddingDim(self):\n",
    "        return self.embedding_model.output.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DDPGAgent(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim, \n",
    "        action_dim, \n",
    "        action_min, \n",
    "        action_max, \n",
    "        memory_size, \n",
    "        batch_size, \n",
    "        gamma, \n",
    "        a_lr, \n",
    "        c_lr, \n",
    "        tau, \n",
    "        epsilon, \n",
    "        epsilon_decay, \n",
    "        epsilon_min,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        #Creates the Replay Buffer\n",
    "        self.memory = ReplayBuffer(self.memory_size, self.batch_size)\n",
    "        \n",
    "        #Creates the autoencoder\n",
    "        self.autoencoder = Autoencoder((128, 128), 3e-3, 7e-4, 0.99991)\n",
    "\n",
    "        #Creates the actor\n",
    "        self.actor = Actor(\n",
    "            state_dim = self.state_dim,\n",
    "            state_fc1_dim=512,\n",
    "            state_fc2_dim = 128,\n",
    "            embedding_dim = self.autoencoder.embeddingDim[0],\n",
    "            embedding_fc1_dim = 512,\n",
    "            embedding_fc2_dim = 128,\n",
    "            embedding_fc3_dim = 64,\n",
    "            concat_fc1_dim = 256,\n",
    "            concat_fc2_dim = 128,\n",
    "            concat_fc3_dim = 32,\n",
    "            out_dim=self.action_dim, \n",
    "            act_range=self.action_max, \n",
    "            lr=self.a_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "\n",
    "        #Creates the critic\n",
    "        self.critic = Critic(\n",
    "            state_inp_dim=self.state_dim, \n",
    "            state_fc1_dim=512, \n",
    "            state_fc2_dim=128,\n",
    "            action_inp_dim=self.action_dim, \n",
    "            action_fc1_dim=32,\n",
    "            conc_fc1_dim=256, \n",
    "            conc_fc2_dim=64, \n",
    "            out_dim=1,\n",
    "            lr=self.c_lr, \n",
    "            tau=self.tau,\n",
    "        )\n",
    "        \n",
    "        #Creates the noise generator\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(action_dim))\n",
    "\n",
    "        \n",
    "        \n",
    "    #-------------------------------------------------------------------- \n",
    "    def policy(self, state, embedding, explore = True):\n",
    "        state = state[np.newaxis, :]\n",
    "        embedding = embedding[np.newaxis, :]\n",
    "        action = self.actor.predict(state, embedding)[0]\n",
    "        #Takes the exploration with the epsilon probability\n",
    "        if explore and np.random.rand() < self.epsilon: action += self.ou_noise()\n",
    "            \n",
    "        action = np.clip(action, a_min=self.action_min, a_max=self.action_max)\n",
    "        return action\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done, embedding, new_embedding):\n",
    "        self.memory.append(state, action, reward, next_state, done, embedding, new_embedding)\n",
    "        \n",
    "        if self.memory.isMin():\n",
    "            self.replay_memory()\n",
    "        \n",
    "    #--------------------------------------------------------------------    \n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        done = np.array([int(exp[4]) for exp in experiences])\n",
    "        embeddings = np.array([exp[5] for exp in experiences])\n",
    "        new_embeddings = np.array([exp[6] for exp in experiences])\n",
    "\n",
    "        \n",
    "        #Change the dimensions of the rewards and done arrays\n",
    "        rewards = rewards[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Train the critic\n",
    "        with GradientTape() as tape:\n",
    "            #Compute the critic target values\n",
    "            target_actions = self.actor.target_predict(next_states, new_embeddings)\n",
    "            y = rewards + self.gamma * self.critic.target_predict(next_states, target_actions) * (1 - done)\n",
    "            #Compute the q_value of each next_state, next_action pair\n",
    "            critic_value = self.critic.predict(states, actions)\n",
    "            #Compute the critic loss \n",
    "            critic_loss = reduce_mean(square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.model.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grad, self.critic.model.trainable_variables))\n",
    "        \n",
    "        #Train the actor\n",
    "        with GradientTape() as tape:\n",
    "            acts = self.actor.predict(states, embeddings)\n",
    "            critic_grads = self.critic.predict(states, acts)\n",
    "            #Used -mean as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -reduce_mean(critic_grads)\n",
    "            \n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.model.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.model.trainable_variables))\n",
    "        \n",
    "        #Update the model weights\n",
    "        self.actor.transferWeights()\n",
    "        self.critic.transferWeights() \n",
    "        \n",
    "        #Decay the epsilon value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        #If its reach the minimum value it stops\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        return\n",
    "    #--------------------------------------------------------------------     \n",
    "    def act(self, env, verbose = False):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render(mode='human')\n",
    "            time.sleep(0.02)\n",
    "            embedding = np.squeeze(self.autoencoder.getEmbedding(env.render(mode = 'rgb_array')))\n",
    "            action = self.policy(observation, embedding, explore=False)\n",
    "            if verbose: print(action)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            observation = new_observation\n",
    "\n",
    "        return\n",
    "        \n",
    "    #--------------------------------------------------------------------     \n",
    "    def train(self, env, envName, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch):\n",
    "        scores_history = []\n",
    "        steps_history = []\n",
    "        images = []\n",
    "        total = 0\n",
    "        hist = None\n",
    "        genearte_embedding = False\n",
    "        \n",
    "        #If the complete_num is smaller than 1 ist interpreted as a percentage else its a number of episodes\n",
    "        if complete_num < 1:\n",
    "            complete_num = int(complete_num*verbose_batch) if int(complete_num*verbose_batch) != 0 else 1\n",
    "        \n",
    "        #Begin the training\n",
    "        print(\"BEGIN\\n\")\n",
    "        #Number of completed episodes per batch\n",
    "        complete = 0\n",
    "        \n",
    "        #Iterate on each episode\n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            steps = 0\n",
    "            observation = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                embedding = (np.random.rand(self.autoencoder.embeddingDim) - 0.5) * 0.01\n",
    "                new_embedding = (np.random.rand(self.autoencoder.embeddingDim) - 0.5) * 0.01\n",
    "                action = self.policy(observation, embedding)\n",
    "                new_observation, reward, done, _ = env.step(action)\n",
    "                \n",
    "                screen = env.render(mode = 'rgb_array')\n",
    "                if (hist is not None and np.mean(hist.history['loss']) < 0.005) or genearte_embedding:\n",
    "                    genearte_embedding = True\n",
    "                    embedding = new_embedding\n",
    "                    new_embedding = self.autoencoder.getEmbedding(screen)\n",
    "                    new_embedding = np.squeeze(new_embedding)\n",
    "                \n",
    "                images.append(screen)\n",
    "                self.learn(observation, action, reward, new_observation, done, embedding, new_embedding)\n",
    "\n",
    "                if(len(images) >= 100):\n",
    "                    hist = self.autoencoder.train(np.array(images))\n",
    "                    images = []\n",
    "\n",
    "                if verbose and total > 100:\n",
    "                    print(\"\\r                                                                                                              \", end=\"\")\n",
    "                    print(\"\\rEpisode: \"+str(episode+1)+\"\\t Step: \"+str(steps)+\"\\tReward: \"+str(score)+\"\\tLoss: \"+str(np.mean(hist.history['loss'])) ,end=\"\")\n",
    "                    \n",
    "                observation = new_observation\n",
    "                score += reward\n",
    "                steps += 1\n",
    "                total += 1\n",
    "\n",
    "            scores_history.append(score)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            #If the score is bigger or equal than the complete score it add one to the completed number\n",
    "            if(score >= complete_value):\n",
    "                complete += 1\n",
    "                #If the flag is true the agent ends the trainig after completing a number of episodes\n",
    "                if end_on_complete and complete >= complete_num:\n",
    "                    break\n",
    "            \n",
    "            #These information are printed after each verbose_batch episodes\n",
    "            if((episode+1)%verbose_batch == 0):\n",
    "                print(\"\\r                                                                                                          \", end=\"\")\n",
    "                print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "                      , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose_batch:])\n",
    "                      , \"\\n\\tLoss: \", np.mean(hist.history['loss'])\n",
    "                      , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose_batch:])\n",
    "                      , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\")\n",
    "                \n",
    "                #If the flag is true the agent act and render the episode after each verbose_batch episodes\n",
    "                if act_after_batch: self.act(env)\n",
    "                \n",
    "                #Set the number of completed episodes on the batch to zero\n",
    "                complete = 0\n",
    "\n",
    "        print(\"\\nFINISHED\")\n",
    "        return scores_history, steps_history\n",
    "    \n",
    "    #--------------------------------------------------------------------     \n",
    "    def save(self, path):\n",
    "        self.actor.saveModel(path)\n",
    "        self.critic.saveModel(path)\n",
    "        self.autoencoder.saveModel(path)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    def load(self, a_path, c_path, ae_path):\n",
    "        self.actor.loadModel(a_path)\n",
    "        self.critic.loadModel(c_path)\n",
    "        self.autoencoder.loadModel(ae_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dimensions:  24\n",
      "Actions Dimensions:  4\n",
      "Action min:  [-1. -1. -1. -1.]\n",
      "Action max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "name = \"BipedalWalkerHardcore-v3\"\n",
    "\n",
    "env = gym.make(name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_min = env.action_space.low\n",
    "action_max = env.action_space.high\n",
    "\n",
    "print(\"State Dimensions: \", state_dim)\n",
    "print(\"Actions Dimensions: \", action_dim)\n",
    "print(\"Action min: \", action_min)\n",
    "print(\"Action max: \", action_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 10:22:02.732892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-09 10:22:03.841114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-07-09 10:22:03.841137: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-09 10:22:03.870968: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "memory_size = 100000\n",
    "batch_size = 128\n",
    "gamma = 0.97\n",
    "a_lr = 4e-5\n",
    "c_lr = 6e-5\n",
    "tau = 2e-3\n",
    "epsilon = 1\n",
    "epsilon_decay = 1-5e-7\n",
    "epsilon_min = 0.5\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, action_min, action_max, memory_size, batch_size, gamma, a_lr, c_lr, tau, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes:  100 / 3000                                                                                         \n",
      "\tTotal reward:  -106.05918441365165 \n",
      "\tLoss:  0.0015178651665337384 \n",
      "\tNum. steps:  77.05 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  200 / 3000                                                                                         \n",
      "\tTotal reward:  -113.95450773369926 \n",
      "\tLoss:  0.001546781975775957 \n",
      "\tNum. steps:  95.59 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episodes:  300 / 3000                                                                                         \n",
      "\tTotal reward:  -106.19664169660683 \n",
      "\tLoss:  0.0011882651597261429 \n",
      "\tNum. steps:  104.29 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "Episode: 380\t Step: 16\tReward: -2.41909371301208\tLoss: 0.001693728321697563                                   "
     ]
    }
   ],
   "source": [
    "num_episodes = 3000\n",
    "verbose = True\n",
    "verbose_batch = 100\n",
    "end_on_complete = True\n",
    "complete_num = 0.5\n",
    "complete_value = 300\n",
    "act_after_batch = True\n",
    "\n",
    "scores, steps = agent.train(env, name, num_episodes, verbose, verbose_batch, end_on_complete, complete_num, complete_value, act_after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "The 'python3105jvsc74a57bd0d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48' kernel is not available. Please pick another suitable kernel instead, or install that kernel. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(40):\n",
    "    act = env.action_space.sample()\n",
    "    env.step(act)\n",
    "    img = env.render(mode='rgb_array')\n",
    "gen = agent.autoencoder.feed(img)\n",
    "plt.imshow(gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "The 'python3105jvsc74a57bd0d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48' kernel is not available. Please pick another suitable kernel instead, or install that kernel. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env_act = gym.make(name, max_episode_steps=500)\n",
    "agent.act(env_act, verbose = True)\n",
    "env_act.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEEP THIS CELL AS THE LAST CELL AND DON'T CALL env.close() ANYWHERE ELSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3c17e3122d25d527b33a00365b6902e14e8de10f291729ab7d4e45a04e4f03d9"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
