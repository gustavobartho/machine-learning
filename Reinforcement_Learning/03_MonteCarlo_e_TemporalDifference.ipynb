{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlos Methos e Temporal Difference Method\n",
    "\n",
    "### Envirorment\n",
    "\n",
    "* N estados não terminais $S+ = {s1, s2, ..., sN}$, onde $N >= 5$\n",
    "* Duas ações deterministicas em cada estado (direita ou esquerda) $A(S) = {0, 1}$ onde `0 = esquerda` e `1 = direita`\n",
    "* Uniform Random Policy $\\pi(.|s) = 0.5$ para todo $s$ pertencente a $S$\n",
    "* Os episodios começam no estado $s3$\n",
    "* Todos os episodios terminam ao serem atingidos a extremidade esquerda($L$) ou direita($D$)\n",
    "* $S = {L, s1, s2, s3, ..., sN, D}$\n",
    "* Os rewards sõ 0 para todas as transições exceto na que leva de $sN$ para $D$, onde o reward é $+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv:\n",
    "    def __init__(self, N = 5, init_state = 3):\n",
    "        self.N = N #Numero de estados não terminais\n",
    "        self.init_state = init_state #Estado no qual sera iniciado cada episodio\n",
    "        self.rewards = np.zeros((self.N + 2)) #Vetor com os rewards de cada estado incluindo os estados terminaais\n",
    "        self.rewards[-1] = 1 #O reward do ultimo estado é 1\n",
    "    \n",
    "    def init(self):\n",
    "        self.state = self.init_state\n",
    "        self.terminal = False\n",
    "        reward = self.rewards[self.state]\n",
    "        return self.state, reward, self.terminal\n",
    "        \n",
    "    def step(self, action):\n",
    "        if (self.terminal == False):\n",
    "            if(action == 0): #Se for para a esquerda\n",
    "                if(self.state == 1): #Se o estado atual for o estado 1\n",
    "                    self.terminal = True #O episodio terminou pois chegou no estado terminal da esquerda\n",
    "                else:\n",
    "                    self.terminal = False\n",
    "                self.state = self.state - 1\n",
    "                reward = self.rewards[self.state]\n",
    "\n",
    "            elif(action == 1):\n",
    "                if(self.state == self.N):\n",
    "                    self.terminal = True\n",
    "                else:\n",
    "                    self.terminal = False\n",
    "                self.state = self.state + 1\n",
    "                reward = self.rewards[self.state]\n",
    "        else:\n",
    "            reward = 0\n",
    "            \n",
    "        return self.state, reward, self.terminal\n",
    "            \n",
    "        \n",
    "    def getStates(self):\n",
    "        return np.arange(self.N)\n",
    "    \n",
    "    def getActions(self):\n",
    "        return [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myAgent:\n",
    "    def __init__(self, pred_type, pol, init_values = 0.5, alpha = 0.1, gamma = 1):\n",
    "        self.pred_type = pred_type\n",
    "        self.init_values = init_values\n",
    "        self.pol = pol\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def train(self, env, n_episodes):\n",
    "        #Tecnica de prediction para Monte Carlo Method\n",
    "        if(self.pred_type == \"MCM\"): \n",
    "            #Inicializa o array dos values de cada estado com o numero de estados do evirorment\n",
    "            self.n_states = len(env.getStates())\n",
    "            self.V = np.full((self.n_states), self.init_values)\n",
    "            #Array com os returns de cada estado\n",
    "            returns = [] \n",
    "            #Inicializa os returns com uma lista vazia para cada estado\n",
    "            for _ in range(self.n_states):\n",
    "                state_g = []\n",
    "                returns.append(state_g)\n",
    "            #Realiza cada episode\n",
    "            for _ in range(n_episodes):\n",
    "                #Arrays dos estados por onde passou e as rewards nesses estados\n",
    "                states = []\n",
    "                rewards = []\n",
    "                #Inicializa o envirorment\n",
    "                state, reward, terminal = env.init()\n",
    "                #Enquanto não atingir o estado terminal\n",
    "                while(terminal == False):\n",
    "                    states.append(state)\n",
    "                    action = self.argmax(pi[state - 1], 2)\n",
    "                    state, reward, terminal = env.step(action)\n",
    "                    rewards.append(reward)\n",
    "                G = 0\n",
    "                ind = len(states) - 1\n",
    "                for rew in reversed(rewards):\n",
    "                    G = self.gamma*G + rew\n",
    "                    returns[states[ind] - 1].append(G)\n",
    "                    self.V[states[ind] - 1] = np.mean(returns[states[ind] - 1])\n",
    "                    ind -= 1\n",
    "            return self.V\n",
    "        elif(self.pred_type == \"TD\"):\n",
    "            self.n_states = len(env.getStates())\n",
    "            self.V = np.full((self.n_states + 2), self.init_values)\n",
    "            self.V[0] = 0\n",
    "            self.V[-1] = 0\n",
    "            for _ in range(n_episodes):\n",
    "                next_state, reward, terminal = env.init()\n",
    "                while(terminal == False):\n",
    "                    state = next_state\n",
    "                    action = self.argmax(pi[state - 1], 2)\n",
    "                    next_state, reward, terminal = env.step(action)\n",
    "                    self.V[state] = self.V[state] + self.alpha*(reward + self.gamma*self.V[next_state] - self.V[state])\n",
    "            return self.V\n",
    "    \n",
    "    \n",
    "    def argmax(self, q_values, act_num):\n",
    "        '''Funcao que retorna o indice do valor maximo de um vetor q_values, \n",
    "        caso haja mais de um valor maximo entao retorna um deles aletoriamente'''\n",
    "        top_value = float(\"-inf\") #maior Q(a)\n",
    "        ties = [] #Acoes que empataram entre o maior valor de Q(a)\n",
    "        for i in range(act_num):\n",
    "            if(q_values[i] > top_value):     #Se o Q(a) for maior que o maximo ate o momento:\n",
    "                top_value = q_values[i]          #Substitui o Q(a) maximo pelo valor do atual\n",
    "                ties = []                        #Zera o vetor de empates das acoes com o Q(a) maximo\n",
    "                ties.append(i)                   #Adiciona a acao atual no vetor de empates\n",
    "            elif(q_values[i] == top_value):  #Se o Q(a) for igual ao maximo ate o momento:\n",
    "                ties.append(i)                   #Adiciona a ação ao vetor de empates\n",
    "        return np.random.choice(ties)            #Retorna um valor aleatorio do vetor de empates do Q maximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 6\n",
    "env = MyEnv(N = n_states)\n",
    "env.init()\n",
    "num_a = len(env.getActions())\n",
    "num_s = len(env.getStates())\n",
    "pi = np.full((num_s, num_a), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCM V(S):\t  [0.13792076 0.28738525 0.4373436  0.57654227 0.7118777  0.85453825]\n",
      "TD V(S):\t  [0.12428888 0.30055886 0.46776297 0.61284068 0.74499033 0.8826055 ]\n"
     ]
    }
   ],
   "source": [
    "agent_mcm = myAgent(\"MCM\", pi)\n",
    "agent_td = myAgent(\"TD\", pi)\n",
    "\n",
    "V_MCM = agent_mcm.train(env, 3000)\n",
    "V_TD = agent_td.train(env, 3000)\n",
    "\n",
    "print(\"MCM V(S):\\t \", V_MCM)\n",
    "print(\"TD V(S):\\t \", V_TD[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
