{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI gym introduction\n",
    "\n",
    "OpenAI Gym is a framework that allows us to easily deploy, compare,\n",
    "and test Reinforcement Learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando a bilblioteca\n",
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Envirorment():\n",
    "    try:\n",
    "        total = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        #Cria o envirorment\n",
    "        env = gym.make(\"BipedalWalker-v3\")\n",
    "        #Seta o env. p/ o estado inicial\n",
    "        env.reset()\n",
    "        #Realiza 50 time-steps\n",
    "        print(\"Actions: \", env.action_space)\n",
    "        print(\"State: \", env.observation_space)\n",
    "        print(\"------------------------------------\")\n",
    "        while done == False:\n",
    "            #Cria a janela visual do ambiente\n",
    "            env.render()\n",
    "            #Espera um tempo pois a simulação é muito rapida no jupyter\n",
    "            time.sleep(0.02)\n",
    "            #Seleciona uma ação aleatória do action space daquele ambiente\n",
    "            action = env.action_space.sample()\n",
    "            #Realiza um step e coleta as informações\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total += reward\n",
    "            \n",
    "            if(step < 5):\n",
    "                print(\"Step: \", step)\n",
    "                print(\"action: \", action)\n",
    "                print(\"observation: \", observation)\n",
    "                print(\"reward: \", reward)\n",
    "                print(\"done: \", done)\n",
    "                print(\"info: \", info)\n",
    "                print(\"--------------\")\n",
    "            step += 1\n",
    "    except:\n",
    "        env.close()\n",
    "    finally:\n",
    "        env.close()\n",
    "    print(\"Total: \"+str(total)+' / '+str(step)) \n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Action – Refers to action taken by the agent within an environment that subsequently yields a reward\n",
    "* Reward – Yielded to the agent. Indicates the quality of action with respect to accomplishing some goal\n",
    "* Observation – Yielded by the action: Refers to the state of the environment after an action has been performed\n",
    "* Done – Boolean that indicates whether the environment needs to be reset\n",
    "* Info – Dictionary with miscellaneous information for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  Box(4,)\n",
      "State:  Box(24,)\n",
      "------------------------------------\n",
      "Step:  0\n",
      "action:  [-0.3761162  -0.33675462  0.71476436 -0.6428799 ]\n",
      "observation:  [-0.01312133 -0.01570254 -0.02448894 -0.01272613  0.46664765  0.75370646\n",
      "  0.07426858 -0.82280477  1.          0.36915979  0.88471621  0.07893729\n",
      " -0.99864133  1.          0.44647983  0.45155028  0.46735349  0.4958424\n",
      "  0.54096764  0.61020452  0.71826363  0.89731878  1.          1.        ]\n",
      "reward:  -0.18624170052073896\n",
      "done:  False\n",
      "info:  {}\n",
      "--------------\n",
      "Step:  1\n",
      "action:  [-0.7792912   0.43466264 -0.56872904  0.15751767]\n",
      "observation:  [ 0.00627774  0.03877634  0.02170096 -0.01006259  0.39183643 -0.91094375\n",
      "  0.16432059  0.58497079  1.          0.30920476 -0.73965609  0.14961171\n",
      "  0.29423732  1.          0.44607088  0.45113668  0.46692541  0.49538824\n",
      "  0.54047215  0.60964561  0.71760577  0.89649689  1.          1.        ]\n",
      "reward:  -0.004817077475290689\n",
      "done:  False\n",
      "info:  {}\n",
      "--------------\n",
      "Step:  2\n",
      "action:  [0.9189723  0.584447   0.6308354  0.31070906]\n",
      "observation:  [-0.00304752 -0.0180186  -0.01031323 -0.01367536  0.42695475  0.19270135\n",
      "  0.12770617 -0.07509031  1.          0.34799379  0.26859993  0.12546003\n",
      " -0.07632513  0.          0.44545728  0.4505161   0.46628311  0.49470681\n",
      "  0.5397287   0.60880697  0.7166186   0.89526367  1.          1.        ]\n",
      "reward:  -0.06156901718303562\n",
      "done:  False\n",
      "info:  {}\n",
      "--------------\n",
      "Step:  3\n",
      "action:  [ 0.2935696  -0.90150976  0.5860624  -0.04585045]\n",
      "observation:  [-0.02443946 -0.04281606 -0.03003937 -0.04389715  0.50068808  0.95000029\n",
      "  0.01154864 -1.          1.          0.42761293  1.00000012  0.01936293\n",
      " -0.89639362  1.          0.44330382  0.44833821  0.46402898  0.49231526\n",
      "  0.53711951  0.60586387  0.71315432  0.89093572  1.          1.        ]\n",
      "reward:  -0.17919785356894136\n",
      "done:  False\n",
      "info:  {}\n",
      "--------------\n",
      "Step:  4\n",
      "action:  [ 0.7287956  -0.08722011 -0.633451   -0.3920005 ]\n",
      "observation:  [-0.03933452 -0.02989753 -0.02189836 -0.06123873  0.57929558  1.\n",
      " -0.10403478 -1.          0.          0.47278672  0.58267951 -0.09689057\n",
      " -1.00000016  0.          0.44035554  0.44535643  0.46094286  0.48904103\n",
      "  0.53354728  0.60183442  0.70841134  0.88501036  1.          1.        ]\n",
      "reward:  -0.14118029473721982\n",
      "done:  False\n",
      "info:  {}\n",
      "--------------\n",
      "Total: -98.98531263316299 / 65\n"
     ]
    }
   ],
   "source": [
    "e = Envirorment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
