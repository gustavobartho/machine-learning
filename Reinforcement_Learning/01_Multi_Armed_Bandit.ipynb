{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multi-Armed Bandit\n",
    "\n",
    "#### We will now look at a practical example of a Reinforcement Learning problem - the multi-armed bandit problem.\n",
    "\n",
    "#### The multi-armed bandit is one of the most popular problems in RL:\n",
    "\n",
    "##### - You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
    "\n",
    "#### You can think of it in analogy to a slot machine (a one-armed bandit). Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot.\n",
    "\n",
    "#### Solving this problem means that we can come come up with an optimal policy: a strategy that allows us to select the best possible action (the one with the highest expected return) at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primeiro a classe do ambiente é criada, onde as distribuições das recompensas são geradas\n",
    "#### O ambiente criado permite que os valores das medias sejam aterados, tornando o ambiente um ambiente não estacionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe com o ambiende \n",
    "class MyEnv:\n",
    "    def __init__(self, arms_num, d_values = [], d_stdev = 1):\n",
    "        #Numero de braços (ações) do hambiente\n",
    "        self.arms_num = arms_num \n",
    "        #Se não for passado um vetor com as medias das distribuições de recompensa entao valores entre -2 e 2 são criados\n",
    "        if not d_values:\n",
    "            self.d_values = [np.random.uniform(-2, 3) for _ in range(self.arms_num)]\n",
    "        else:\n",
    "            self.d_values = d_values\n",
    "        #Desvio padrao das distribuições de recompensas   \n",
    "        self.d_stdev = d_stdev \n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def getReward(self, action):\n",
    "        #Dada uma ação retorna um valor aleatorio da distribuição normal de recompensas para aquela ação\n",
    "        reward = np.random.normal(self.d_values[action], self.d_stdev)\n",
    "        return reward\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def changeEnv(self, d_change_val = []):\n",
    "        #Modifica a media das distribuiçõe\n",
    "        if not d_change_val:\n",
    "            self.d_values = [np.random.uniform(-2, 2) for _ in range(self.arms_num)] \n",
    "        else:\n",
    "            self.d_values = d_change_val \n",
    "    #---------------------------------------------------------------------------------------------------------        \n",
    "    def printVal(self):\n",
    "        for i in range(self.arms_num):\n",
    "            print(\"Q*(%d)\"%i, \" = %.5f\"%self.d_values[i])\n",
    "        print(\"\\n\")\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def getVal(self):\n",
    "        return self.d_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q*(0)  = 2.10030\n",
      "Q*(1)  = -1.89725\n",
      "Q*(2)  = 2.28026\n",
      "Q*(3)  = -0.38324\n",
      "Q*(4)  = 1.61775\n",
      "\n",
      "\n",
      "Q*(0)  = 0.83117\n",
      "Q*(1)  = -0.31221\n",
      "Q*(2)  = -0.51661\n",
      "Q*(3)  = 1.00935\n",
      "Q*(4)  = -0.49362\n",
      "\n",
      "\n",
      "Q*(0)  = 3.50000\n",
      "Q*(1)  = 12.60000\n",
      "Q*(2)  = 4.60000\n",
      "Q*(3)  = -99.00000\n",
      "Q*(4)  = 1.22000\n",
      "\n",
      "\n",
      "Q*(0)  = -3.50000\n",
      "Q*(1)  = -12.60000\n",
      "Q*(2)  = -4.60000\n",
      "Q*(3)  = 99.00000\n",
      "Q*(4)  = -1.22000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "#Cria um ambiente com k elementos e valores aleatorios\n",
    "env = MyEnv(arms_num = k) \n",
    "env.printVal()\n",
    "#Muda os valores para novos valores aleatorios\n",
    "env.changeEnv()\n",
    "env.printVal()\n",
    "#Muda os valores para valores passados\n",
    "env.changeEnv([3.5, 12.6, 4.6, -99, 1.22])\n",
    "env.printVal()\n",
    "#Inverte os valores atuais e os passa como novos valores\n",
    "env.changeEnv([i*(-1) for i in env.getVal()])\n",
    "env.printVal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calsse do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent:\n",
    "    def __init__(self, act_num, step_size = -1, sel_type = 'greedy', eps = 0.1, init_val = 0, ucb_c = 0.1):\n",
    "        self.act_num = act_num\n",
    "        self.step_size = step_size\n",
    "        self.sel_type = sel_type\n",
    "        self.eps = eps\n",
    "        self.init_val = init_val\n",
    "        self.ucb_c = ucb_c\n",
    "        self.q_values = [init_val for _ in range(act_num)]\n",
    "        self.n_values = [0 for _ in range(act_num)]\n",
    "        self.time_steps = 1\n",
    "        self.total_reward = 0\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def argmax(self, q_values):\n",
    "        '''Funcao que retorna o indice do valor maximo de um vetor q_values, \n",
    "        caso haja mais de um valor maximo entao retorna um deles aletoriamente'''\n",
    "        top_value = float(\"-inf\") #maior Q(a)\n",
    "        ties = [] #Acoes que empataram entre o maior valor de Q(a)\n",
    "        for i in range(self.act_num):\n",
    "            if(q_values[i] > top_value):     #Se o Q(a) for maior que o maximo ate o momento:\n",
    "                top_value = q_values[i]          #Substitui o Q(a) maximo pelo valor do atual\n",
    "                ties = []                        #Zera o vetor de empates das acoes com o Q(a) maximo\n",
    "                ties.append(i)                   #Adiciona a acao atual no vetor de empates\n",
    "            elif(q_values[i] == top_value):  #Se o Q(a) for igual ao maximo ate o momento:\n",
    "                ties.append(i)                   #Adiciona a ação ao vetor de empates\n",
    "        return np.random.choice(ties)            #Retorna um valor aleatorio do vetor de empates do Q maximo\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def getAction(self):\n",
    "        \n",
    "        if(self.sel_type == 'eps-greedy'):\n",
    "            '''Caso jeja epsilon-greedy gera um valor aleatorio entre 0 e 1 e \n",
    "            se for menor que o epsilon entao pega uma ação aleatoria, caso \n",
    "            contrario se comporta como o greedy'''\n",
    "            if(np.random.rand() <= self.eps):\n",
    "                act = np.random.randint(self.act_num)\n",
    "            else:\n",
    "                act = self.argmax(self.q_values)\n",
    "        \n",
    "        elif(self.sel_type == 'ucb'):\n",
    "            '''Se for Upper Confidence Bound utiliza a formula para calcular qual acao \n",
    "            é mais indicada baseando na borda superior de incerteza dos Q(a)'''\n",
    "            ucb_q = [0 for _ in range(self.act_num)]\n",
    "            for i in range(self.act_num):\n",
    "                ucb_q[i] = self.q_values[i] + self.ucb_c * math.sqrt(math.log(self.time_steps)/(self.n_values[i]+1))\n",
    "            act = self.argmax(ucb_q)\n",
    "        \n",
    "        else:\n",
    "            '''Caso seja greedy entao ve qual o maior Q(a) e retorna aquela acao, se tiver \n",
    "            mais de um maior entao seleciona aleatriamente entre\n",
    "            as ações com os Q(a) iguais'''\n",
    "            act = self.argmax(self.q_values)\n",
    "        \n",
    "        self.time_steps += 1\n",
    "        self.n_values[act] += 1\n",
    "        self.last_action = act\n",
    "        return act\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def updateValues(self, reward):\n",
    "        self.total_reward += reward\n",
    "        '''Atualiza os valore dos Q(a), se o step size for menor que zero usa a \n",
    "        media aritimetica dos rewards recebidos, se for maior ou igual a zero usa \n",
    "        o weighted average com o step-size passado'''\n",
    "        if(self.step_size < 0):\n",
    "            self.q_values[self.last_action] += ((1/self.n_values[self.last_action]) * (reward - self.q_values[self.last_action]))\n",
    "        else:\n",
    "            self.q_values[self.last_action] += ((self.step_size) * (reward - self.q_values[self.last_action]))\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    def printVal(self):\n",
    "        for i in range(self.act_num):\n",
    "            print(\"Q(%d)\"%i, \" = %.5f\"%self.q_values[i])\n",
    "        print(self.n_values)\n",
    "        print(\"%.5f\"%self.total_reward)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primeiro vamos testar diferentes formas de escolher a ação mais vantajosa em um hambiente estacionario\n",
    "#### A maneira de atualizar os Q(a) usado nessa parte é a media aritimetica dos rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Values\n",
      "Q*(0)  = 1.99790\n",
      "Q*(1)  = -1.05022\n",
      "Q*(2)  = -0.16734\n",
      "Q*(3)  = -0.25097\n",
      "Q*(4)  = 1.16774\n",
      "\n",
      "\n",
      "Greedy Agent\n",
      "Q(0)  = 0.00000\n",
      "Q(1)  = -1.09516\n",
      "Q(2)  = 0.00000\n",
      "Q(3)  = -0.03776\n",
      "Q(4)  = 1.16196\n",
      "[0, 1, 0, 5, 4994]\n",
      "5801.54213\n",
      "\n",
      "\n",
      "Epsilon Greedy agent\n",
      "Q(0)  = 2.00280\n",
      "Q(1)  = -0.99276\n",
      "Q(2)  = -0.17433\n",
      "Q(3)  = -0.30283\n",
      "Q(4)  = 1.39257\n",
      "[4729, 66, 61, 75, 69]\n",
      "9468.46923\n",
      "\n",
      "\n",
      "UCB Agent\n",
      "Q(0)  = 2.01375\n",
      "Q(1)  = -1.09336\n",
      "Q(2)  = -0.12597\n",
      "Q(3)  = -0.13149\n",
      "Q(4)  = 1.19498\n",
      "[4840, 9, 20, 19, 112]\n",
      "9865.55047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "ep_num = 5000\n",
    "#CRIAÇÃO DO AMBIENTE\n",
    "env = MyEnv(arms_num = k, d_stdev=0.5)\n",
    "print(\"Actual Values\")\n",
    "env.printVal()\n",
    "#CRIAÇÃO DOS AGENTES\n",
    "g_agent = MyAgent(act_num=k)\n",
    "eg_agent = MyAgent(act_num=k, sel_type=\"eps-greedy\", eps=0.07)\n",
    "ucb_agent = MyAgent(act_num=k, sel_type=\"ucb\", ucb_c = 3.5)\n",
    "#TREINAMENTO DOS AGENTES\n",
    "for i in range(ep_num):\n",
    "    g_agent.updateValues(env.getReward(g_agent.getAction()))\n",
    "    eg_agent.updateValues(env.getReward(eg_agent.getAction()))\n",
    "    ucb_agent.updateValues(env.getReward(ucb_agent.getAction()))\n",
    "#IMPRESSÃO DOS RESULTADOS   \n",
    "print(\"Greedy Agent\")\n",
    "g_agent.printVal()\n",
    "print(\"Epsilon Greedy agent\")\n",
    "eg_agent.printVal()\n",
    "print(\"UCB Agent\")\n",
    "ucb_agent.printVal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora usando apenas o agente UCB iremos testar deiferentes métoods de atualização dos action values, o primeiro metodo é a media aritimetica dos pesos e o segundo o weighted mean, para isso, no meio do treinamento os valores do ambiente irão mudar para novos valores aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Values\n",
      "Q*(0)  = 2.51600\n",
      "Q*(1)  = -1.28748\n",
      "Q*(2)  = 0.59557\n",
      "Q*(3)  = -0.87445\n",
      "Q*(4)  = 1.85780\n",
      "\n",
      "\n",
      "Mean Agent\n",
      "Q(0)  = 1.33225\n",
      "Q(1)  = 1.25677\n",
      "Q(2)  = 0.09703\n",
      "Q(3)  = 0.75818\n",
      "Q(4)  = 0.75611\n",
      "[3057, 1527, 51, 181, 184]\n",
      "6273.09736\n",
      "\n",
      "\n",
      "Weighted Mean agent\n",
      "Q(0)  = -0.36970\n",
      "Q(1)  = 1.29762\n",
      "Q(2)  = -0.61435\n",
      "Q(3)  = 0.16120\n",
      "Q(4)  = -0.13561\n",
      "[2095, 2242, 49, 268, 346]\n",
      "8905.13537\n",
      "\n",
      "\n",
      "Changed Values\n",
      "Q*(0)  = -2.51600\n",
      "Q*(1)  = 1.28748\n",
      "Q*(2)  = -0.59557\n",
      "Q*(3)  = 0.87445\n",
      "Q*(4)  = -1.85780\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "ep_num = 5000\n",
    "#CRIAÇÃO DO AMBIENTE\n",
    "env = MyEnv(arms_num = k, d_stdev=0.5)\n",
    "print(\"Initial Values\")\n",
    "env.printVal()\n",
    "#CRIAÇÃO DOS AGENTES\n",
    "s_agent = MyAgent(act_num=k, sel_type=\"ucb\", ucb_c = 3.5)\n",
    "ns_agent = MyAgent(act_num=k, sel_type=\"ucb\", ucb_c = 3.5, step_size=0.3)\n",
    "#TREINAMENTO DOS AGENTES\n",
    "for i in range(ep_num):\n",
    "    s_agent.updateValues(env.getReward(s_agent.getAction()))\n",
    "    ns_agent.updateValues(env.getReward(ns_agent.getAction()))\n",
    "    if(i == ep_num/2):\n",
    "        #env.changeEnv()\n",
    "        env.changeEnv([i*(-1) for i in env.getVal()])\n",
    "#IMPRESSÃO DOS RESULTADOS   \n",
    "print(\"Mean Agent\")\n",
    "s_agent.printVal()\n",
    "print(\"Weighted Mean agent\")\n",
    "ns_agent.printVal()\n",
    "print(\"Changed Values\")\n",
    "env.printVal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
