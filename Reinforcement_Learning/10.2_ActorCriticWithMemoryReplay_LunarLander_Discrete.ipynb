{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.layers import Activation, Dense, Input\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from tensorflow import config\n",
    "#config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "        self.max_size = size\n",
    "    #--------------------------------------------------------------------------------    \n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    #--------------------------------------------------------------------------------    \n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terminal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    #--------------------------------------------------------------------------------    \n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of elements in the buffer\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, input_dims, layer1_size, layer2_size, n_actions, alpha, beta, gamma=0.99, memory_size=20000, batch_size=20):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input_dims(int): Observation dimension\n",
    "            layer1_size(int): Number of elements in the first hidden layer\n",
    "            layer2_size(int): Number of elements in the second hidden layer\n",
    "            n_actions(int): Number of possible discrete actions\n",
    "            alpha(float): Actor learning rate\n",
    "            beta(float): Critic learning rate\n",
    "            gamma(float): Discount factor\n",
    "        '''\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_size = layer1_size\n",
    "        self.fc2_size = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
    "        \n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "        self.action_space = np.arange(self.n_actions)\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def build_actor_critic_network(self):\n",
    "        #The actor model ----------------\n",
    "        input_layer = Input(shape=(self.input_dims,))\n",
    "        delta = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_size, activation='relu')(input_layer)\n",
    "        dense2 = Dense(self.fc2_size, activation='relu')(dense1)\n",
    "        #Actor output - Probabilities of chosing each action on a given state\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "    \n",
    "        #To implement a custom loss function in Keras it takes as input the label and the values and return a function\n",
    "        #    and must be implemented inside the function it is used\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            '''\n",
    "            Inputs:\n",
    "                y_true(one hot array)(1, n_actions): The action the agent actually took (the label used in the backprop.)\n",
    "                y_pred(one hot array)(1, n_actions: The output of the neural network\n",
    "            '''\n",
    "            #Since the loss function will be calculating the log of y_pred it can not be zero in any position\n",
    "            out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            #Since the y_true is a one hot array, the value will change only in the selected action\n",
    "            log_lik = y_true * K.log(out)\n",
    "\n",
    "            #delta - Calculated in the learning function - Related to the output of the Critic network\n",
    "            return K.sum(-log_lik * delta)\n",
    "    \n",
    "        actor = Model(inputs=[input_layer, delta], outputs=probs)\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha), loss=custom_loss)\n",
    "        \n",
    "        #The critic model -------------\n",
    "        c_input_layer = Input(shape=(self.input_dims,))\n",
    "        c_dense1 = Dense(self.fc1_size, activation='relu')(c_input_layer)\n",
    "        c_dense2 = Dense(self.fc2_size, activation='relu')(c_dense1)\n",
    "        #Critic output - State value of a given state\n",
    "        values = Dense(1, activation='linear')(c_dense2)\n",
    "        \n",
    "        critic = Model(inputs=c_input_layer, outputs=values)\n",
    "        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')\n",
    "        \n",
    "        #The policy model - Same as the actor but with no need to train --------\n",
    "        policy = Model(inputs=input_layer, outputs=probs)\n",
    "        \n",
    "        return actor, critic, policy\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def choose_action(self, observation, greedy=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "            observation(array)(1, input_dims): Observations returned by the envirorment\n",
    "        '''\n",
    "        #Add one dimension to the observation\n",
    "        state = observation[np.newaxis, :] #(1, 1, input_dims)\n",
    "        #Calculate the probabilities of an action given a state\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        #If the policy is greedy the agent choose the max valued action\n",
    "        if greedy:\n",
    "            action = np.argmax(probabilities)\n",
    "            return action\n",
    "        \n",
    "        #Select an action based on the probabilities of each action\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Inputs:\n",
    "            state(array)(1, input_dims): Current state of the agent\n",
    "            action(int): Action chosed by the agent\n",
    "            reward(float): Reward recieved after selection the action in the state\n",
    "            next_state(array)(1, input_dims): State after selectin action in state\n",
    "            done(bool): If the episode ended or not\n",
    "        '''\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "        \n",
    "        #If the memory have at least the batch_size number of elements\n",
    "        if self.memory.size() > self.memory.minibatch_size:\n",
    "            self.replay_memory()\n",
    "        \n",
    "        #Add one dimension to the state\n",
    "        state = state[np.newaxis, :] #(1, 1, input_dims)\n",
    "        #Add one dimension to the next_state\n",
    "        next_state = next_state[np.newaxis, :] #(1, 1, input_dims)\n",
    "        \n",
    "        #Feed the state to the critic so it can output its value \n",
    "        critic_value = self.critic.predict(state)\n",
    "        #Feed the next state to the critic so it can output its value \n",
    "        critic_next_value = self.critic.predict(next_state)\n",
    "        \n",
    "        #Calculate the TD error\n",
    "        target = reward + self.gamma * critic_next_value * (1 - int(done))\n",
    "        delta = target - critic_value\n",
    "        \n",
    "        #Generate the one hot encoding for the action\n",
    "        actions = np.zeros([1, self.n_actions])\n",
    "        actions[np.arange(1), action] = 1.0\n",
    "        \n",
    "        #Train the actor model with the state and the delta as inputs and actions as label\n",
    "        #    the delta is using in the custom loss function\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "        \n",
    "        #Train the critic model with the state as the input and the target as label\n",
    "        self.critic.fit(state, target, verbose=0)\n",
    "        \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def replay_memory(self):\n",
    "        # Get sample experiences from the replay buffer\n",
    "        experiences = self.memory.sample()\n",
    "        \n",
    "        #Get each term of the esxperiences\n",
    "        state = np.array([exp[0] for exp in experiences])\n",
    "        action = np.array([exp[1] for exp in experiences])\n",
    "        reward = np.array([exp[2] for exp in experiences])\n",
    "        next_state = np.array([exp[3] for exp in experiences])\n",
    "        done = np.array([int(exp[4]) for exp in experiences])\n",
    "\n",
    "        #Feed the state to the critic so it can output its value \n",
    "        critic_value = self.critic.predict(state)\n",
    "        \n",
    "        #Feed the next state to the critic so it can output its value \n",
    "        critic_next_value = self.critic.predict(next_state)\n",
    "        \n",
    "        reward = reward[:, np.newaxis]\n",
    "        done = done[:, np.newaxis]\n",
    "        \n",
    "        #Calculate the TD error\n",
    "        target = reward + self.gamma * critic_next_value * (1 - done)\n",
    "        delta = target - critic_value\n",
    "        \n",
    "        #Generate the one hot encoding for the actions\n",
    "        actions = np.array([np.zeros([self.n_actions]) for _ in range(len(action))])\n",
    "        actions[np.arange(len(action)), action] = 1.0    \n",
    "        #Train the actor model with the state and the delta as inputs and actions as label\n",
    "        #    the delta is using in the custom loss function\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "        \n",
    "        #Train the critic model with the state as the input and the target as label\n",
    "        self.critic.fit(state, target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes:  10 / 1000                                                                     \n",
      "\tTotal reward:  -290.60193992998853 \n",
      "\tNum. steps:  90.66666666666667 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  20 / 1000                                                                     \n",
      "\tTotal reward:  -144.81667361020146 \n",
      "\tNum. steps:  149.2 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  30 / 1000                                                                     \n",
      "\tTotal reward:  -60.77218454873876 \n",
      "\tNum. steps:  145.4 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  40 / 1000                                                                     \n",
      "\tTotal reward:  4.181027567577637 \n",
      "\tNum. steps:  959.7 \n",
      "\tCompleted:  1 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  50 / 1000                                                                     \n",
      "\tTotal reward:  -42.759693039042006 \n",
      "\tNum. steps:  908.7 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  60 / 1000                                                                     \n",
      "\tTotal reward:  -33.16121875519879 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  70 / 1000                                                                     \n",
      "\tTotal reward:  78.53064385755685 \n",
      "\tNum. steps:  816.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  80 / 1000                                                                     \n",
      "\tTotal reward:  11.248434212416129 \n",
      "\tNum. steps:  840.6 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  90 / 1000                                                                     \n",
      "\tTotal reward:  -50.22090087734979 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  100 / 1000                                                                    \n",
      "\tTotal reward:  -72.42306832808195 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  110 / 1000                                                                    \n",
      "\tTotal reward:  -77.89451753058354 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  120 / 1000                                                                    \n",
      "\tTotal reward:  -122.43646355613774 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  130 / 1000                                                                    \n",
      "\tTotal reward:  -83.91707425612825 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  140 / 1000                                                                    \n",
      "\tTotal reward:  -60.328289316850736 \n",
      "\tNum. steps:  881.3 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  150 / 1000                                                                    \n",
      "\tTotal reward:  78.86059178574936 \n",
      "\tNum. steps:  454.8 \n",
      "\tCompleted:  2 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  160 / 1000                                                                    \n",
      "\tTotal reward:  -36.10110063238799 \n",
      "\tNum. steps:  839.1 \n",
      "\tCompleted:  1 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  170 / 1000                                                                    \n",
      "\tTotal reward:  -66.3600238001523 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  180 / 1000                                                                    \n",
      "\tTotal reward:  -54.84332610776081 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  190 / 1000                                                                    \n",
      "\tTotal reward:  -29.13693851904481 \n",
      "\tNum. steps:  1000.0 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  200 / 1000                                                                    \n",
      "\tTotal reward:  -9.185788319958442 \n",
      "\tNum. steps:  960.4 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  210 / 1000                                                                    \n",
      "\tTotal reward:  34.47282016144641 \n",
      "\tNum. steps:  895.5 \n",
      "\tCompleted:  2 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  220 / 1000                                                                    \n",
      "\tTotal reward:  176.8059965903542 \n",
      "\tNum. steps:  631.9 \n",
      "\tCompleted:  4 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  230 / 1000                                                                    \n",
      "\tTotal reward:  132.3582279707299 \n",
      "\tNum. steps:  558.6 \n",
      "\tCompleted:  3 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  240 / 1000                                                                    \n",
      "\tTotal reward:  194.96641610944224 \n",
      "\tNum. steps:  321.6 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  250 / 1000                                                                    \n",
      "\tTotal reward:  184.7711655373385 \n",
      "\tNum. steps:  365.7 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  260 / 1000                                                                    \n",
      "\tTotal reward:  237.4797686433632 \n",
      "\tNum. steps:  353.6 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  270 / 1000                                                                    \n",
      "\tTotal reward:  172.352025518423 \n",
      "\tNum. steps:  280.2 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  280 / 1000                                                                    \n",
      "\tTotal reward:  220.50483011698043 \n",
      "\tNum. steps:  537.6 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  290 / 1000                                                                    \n",
      "\tTotal reward:  159.7582248919837 \n",
      "\tNum. steps:  365.0 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  300 / 1000                                                                    \n",
      "\tTotal reward:  210.88471417654023 \n",
      "\tNum. steps:  197.7 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  310 / 1000                                                                    \n",
      "\tTotal reward:  222.69925510551343 \n",
      "\tNum. steps:  214.3 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  320 / 1000                                                                    \n",
      "\tTotal reward:  223.1793229383265 \n",
      "\tNum. steps:  328.7 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  330 / 1000                                                                    \n",
      "\tTotal reward:  210.01740505751795 \n",
      "\tNum. steps:  304.3 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  340 / 1000                                                                    \n",
      "\tTotal reward:  222.77317307232084 \n",
      "\tNum. steps:  364.0 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  350 / 1000                                                                    \n",
      "\tTotal reward:  218.8698202303427 \n",
      "\tNum. steps:  342.6 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  360 / 1000                                                                    \n",
      "\tTotal reward:  240.41707956893788 \n",
      "\tNum. steps:  292.2 \n",
      "\tCompleted:  10 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  370 / 1000                                                                    \n",
      "\tTotal reward:  227.6443304936643 \n",
      "\tNum. steps:  330.2 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  380 / 1000                                                                    \n",
      "\tTotal reward:  226.06937035505453 \n",
      "\tNum. steps:  294.2 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  390 / 1000                                                                    \n",
      "\tTotal reward:  160.86581502457545 \n",
      "\tNum. steps:  317.4 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  400 / 1000                                                                    \n",
      "\tTotal reward:  139.40913972871783 \n",
      "\tNum. steps:  271.5 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  410 / 1000                                                                    \n",
      "\tTotal reward:  220.76616439831747 \n",
      "\tNum. steps:  259.1 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  420 / 1000                                                                    \n",
      "\tTotal reward:  242.7298679228435 \n",
      "\tNum. steps:  227.7 \n",
      "\tCompleted:  10 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  430 / 1000                                                                    \n",
      "\tTotal reward:  198.85723658452224 \n",
      "\tNum. steps:  400.1 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  440 / 1000                                                                    \n",
      "\tTotal reward:  211.67732780519947 \n",
      "\tNum. steps:  406.1 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  450 / 1000                                                                    \n",
      "\tTotal reward:  201.45239069316915 \n",
      "\tNum. steps:  426.2 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  460 / 1000                                                                    \n",
      "\tTotal reward:  244.33762006007 \n",
      "\tNum. steps:  303.3 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  470 / 1000                                                                    \n",
      "\tTotal reward:  241.31848971965528 \n",
      "\tNum. steps:  441.2 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  480 / 1000                                                                    \n",
      "\tTotal reward:  202.21375695077455 \n",
      "\tNum. steps:  217.1 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  490 / 1000                                                                    \n",
      "\tTotal reward:  235.43875663819898 \n",
      "\tNum. steps:  209.8 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  500 / 1000                                                                    \n",
      "\tTotal reward:  216.37698137767916 \n",
      "\tNum. steps:  241.9 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  510 / 1000                                                                    \n",
      "\tTotal reward:  215.31003714981512 \n",
      "\tNum. steps:  197.9 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  520 / 1000                                                                    \n",
      "\tTotal reward:  234.02071799581546 \n",
      "\tNum. steps:  218.5 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  530 / 1000                                                                    \n",
      "\tTotal reward:  234.04733633015726 \n",
      "\tNum. steps:  302.5 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  540 / 1000                                                                    \n",
      "\tTotal reward:  197.75343350861013 \n",
      "\tNum. steps:  340.3 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  550 / 1000                                                                    \n",
      "\tTotal reward:  215.69444173134016 \n",
      "\tNum. steps:  341.2 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  560 / 1000                                                                    \n",
      "\tTotal reward:  228.9192805934161 \n",
      "\tNum. steps:  336.3 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  570 / 1000                                                                    \n",
      "\tTotal reward:  179.20861904611232 \n",
      "\tNum. steps:  410.8 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  580 / 1000                                                                    \n",
      "\tTotal reward:  184.06346683057936 \n",
      "\tNum. steps:  539.4 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  590 / 1000                                                                    \n",
      "\tTotal reward:  193.67550719451236 \n",
      "\tNum. steps:  359.9 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  600 / 1000                                                                    \n",
      "\tTotal reward:  231.04723357201706 \n",
      "\tNum. steps:  338.5 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  610 / 1000                                                                    \n",
      "\tTotal reward:  199.92254061283663 \n",
      "\tNum. steps:  256.5 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  620 / 1000                                                                    \n",
      "\tTotal reward:  245.73745774780028 \n",
      "\tNum. steps:  275.2 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  630 / 1000                                                                    \n",
      "\tTotal reward:  185.763158371035 \n",
      "\tNum. steps:  316.9 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  640 / 1000                                                                    \n",
      "\tTotal reward:  194.1286583436861 \n",
      "\tNum. steps:  232.1 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  650 / 1000                                                                    \n",
      "\tTotal reward:  167.64971823965797 \n",
      "\tNum. steps:  245.1 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  660 / 1000                                                                    \n",
      "\tTotal reward:  162.91582895385926 \n",
      "\tNum. steps:  238.9 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  670 / 1000                                                                    \n",
      "\tTotal reward:  126.14891532216049 \n",
      "\tNum. steps:  321.6 \n",
      "\tCompleted:  4 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  680 / 1000                                                                    \n",
      "\tTotal reward:  178.05366868427762 \n",
      "\tNum. steps:  226.6 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  690 / 1000                                                                    \n",
      "\tTotal reward:  181.3140202669414 \n",
      "\tNum. steps:  341.1 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  700 / 1000                                                                    \n",
      "\tTotal reward:  125.6841111874538 \n",
      "\tNum. steps:  247.1 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  710 / 1000                                                                    \n",
      "\tTotal reward:  166.2854380643789 \n",
      "\tNum. steps:  305.6 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  720 / 1000                                                                    \n",
      "\tTotal reward:  225.77100144511132 \n",
      "\tNum. steps:  298.3 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  730 / 1000                                                                    \n",
      "\tTotal reward:  165.22612472364955 \n",
      "\tNum. steps:  235.6 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  740 / 1000                                                                    \n",
      "\tTotal reward:  186.99528335537426 \n",
      "\tNum. steps:  229.7 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  750 / 1000                                                                    \n",
      "\tTotal reward:  185.09441251100432 \n",
      "\tNum. steps:  165.4 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  760 / 1000                                                                    \n",
      "\tTotal reward:  158.44940215114076 \n",
      "\tNum. steps:  227.6 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  770 / 1000                                                                    \n",
      "\tTotal reward:  131.9015491790803 \n",
      "\tNum. steps:  372.1 \n",
      "\tCompleted:  4 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  780 / 1000                                                                    \n",
      "\tTotal reward:  198.26253228437074 \n",
      "\tNum. steps:  244.6 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  790 / 1000                                                                    \n",
      "\tTotal reward:  214.17086910535582 \n",
      "\tNum. steps:  308.5 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  800 / 1000                                                                    \n",
      "\tTotal reward:  211.86997006100287 \n",
      "\tNum. steps:  354.0 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  810 / 1000                                                                    \n",
      "\tTotal reward:  199.11524048726378 \n",
      "\tNum. steps:  252.9 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  820 / 1000                                                                    \n",
      "\tTotal reward:  232.76385820955448 \n",
      "\tNum. steps:  243.0 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  830 / 1000                                                                    \n",
      "\tTotal reward:  199.2326340670941 \n",
      "\tNum. steps:  395.0 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  840 / 1000                                                                    \n",
      "\tTotal reward:  205.88975388494814 \n",
      "\tNum. steps:  356.6 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  850 / 1000                                                                    \n",
      "\tTotal reward:  246.4046520390313 \n",
      "\tNum. steps:  268.0 \n",
      "\tCompleted:  10 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  860 / 1000                                                                    \n",
      "\tTotal reward:  215.80678426982132 \n",
      "\tNum. steps:  165.7 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  870 / 1000                                                                    \n",
      "\tTotal reward:  203.3514604593389 \n",
      "\tNum. steps:  263.2 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  880 / 1000                                                                    \n",
      "\tTotal reward:  256.5354370721916 \n",
      "\tNum. steps:  190.4 \n",
      "\tCompleted:  10 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  890 / 1000                                                                    \n",
      "\tTotal reward:  240.094239002022 \n",
      "\tNum. steps:  253.0 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  900 / 1000                                                                    \n",
      "\tTotal reward:  171.20573240345257 \n",
      "\tNum. steps:  319.4 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  910 / 1000                                                                    \n",
      "\tTotal reward:  208.71830615390817 \n",
      "\tNum. steps:  344.4 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  920 / 1000                                                                    \n",
      "\tTotal reward:  138.19568149884813 \n",
      "\tNum. steps:  331.5 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  930 / 1000                                                                    \n",
      "\tTotal reward:  232.37058640637346 \n",
      "\tNum. steps:  264.6 \n",
      "\tCompleted:  9 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  940 / 1000                                                                    \n",
      "\tTotal reward:  192.39364553455786 \n",
      "\tNum. steps:  293.2 \n",
      "\tCompleted:  6 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  950 / 1000                                                                    \n",
      "\tTotal reward:  177.547829461385 \n",
      "\tNum. steps:  247.5 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  960 / 1000                                                                    \n",
      "\tTotal reward:  180.9894403310544 \n",
      "\tNum. steps:  307.8 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  970 / 1000                                                                    \n",
      "\tTotal reward:  129.83417193899223 \n",
      "\tNum. steps:  343.8 \n",
      "\tCompleted:  5 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  980 / 1000                                                                    \n",
      "\tTotal reward:  207.80930036207445 \n",
      "\tNum. steps:  212.7 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  990 / 1000                                                                    \n",
      "\tTotal reward:  203.36693253111034 \n",
      "\tNum. steps:  378.8 \n",
      "\tCompleted:  8 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1000 / 1000                                                                   \n",
      "\tTotal reward:  219.10164340203067 \n",
      "\tNum. steps:  219.5 \n",
      "\tCompleted:  7 \n",
      "--------------------------\n",
      "\n",
      "\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "scores_history = []\n",
    "steps_history = []\n",
    "num_episodes = 1000\n",
    "verbose = 10\n",
    "\n",
    "input_dims = env.observation_space.shape[0]\n",
    "layer1_size = 1024\n",
    "layer2_size = 512\n",
    "n_actions = env.action_space.n\n",
    "alpha = 1e-5\n",
    "beta = 5e-5\n",
    "gamma = 0.98\n",
    "memory_size = 50000\n",
    "batch_size = 35\n",
    "\n",
    "agent = Agent(input_dims, layer1_size, layer2_size, n_actions, alpha, beta, gamma, memory_size, batch_size)\n",
    "\n",
    "print(\"BEGIN\\n\")\n",
    "complete = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        print(\"\\r                                                                                         \", end=\"\")\n",
    "        print(\"\\rEpisode: \"+str(episode+1)+\"\\tStep: \"+str(steps)+\"\\tReward: \"+str(score), end=\"\")\n",
    "        action = agent.choose_action(observation)\n",
    "        new_observation, reward, done, _ = env.step(action)\n",
    "        agent.learn(observation, action, reward, new_observation, done)\n",
    "        observation = new_observation\n",
    "        score += reward\n",
    "        steps += 1\n",
    "    \n",
    "    if(score >= 200):\n",
    "        complete += 1\n",
    "        \n",
    "    if((episode+1)%verbose == 0):\n",
    "        print(\"\\r                                                                                         \", end=\"\")\n",
    "        print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "              , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose:])\n",
    "              , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose:])\n",
    "              , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\\n\")\n",
    "        \n",
    "        complete = 0\n",
    "        \n",
    "    \n",
    "    scores_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "\n",
    "print(\"\\nFINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.03)\n",
    "    action = agent.choose_action(observation, greedy=True)\n",
    "    new_observation, reward, done, _ = env.step(action)\n",
    "    observation = new_observation\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
