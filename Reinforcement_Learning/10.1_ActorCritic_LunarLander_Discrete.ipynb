{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.layers import Activation, Dense, Input\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from tensorflow import config\n",
    "#config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, input_dims, layer1_size, layer2_size, n_actions, alpha, beta, gamma=0.99):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input_dims(int): Observation dimension\n",
    "            layer1_size(int): Number of elements in the first hidden layer\n",
    "            layer2_size(int): Number of elements in the second hidden layer\n",
    "            n_actions(int): Number of possible discrete actions\n",
    "            alpha(float): Actor learning rate\n",
    "            beta(float): Critic learning rate\n",
    "            gamma(float): Discount factor\n",
    "        '''\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_size = layer1_size\n",
    "        self.fc2_size = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def build_actor_critic_network(self):\n",
    "        #The actor and the critic will share the same network, differing only in the output\n",
    "        input_layer = Input(shape=(self.input_dims,))\n",
    "        delta = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_size, activation='relu')(input_layer)\n",
    "        dense2 = Dense(self.fc2_size, activation='relu')(dense1)\n",
    "        \n",
    "        #Actor output - Probabilities of chosing each action on a given state\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "        \n",
    "        #Critic output - State value of a given state\n",
    "        values = Dense(1, activation='linear')(dense2)\n",
    "    \n",
    "        #To implement a custom loss function in Keras it takes as input the label and the values and return a function\n",
    "        #    and must be implemented inside the function it is used\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            '''\n",
    "            Inputs:\n",
    "                y_true(one hot array)(1, n_actions): The action the agent actually took (the label used in the backprop.)\n",
    "                y_pred(one hot array)(1, n_actions: The output of the neural network\n",
    "            '''\n",
    "            #Since the loss function will be calculating the log of y_pred it can not be zero in any position\n",
    "            out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            #Since the y_true is a one hot array, the value will change only in the selected action\n",
    "            log_lik = y_true * K.log(out)\n",
    "\n",
    "            #delta - Calculated in the learning function - Related to the output of the Critic network\n",
    "            return K.sum(-log_lik * delta)\n",
    "    \n",
    "        actor = Model(inputs=[input_layer, delta], outputs=[probs])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha), loss=custom_loss)\n",
    "        \n",
    "        critic = Model(inputs=[input_layer], outputs=[values])\n",
    "        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')\n",
    "        \n",
    "        policy = Model(inputs=[input_layer], outputs=[probs])\n",
    "        \n",
    "        return actor, critic, policy\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def choose_action(self, observation, greedy=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "            observation(array)(1, input_dims): Observations returned by the envirorment\n",
    "        '''\n",
    "        #Add one dimension to the observation\n",
    "        state = observation[np.newaxis, :] #(1, 1, input_dims)\n",
    "        #Calculate the probabilities of an action given a state\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        #If the policy is greedy the agent choose the max valued action\n",
    "        if greedy:\n",
    "            action = np.argmax(probabilities)\n",
    "            return action\n",
    "        \n",
    "        #Select an action based on the probabilities of each action\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Inputs:\n",
    "            state(array)(1, input_dims): Current state of the agent\n",
    "            action(int): Action chosed by the agent\n",
    "            reward(float): Reward recieved after selection the action in the state\n",
    "            next_state(array)(1, input_dims): State after selectin action in state\n",
    "            done(bool): If the episode ended or not\n",
    "        '''\n",
    "        #Add one dimension to the state\n",
    "        state = state[np.newaxis, :] #(1, 1, input_dims)\n",
    "        #Add one dimension to the next_state\n",
    "        next_state = next_state[np.newaxis, :] #(1, 1, input_dims)\n",
    "        \n",
    "        #Feed the state to the critic so it can output its value \n",
    "        critic_value = self.critic.predict(state)\n",
    "        #Feed the next state to the critic so it can output its value \n",
    "        critic_next_value = self.critic.predict(next_state)\n",
    "        \n",
    "        #Calculate the TD error\n",
    "        target = reward + self.gamma * critic_next_value * (1 - int(done))\n",
    "        delta = target - critic_value\n",
    "        \n",
    "        #Generate the one hot encoding for the action\n",
    "        actions = np.zeros([1, self.n_actions])\n",
    "        actions[np.arange(1), action] = 1.0\n",
    "        \n",
    "        #Train the actor model with the state and the delta as inputs and actions as label\n",
    "        #    the delta is using in the custom loss function\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "        \n",
    "        #Train the critic model with the state as the input and the target as label\n",
    "        self.critic.fit(state, target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "\n",
      "Episodes:  100 / 3000                                                                    \n",
      "\tTotal reward:  -511.01451026453014 \n",
      "\tNum. steps:  71.41414141414141 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  200 / 3000                                                                    \n",
      "\tTotal reward:  -556.6097620066683 \n",
      "\tNum. steps:  66.13 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  300 / 3000                                                                    \n",
      "\tTotal reward:  -579.0561971934947 \n",
      "\tNum. steps:  67.28 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  400 / 3000                                                                    \n",
      "\tTotal reward:  -585.9226365052222 \n",
      "\tNum. steps:  67.09 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  500 / 3000                                                                    \n",
      "\tTotal reward:  -580.8779547140488 \n",
      "\tNum. steps:  67.33 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  600 / 3000                                                                    \n",
      "\tTotal reward:  -577.4981856068405 \n",
      "\tNum. steps:  66.98 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  700 / 3000                                                                    \n",
      "\tTotal reward:  -600.265065928955 \n",
      "\tNum. steps:  68.67 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  800 / 3000                                                                    \n",
      "\tTotal reward:  -507.18150591843073 \n",
      "\tNum. steps:  69.12 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  900 / 3000                                                                    \n",
      "\tTotal reward:  -233.26575268538173 \n",
      "\tNum. steps:  72.81 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1000 / 3000                                                                   \n",
      "\tTotal reward:  -231.5697569905609 \n",
      "\tNum. steps:  125.81 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1100 / 3000                                                                   \n",
      "\tTotal reward:  -190.80439277455093 \n",
      "\tNum. steps:  168.99 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1200 / 3000                                                                   \n",
      "\tTotal reward:  -220.38986660835224 \n",
      "\tNum. steps:  177.18 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1300 / 3000                                                                   \n",
      "\tTotal reward:  -206.96431327432924 \n",
      "\tNum. steps:  157.48 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episodes:  1400 / 3000                                                                   \n",
      "\tTotal reward:  -194.53095742086367 \n",
      "\tNum. steps:  153.89 \n",
      "\tCompleted:  0 \n",
      "--------------------------\n",
      "\n",
      "Episode: 1445\tStep: 129\tReward: -4.703706123953973                                       "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "scores_history = []\n",
    "steps_history = []\n",
    "num_episodes = 3000\n",
    "verbose = 100\n",
    "\n",
    "input_dims = env.observation_space.shape[0]\n",
    "layer1_size = 1024\n",
    "layer2_size = 512\n",
    "n_actions = env.action_space.n\n",
    "alpha = 1e-5\n",
    "beta = 5e-5\n",
    "\n",
    "agent = Agent(input_dims, layer1_size, layer2_size, n_actions, alpha, beta)\n",
    "\n",
    "print(\"BEGIN\\n\")\n",
    "complete = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        print(\"\\r                                                                                         \", end=\"\")\n",
    "        print(\"\\rEpisode: \"+str(episode+1)+\"\\tStep: \"+str(steps)+\"\\tReward: \"+str(score), end=\"\")\n",
    "        action = agent.choose_action(observation)\n",
    "        new_observation, reward, done, _ = env.step(action)\n",
    "        agent.learn(observation, action,reward, new_observation, done)\n",
    "        observation = new_observation\n",
    "        score += reward\n",
    "        steps += 1\n",
    "    \n",
    "    if(score >= 200):\n",
    "        complete += 1\n",
    "        \n",
    "    if((episode+1)%verbose == 0):\n",
    "        print(\"\\r                                                                                         \", end=\"\")\n",
    "        print(\"\\rEpisodes: \", episode+1, \"/\", num_episodes\n",
    "              , \"\\n\\tTotal reward: \", np.mean(scores_history[-verbose:])\n",
    "              , \"\\n\\tNum. steps: \", np.mean(steps_history[-verbose:])\n",
    "              , \"\\n\\tCompleted: \", complete, \"\\n--------------------------\\n\")\n",
    "        \n",
    "        complete = 0\n",
    "        \n",
    "    \n",
    "    scores_history.append(score)\n",
    "    steps_history.append(steps)\n",
    "\n",
    "print(\"\\nFINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.choose_action(observation, greedy=True)\n",
    "    new_observation, reward, done, _ = env.step(action)\n",
    "    observation = new_observation\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
